## __init__.py

```python
"""Meta-programming tools to build declarative frameworks"""

from i2.deco import (
    FuncFactory,
    preprocess,
    postprocess,
    preprocess_arguments,
    input_output_decorator,
    wrap_class_methods_input_and_output,
    double_up_as_factory,
)

from i2.signatures import (
    Sig,  # An extended Signature object
    Param,
    sort_params,
    call_forgivingly,  # Call a function extracting the arguments from a pool of arguments
    call_somewhat_forgivingly,  # call_forgivingly with a bit more control
    name_of_obj,  # Get the name of an object, and control how it's done
    empty as empty_param_attr,
)

from i2.multi_object import (
    MultiObj,
    MultiFunc,
    Pipe,
    FuncFanout,
    FlexFuncFanout,
    ParallelFuncs,
    ContextFanout,
)

from i2.errors import (
    InterruptWithBlock,
    HandleExceptions,
)

from i2.wrapper import (
    wrap,
    Wrap,
    Wrapx,
    Ingress,
    include_exclude,
    rm_params,
    partialx,
    ch_names,
    func_to_method_func,
    bind_funcs_object_attrs,
    kwargs_trans,
)

from i2.util import (
    register_object,  # Register an object in a registry
    asis,  # the identity function: f(x) := x (takes only one argument, and returns it)
    return_true,  # a function that returns True (takes any number of arguments)
    return_false,  # a function that returns False (takes any number of arguments)
    ConditionalExceptionCatcher,  # A context manager that catches exceptions based on a condition.
    AttributeMapping,  # a mapping that provides attribute-access to the keys that are valid attribute names
    AttributeMutableMapping,  # a mutable mapping version of AttributeMapping
    copy_func,  # Copy a function.
    get_app_folder,  # Get the application folder of a specified kind for the current system.
    get_app_config_folder,  # Get the application config folder of the current system.
    LiteralVal,  # An object to indicate that the value should be considered literally.
    path_extractor,  # Get items from a tree-structured object from a sequence of tree-traversal indices.
    get_function_body,
    lazyprop,  # Like functools.cached_property, but with a bit more.
    frozendict,  # A hashable dictionary.
    inject_method,  # Inject a method into an object instance
    mk_sentinel,  # Make a sentinel instance.
    ensure_identifiers,  # Ensure that one or several strings are valid python identifiers.
)

identity = asis  # alias for asis, the identity function

from i2.footprints import MethodTrace

from i2.itypes import validate_literal, ObjectClassifier

from i2.doc_mint import (
    params_to_docstring,  # make the params-info part of the docstring
    docstring_to_params,  # extract the params from a docstring
    find_in_params,  # find a substring in the params-info part of the docstring
)


def __getattr__(name):
    """Handle deprecated imports at module level."""
    if name == "get_app_data_folder":
        import warnings

        warnings.warn(
            "`get_app_data_folder` is deprecated. Use `get_app_config_folder` instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        return get_app_config_folder
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
```

## _deprecated.py

```python
"""
Deprecated code I'm keeping around so I can at least reuse it's docs at some point.
"""

from functools import partial
from typing import Any
from collections.abc import Iterable, Callable

from i2 import Sig, name_of_obj
from i2.signatures import extract_arguments


class Command:
    """A class that holds a `(caller, args, kwargs)` triple and allows one to execute
    `caller(*args, **kwargs)`

    :param func: A callable that will be called with (*args, **kwargs) argument.
    :param args: The positional arguments to call the func with.
    :param kwargs: The keyword arguments to call the func with.

    >>> c = Command(print, "hello", "world", sep=", ")
    >>> c()
    hello, world

    What happens (when a command is executed) if some of the arguments are commands
    themselves? Well, the sensible thing happens. These commands are executed.
    You can use this to define, declaratively, some pretty complex instructions, and
    only fetch the data you need and execute everything, once you're ready.

    >>> def show(a, b):
    ...     print(f"Showing this: {a=}, {b=}")
    >>> def take_five():
    ...     return 5
    >>> def double_val(val):
    ...     return val * 2
    >>> command = Command(
    ...     show,
    ...     Command(take_five),
    ...     b=Command(double_val, 'hello'),
    ... )
    >>> command
    Command(show, Command(take_five), b=Command(double_val, 'hello'))
    >>> command()
    Showing this: a=5, b='hellohello'

    Of course, as your use of Command gets more complex, you may want to subclass it
    and include some "validation" and "compilation" in the init.

    The usual way to call a function is to... erm... call it.
    But sometimes you want to do things differently.
    Like validate it, put it on a queue, etc.
    That's where specifying a different _caller will be useful.

    >>> class MyCommand(Command):
    ...     def _caller(self):
    ...         f, a, k = self.func, self.args, self.kwargs
    ...         print(f"Calling {f}(*{a}, **{k}) with result: {f(*a, **k)}")
    ...
    >>> c = MyCommand(print, "hello", "world", sep=", ")
    >>> c()
    hello, world
    Calling <built-in function print>(*('hello', 'world'), **{'sep': ', '}) with result: None

    """

    def __init__(self, func, *args, **kwargs):
        self.func = func
        self.args = args
        self.kwargs = kwargs

    @classmethod
    def curried(cls, func, **kw_defaults):
        """Get an Command maker for a specific function, with defaults and signature!

        >>> def foo(x: str, y: int):
        ...     return x * y
        ...
        >>> foo('hi', 3)
        'hihihi'
        >>>
        >>> foo_command = Command.curried(foo, y=2)
        >>> Sig(foo_command)
        <Sig (x: str, y: int = 2)>
        >>> f = foo_command('hi', y=4)
        >>> f()
        'hihihihi'
        >>> ff = foo_command('hi')
        >>> ff
        Command(foo, 'hi')
        >>> ff()
        'hihi'

        """
        sig = Sig(func)
        sig = sig.ch_defaults(**kw_defaults)

        if kw_defaults:
            func = partial(func, **kw_defaults)

        curried_command_cls = partial(Command, func)
        return sig(curried_command_cls)

    def __repr__(self):
        def to_str(x, quote="'"):
            if isinstance(x, str):
                return quote + x + quote
            else:
                return str(x)

        args_str = ", ".join(to_str(a) for a in self.args)
        kwargs_str = ", ".join(f"{k}={to_str(v)}" for k, v in self.kwargs.items())
        if args_str and kwargs_str:
            sep = ", "
        else:
            sep = ""
        args_kwargs_str = args_str + sep + kwargs_str

        func_name = name_of_obj(self.func)
        if args_kwargs_str:
            return f"{type(self).__name__}({func_name}, {args_kwargs_str})"
        else:
            return f"{type(self).__name__}({func_name})"

    def _caller(self):
        return self.func(*self.args, **self.kwargs)

    def _args_with_executed_commands(self):
        for v in self.args:
            if isinstance(v, Command):
                v = v()  # if a command, execute it
            yield v

    def _kwargs_with_executed_commands(self):
        for k, v in self.kwargs.items():
            if isinstance(v, Command):
                v = v()  # if a command, execute it
            yield k, v

    def _caller(self):
        return self.func(
            *self._args_with_executed_commands(),
            **dict(self._kwargs_with_executed_commands()),
        )

    def __call__(self):
        return self._caller()


def extract_commands(
    funcs: Iterable[Callable],
    *,
    mk_command: Callable[[Callable, tuple, dict], Any] = Command,
    what_to_do_with_remainding="ignore",
    **kwargs,
):
    """

    :param funcs: An iterable of functions
    :param mk_command: The function to make a command object
    :param kwargs: The argname=argval items that the functions should draw from.
    :return:

    >>> def add(a, b: float = 0.0) -> float:
    ...     return a + b
    >>> def mult(x: float, y=1):
    ...     return x * y
    >>> def formula1(w, /, x: float, y=1, *, z: int = 1):
    ...     return ((w + x) * y) ** z
    >>> commands = extract_commands(
    ...     (add, mult, formula1), a=1, b=2, c=3, d=4, e=5, w=6, x=7
    ... )
    >>> for command in commands:
    ...     print(
    ...         f"Calling {command.func.__name__} with "
    ...         f"args={command.args} and kwargs={command.kwargs}"
    ...     )
    ...     print(command())
    ...
    Calling add with args=() and kwargs={'a': 1, 'b': 2}
    3
    Calling mult with args=() and kwargs={'x': 7}
    7
    Calling formula1 with args=(6,) and kwargs={'x': 7}
    13
    """
    extract = partial(
        extract_arguments,
        what_to_do_with_remainding=what_to_do_with_remainding,
        include_all_when_var_keywords_in_params=False,
        assert_no_missing_position_only_args=True,
    )

    if callable(funcs):
        funcs = [funcs]

    for func in funcs:
        func_args, func_kwargs = extract(func, **kwargs)
        yield mk_command(func, *func_args, **func_kwargs)


def commands_dict(
    funcs,
    *,
    mk_command: Callable[[Callable, tuple, dict], Any] = Command,
    what_to_do_with_remainding="ignore",
    **kwargs,
):
    """

    :param funcs:
    :param mk_command:
    :param kwargs:
    :return:

    >>> def add(a, b: float = 0.0) -> float:
    ...     return a + b
    >>> def mult(x: float, y=1):
    ...     return x * y
    >>> def formula1(w, /, x: float, y=1, *, z: int = 1):
    ...     return ((w + x) * y) ** z
    >>> d = commands_dict((add, mult, formula1), a=1, b=2, c=3, d=4, e=5, w=6, x=7)
    >>> d[add]()
    3
    >>> d[mult]()
    7
    >>> d[formula1]()
    13

    """
    if callable(funcs):
        funcs = [funcs]
    it = extract_commands(
        funcs,
        what_to_do_with_remainding=what_to_do_with_remainding,
        mk_command=mk_command,
        **kwargs,
    )
    return dict(zip(funcs, it))
```

## base.py

```python
"""
Tools to provide meta-interfaces of python objects.
See also:
    py2misc/py2store/tree_store.py
"""

import inspect
from inspect import signature, Parameter
from collections.abc import Mapping

from i2.util import lazyprop, FrozenDict, get_function_body

inspect_empty = Parameter.empty


class NotFoundType:
    def __bool__(self):
        return False

    def __repr__(self):
        return "NotFound"


# Note, if the "empty" classes the are used to check if an object is empty are not singletons, could lead to bugs
# TODO: Look into metaprogramming tricks for this.
not_found = NotFoundType()
no_name = type("NoName", (NotFoundType,), {})()
inspect_is_empty = inspect._empty


def is_not_empty(obj):
    if obj is inspect_is_empty or isinstance(obj, NotFoundType):
        return False
    else:
        return True


def _is_property(attr_name, attr_val):
    return not attr_name.startswith("_") and isinstance(attr_val, (property, lazyprop))


def _property_names_of(obj):
    if not isinstance(obj, type):
        obj = type(obj)
    for attr_name in dir(obj):
        attr_val = getattr(obj, attr_name)
        if _is_property(attr_name, attr_val):
            yield attr_name


# Note: For a more powerful, and customizable implementation,
#  see `i2.signatures.name_of_obj`.
# TODO: Analyse usages and reroute to i2.signatures.name_of_obj instead.
def name_of_obj(o):
    from warnings import warn

    warn(
        "Deprecated: Use i2.name_of_obj or i2.signatures.name_of_object instead",
        DeprecationWarning,
    )

    from i2.signatures import name_of_obj as signatures_name_of_obj

    return signatures_name_of_obj(o)


class AttrFromKey:
    def __init__(self, d):
        self._d = d

    def __getattr__(self, k):
        return self._d.__getitem__(k)

    __getitem__ = __getattr__


class KeyFromAttr:
    def __init__(self, d):
        self._d = d

    def __getitem__(self, k):
        return getattr(self._d, k)

    __getattr__ = __getitem__


class _ParameterMintLazy:  # Note: Experimental
    _attrs = ["name", "default", "annotation"]

    #     find = DelegatedAttribute('_params', '__getattribute__')
    #     __getattribute__ = DelegatedAttribute('_params', '__getattribute__')
    def __init__(self, param, _attrs=None):
        self._param = param
        if _attrs is not None:
            # usually used to get full attrs: ['name', 'kind', 'default', 'annotation']
            self._attrs = _attrs

    def items(self):
        for k in self._attrs:
            v = getattr(self._param, k)
            if v is not inspect_empty:
                yield k, v

    def __getitem__(self, k):
        return getattr(self._param, k)

    def __contains__(self, k):
        try:
            _ = self[k]
            return True
        except Exception:
            return False

    def __getattr__(self, k):  # TODO: Use descriptor to make it faster
        return getattr(self._param, k)

    def __getstate__(self):
        return {k: v for k, v in self.items()}


class ParameterMint:
    _attrs = ["name", "kind", "default", "annotation"]

    def __init__(self, param, position=None):
        if hasattr(param, "__getitem__"):
            param = AttrFromKey(param)

        for attr in self._attrs:
            try:
                val = getattr(param, attr, inspect_empty)
            except KeyError:
                val = inspect_empty
            setattr(self, attr, val)
        if position is not None:
            self.position = position

    def __getitem__(self, k):
        return getattr(self, k)

    def __contains__(self, k):
        try:
            _ = self[k]
            return True
        except Exception:
            return False

    def items(self):
        for k in self._attrs:
            v = getattr(self, k)
            if v is not inspect_empty:
                yield k, v
        if hasattr(self, "position"):
            yield "position", self.position

    def __getstate__(self):
        return {k: v for k, v in self.items()}

    def __repr__(self):
        d = dict()
        for k, v in self.items():
            if k == "kind":
                v = v.name
            d[k] = v
        return str(d)


dflt_params = FrozenDict({})


class ParametersMint(Mapping):
    """
    Get mint of the parameters of a callable.

    >>> import inspect
    >>> from pprint import pprint
    >>>
    >>> def g(a, b: 'some_type', c=1, d: int = 1) -> float:
    ...     return a * b * c * d
    >>> mint = ParametersMint(inspect.signature(g).parameters)
    >>> # mint is a mapping (like a read-only dict), so...
    >>> list(mint)
    ['a', 'b', 'c', 'd']
    >>>
    >>> for arg_spec in mint.values():
    ...     print(arg_spec)
    {'name': 'a', 'kind': 'POSITIONAL_OR_KEYWORD', 'position': 0}
    {'name': 'b', 'kind': 'POSITIONAL_OR_KEYWORD', 'annotation': 'some_type', 'position': 1}
    {'name': 'c', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 1, 'position': 2}
    {'name': 'd', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 1, 'annotation': <class 'int'>, 'position': 3}
    >>> t = list(mint.items())
    >>> t[0]
    ('a', {'name': 'a', 'kind': 'POSITIONAL_OR_KEYWORD', 'position': 0})
    >>> t[1]
    ('b', {'name': 'b', 'kind': 'POSITIONAL_OR_KEYWORD', 'annotation': 'some_type', 'position': 1})
    >>>
    >>> mint = ParametersMint(inspect.signature(g).parameters)
    >>> pprint(dict(mint))
    {'a': {'name': 'a', 'kind': 'POSITIONAL_OR_KEYWORD', 'position': 0},
     'b': {'name': 'b', 'kind': 'POSITIONAL_OR_KEYWORD', 'annotation': 'some_type', 'position': 1},
     'c': {'name': 'c', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 1, 'position': 2},
     'd': {'name': 'd', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 1, 'annotation': <class 'int'>, 'position': 3}}

    >>> # and now, some cannibalistic fun...
    >>> # The following is skipped because not working in 3.10
    >>> pprint(
    ...     dict(ParametersMint(inspect.signature(ParametersMint).parameters))
    ... )  # doctest: +SKIP
    {'args': {'name': 'args', 'kind': 'VAR_POSITIONAL', 'position': 0},
     'kwds': {'name': 'kwds', 'kind': 'VAR_KEYWORD', 'position': 1}}
    >>> pprint(
    ...     dict(ParametersMint(inspect.signature(ParametersMint.__init__).parameters))
    ... )  # doctest: +SKIP
    {'params': {'name': 'params', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': FrozenDict({}), 'position': 1},
     'self': {'name': 'self', 'kind': 'POSITIONAL_OR_KEYWORD', 'position': 0}}
    >>> pprint(
    ...     dict(ParametersMint(inspect.signature(ParametersMint.__new__).parameters))
    ... )  # doctest: +SKIP
    {'args': {'name': 'args', 'kind': 'VAR_POSITIONAL', 'position': 1},
     'cls': {'name': 'cls', 'kind': 'POSITIONAL_OR_KEYWORD', 'position': 0},
     'kwds': {'name': 'kwds', 'kind': 'VAR_KEYWORD', 'position': 2}}
    """

    def __init__(self, params=dflt_params):
        self._param_names = list()
        for i, (k, v) in enumerate(params.items()):
            self._param_names.append(k)
            setattr(self, k, ParameterMint(v, position=i))

    def __getitem__(self, k):
        return getattr(self, k)

    def __iter__(self):
        yield from self._param_names

    def items(self):
        for k in self._param_names:
            yield k, getattr(self, k)

    def __len__(self):
        c = 0
        for k in self.__iter__():
            c += 1
        return c

    def __getstate__(self):
        return {k: v.__getstate__() for k, v in self.items()}

    def __setstate__(self, state):
        return type(self)(state)

    def to_dict(self):
        return self.__getstate__()

    def __repr__(self):
        arg_repr = str({k: v.__repr__() for k, v in self.items()})
        return f"{self.__class__.__name__}({arg_repr})"


class Mint(Mapping):
    """
    Get a Mint object of a python object.
    A Mint will provide parameters that provide (meta-)information about the interface of the python object.

    >>> from pprint import pprint
    >>> # Mint of a function
    >>> def f(my_arg: int = 7) -> int:
    ...     return my_arg + 10
    >>> mint = Mint(f)
    >>> mint.obj_name, mint.type_name, mint.module_name, mint.obj_name
    ('f', 'function', 'i2.base', 'f')
    >>> # Mint of a module
    >>> import os as myos
    >>> mint = Mint(myos)
    >>> mint.obj_name, mint.type_name, mint.module_name, mint.obj_name
    ('os', 'module', 'os', 'os')
    >>> assert set(list(mint)) == {'module_name', 'module', 'type_name', 'obj_name'}
    >>> # Mint of a variable
    >>> v = 10
    >>> mint = Mint(v)
    >>> mint.obj_name, mint.type_name, mint.module_name, mint.obj_name
    (NotFound, 'int', NotFound, NotFound)
    >>> assert set(list(mint)) == {'type_name'}  # see that there's only one non-null attr!
    """

    def __init__(self, obj, attrs=None):
        self._obj = obj
        if attrs is None:
            attrs = frozenset(_property_names_of(type(self)))
        else:
            self_type = type(self)
            for attr in attrs:
                if not _is_property(attr, getattr(self_type, attr, None)):
                    raise AttributeError(
                        f"{self.__class__} doesn't have attribute: {attr}"
                    )
        self._attrs = attrs

    def __len__(self):
        return self._length

    def __iter__(self):
        yield from self._non_empty_attrs

    def __getitem__(self, k):
        if k in self._non_empty_attrs:
            return getattr(self, k)
        else:
            raise KeyError("No such key: {k}")

    def items(self):
        for attr in self._non_empty_attrs:
            yield attr, getattr(self, attr)

    @lazyprop
    def _non_empty_attrs(self):
        non_empty_attrs = list()
        for attr in self._attrs:
            attr_val = getattr(self, attr)
            if is_not_empty(attr_val):
                non_empty_attrs.append(attr)
        return non_empty_attrs

    @lazyprop
    def _length(self):
        return len(self._non_empty_attrs)

    @lazyprop
    def obj_name(self):
        return getattr(self._obj, "__name__", not_found)

    @lazyprop
    def type_name(self):
        return getattr(type(self._obj), "__name__", not_found)

    @lazyprop
    def module(self):
        return inspect.getmodule(self._obj) or not_found

    @lazyprop
    def module_name(self):
        return getattr(self.module, "__name__", not_found)


class MintOfCallableMixin:
    @lazyprop
    def _signature(self):
        """Here's some doc"""
        return signature(self._obj)

    @lazyprop
    def parameters(self):
        return ParametersMint(self._signature.parameters)

    @lazyprop
    def return_annotation(self):
        return self._signature.return_annotation or not_found

    @lazyprop
    def doc_string(self):
        return inspect.getdoc(self._obj) or not_found

    @lazyprop
    def comments_preceeding_def(self):
        return inspect.getcomments(self._obj) or not_found

    @lazyprop
    def default_of(self):
        d = dict()
        for k, v in self.parameters.items():
            if "default" in v:
                d[k] = v["default"]
        return d

    @lazyprop
    def annotation_of(self):
        d = dict()
        for k, v in self.parameters.items():
            if "annotation" in v:
                d[k] = v["annotation"]
        return d


class MintOfDocMixin:
    @lazyprop
    def _parsed_doc(self):
        return "Not yet implemented (correctly)"
        from i2.scrap import parse_mint_doc

        # return parse_mint_doc(self.doc_string)


class MintOfCallable(Mint, MintOfCallableMixin, MintOfDocMixin):
    """
    Get a Mint object of a python object.
    A Mint will provide parameters that provide (meta-)information about the interface of the python object.

    >>> from pprint import pprint
    >>> def f(my_arg: int = 7) -> int:
    ...     return my_arg + 10
    >>> f.__doc__ = 'some documentation'
    >>>
    >>> mint = MintOfCallable(f)
    >>> mint.obj_name
    'f'
    >>> mint.type_name
    'function'
    >>> mint.module_name
    'i2.base'
    >>> mint.parameters.my_arg
    {'name': 'my_arg', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 7, 'annotation': <class 'int'>, 'position': 0}
    >>> mint.doc_string
    'some documentation'
    >>> mint.return_annotation
    <class 'int'>
    >>> def g(a, b: 'some_string_id_of_a_custom_type', c=1, d: int = 1) -> float:
    ...     return a * b * c * d
    >>> pprint(dict(MintOfCallable(g).parameters))
    {'a': {'name': 'a', 'kind': 'POSITIONAL_OR_KEYWORD', 'position': 0},
     'b': {'name': 'b', 'kind': 'POSITIONAL_OR_KEYWORD', 'annotation': 'some_string_id_of_a_custom_type', 'position': 1},
     'c': {'name': 'c', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 1, 'position': 2},
     'd': {'name': 'd', 'kind': 'POSITIONAL_OR_KEYWORD', 'default': 1, 'annotation': <class 'int'>, 'position': 3}}
    """

    pass


# class FunctionBuilderMint(Mint):
#     name = lazyprop(Mint.obj_name)
#     module = lazyprop(Mint.module_name)
#     module.__doc__ = 'Name of the module from which this function was imported.'
#     doc = lazyprop(MintOfCallableMixin.doc_string)
#
#     @lazyprop
#     def _signature(self):
#         """ Here's some doc """
#         return signature(self._obj)
#
#     @lazyprop
#     def body(self):
#         """String version of the code representing the body
#         of the function. Defaults to ``'pass'``, which will result
#         in a function which does nothing and returns ``None``."""
#         return get_function_body(self._obj)
#
#     @lazyprop
#     def args(self):
#         return [p['name'] for p in self._signature.parameters]
#
#     @lazyprop
#     def varargs(self):
#         return [p['name'] for p in self._signature.parameters]
#
# """    name (str): Name of the function.
#     doc (str): `Docstring`_ for the function, defaults to empty.
#     module (str): Name of the module from which this function was
#         imported. Defaults to None.
#     body (str): String version of the code representing the body
#         of the function. Defaults to ``'pass'``, which will result
#         in a function which does nothing and returns ``None``.
#     args (list): List of argument names, defaults to empty list,
#         denoting no arguments.
#     varargs (str): Name of the catch-all variable for positional
#         arguments. E.g., "args" if the resultant function is to have
#         ``*args`` in the signature. Defaults to None.
#     varkw (str): Name of the catch-all variable for keyword
#         arguments. E.g., "kwargs" if the resultant function is to have
#         ``**kwargs`` in the signature. Defaults to None.
#     defaults (dict): A mapping of argument names to default values.
#     kwonlyargs (list): Argument names which are only valid as
#         keyword arguments. **Python 3 only.**
#     kwonlydefaults (dict): A mapping, same as normal *defaults*,
#         but only for the *kwonlyargs*. **Python 3 only.**
#     annotations (dict): Mapping of type hints and so
#         forth. **Python 3 only.**
#     filename (str): The filename that will appear in
#         tracebacks. Defaults to "boltons.funcutils.FunctionBuilder".
#     indent (int): Number of spaces with which to indent the
#         function *body*. Values less than 1 will result in an error.
#     dict (dict): Any other attributes which should be added to the
#         functions compiled with this FunctionBuilder."""
```

## castgraph.py

```python
"""
castgraph
=========

A lightweight transformation service for Python that solves the "stable role,
unstable representation" problem: a resource has a consistent semantic role
(e.g., configuration, text, structured record) but appears in many forms
(filepath, string, dict, custom class), while consumers expect specific
representations. castgraph organizes transformations as a graph of "kinds" 
(data representations) and routes requests through the best available path.

Key concepts
------------
- **Kind**: Any hashable identifier for a data representation (type, string, custom marker)
- **Transformation**: An edge in the graph that converts one kind to another
- **Kind Predicate (isa)**: A function that determines if an object is of a kind
- **TransformationGraph**: The main registry with graph-oriented interface

Solution patterns
-----------------
- **Type Converter / Conversion Service**: central registry mapping (FromKind, ToKind) to transformer functions.
- **Adapter**: each edge adapts one representation to another.
- **Strategy**: routing/selection among multiple possible transformations via cost/priority.
- **(Optional) Canonical Data Model**: a hub kind to reduce pairwise conversions.
- **DDD Anti-Corruption Layer (ACL)**: keep external formats outside the core domain.
- **Typeclass / Multimethod idiom**: dispatch based on (source kind, target kind).

Minimal example (new kind-based interface)
-------------------------------------------
Use the new TransformationGraph with flexible kinds (not limited to types).

    >>> from i2.castgraph import TransformationGraph
    >>> graph = TransformationGraph()
    >>> # Add nodes with predicates
    >>> graph.add_node('text', isa=lambda x: isinstance(x, str))
    >>> graph.add_node('json_dict', isa=lambda x: isinstance(x, dict))
    >>> # Add transformation edges
    >>> @graph.register_edge('text', 'json_dict')
    ... def text_to_json(t, ctx):
    ...     import json
    ...     return json.loads(t or "{}")
    >>> # Transform using kinds (need explicit from_kind since 'text' != str)
    >>> result = graph.transform('{"x": 1}', 'json_dict', from_kind='text')
    >>> result["x"]
    1

Legacy example (type-based interface)
--------------------------------------
The old ConversionRegistry interface still works but is deprecated.

    >>> from i2.castgraph import ConversionRegistry
    >>> import warnings
    >>> class Path(str): ...
    >>> class Text(str): ...
    >>> class Record(dict): ...
    >>> reg = ConversionRegistry()
    >>> with warnings.catch_warnings():
    ...     warnings.simplefilter("ignore")
    ...     @reg.register(Path, Text)
    ...     def path_to_text(p, ctx):
    ...         fs = (ctx or {}).get("fs", {})
    ...         return Text(fs.get(str(p), ""))
    ...     @reg.register(Text, Record, cost=0.5)
    ...     def text_to_record(t, ctx):
    ...         import json
    ...         return Record(json.loads(t or "{}"))
    >>> ctx = {"fs": {"/app/data.json": '{"x": 1}'}}
    >>> with warnings.catch_warnings():
    ...     warnings.simplefilter("ignore")
    ...     out = reg.convert(Path("/app/data.json"), Record, context=ctx)
    >>> isinstance(out, Record) and out["x"] == 1
    True

Main tools
----------
- **TransformationGraph**: the main graph-based registry (recommended).
  - `.add_node(kind, isa=None)`: add a kind with optional predicate.
  - `.add_edge(src, dst, func, cost=1.0)`: add a transformation edge.
  - `.register_edge(src, dst, cost=1.0)`: decorator to add an edge.
  - `.transform(obj, to_kind, from_kind=None, context=None)`: transform with multi-hop routing.
  - `.transform_any(obj, to_kind, context=None)`: transform with automatic kind detection.
  - `.get_transformer(from_kind, to_kind)`: get a composed transformer function.
  - `.detect_kind(obj)`: detect the kind of an object.
  - `.reachable_from(kind)`: get all reachable kinds.
  - `.sources_for(kind)`: get all source kinds.
  - `.kinds()`: get all registered kinds.

- **ConversionRegistry**: DEPRECATED - use TransformationGraph instead.
  - `.register(From, To, cost=1.0)`: DEPRECATED - use `.register_edge()` instead.
  - `.convert(obj, ToType, context=None)`: DEPRECATED - use `.transform()` instead.

- **Kind**: Optional wrapper for explicit kind specification with predicates.
- **KindMatch**: Truthy result from kind predicates that can carry metadata.
- **ConversionError**: raised when no route exists between kinds.

Design guidelines
-----------------
- Define a single TransformationGraph per bounded context; keep edges local.
- Prefer small, testable transformer functions with explicit kinds.
- Use a canonical domain kind as a **hub** when many formats interoperate.
- Assign **costs** to prefer fast/accurate routes; tune with metrics.
- Pass a **context** dict for side-channel knobs (I/O, flags, cache handles).
- Cache paths (via lru_cache) and consider result caching for hot transformations.
- Keep adapters at the boundaries; the core domain should consume domain kinds.
- Add identity edges implicitly; avoid no-op boilerplate.
- Write doctests on each transformer to lock behavior and invariants.
- Use bare hashables (types, strings) as kinds; Kind class is optional.

Migration guide
---------------
Old code using ConversionRegistry::

    reg = ConversionRegistry()
    @reg.register(SrcType, DstType)
    def convert_func(obj, ctx): ...
    result = reg.convert(obj, DstType)

New code using TransformationGraph::

    graph = TransformationGraph()
    @graph.register_edge(SrcType, DstType)
    def transform_func(obj, ctx): ...
    result = graph.transform(obj, DstType)

Or with string kinds::

    graph = TransformationGraph()
    graph.add_node('src_format', isa=lambda x: ...)
    @graph.register_edge('src_format', 'dst_format')
    def transform_func(obj, ctx): ...
    result = graph.transform(obj, 'dst_format')

Design heritage
---------------
castgraph is a composition of well-known patterns centered on a **Type Converter /
Conversion Service**, with **Adapter** edges and **Strategy**-based route selection.
At system boundaries, it complements DDD’s **Anti-Corruption Layer** and can employ
an integration **Canonical Data Model** to curb O(n²) pairwise mappings.
Its (FromType, ToType) dispatch style mirrors **typeclass/multimethod** idioms.
For background reading, see:
- .NET TypeConverter: https://learn.microsoft.com/dotnet/api/system.componentmodel.typeconverter
- Spring ConversionService: https://docs.spring.io/spring-framework/reference/core/validation/convert.html
- Apache Camel Type Converter: https://camel.apache.org/manual/type-converter.html
- Adapter: https://refactoring.guru/design-patterns/adapter
- Strategy: https://refactoring.guru/design-patterns/strategy
- Anti-Corruption Layer: https://martinfowler.com/bliki/AntiCorruptionLayer.html
- Canonical Data Model: https://www.enterpriseintegrationpatterns.com/patterns/messaging/CanonicalDataModel.html
- PEP 443 singledispatch: https://peps.python.org/pep-0443/

Related
-------

- Issue that sparked this implementation: https://github.com/i2mint/i2/issues/79
- Computational path resolution: https://github.com/i2mint/meshed/discussions/71
- Subsuming concept - "routing": https://github.com/i2mint/i2/discussions/68

"""

from __future__ import annotations

from collections import defaultdict, deque
from dataclasses import dataclass
from functools import lru_cache, wraps
import inspect
import warnings
from typing import (
    Any,
    Deque,
    Dict,
    List,
    Optional,
    Tuple,
    get_type_hints,
    get_origin,
    Type,
    TypeVar,
)
from collections.abc import Callable, Hashable, Iterable, MutableMapping


T = TypeVar("T")
U = TypeVar("U")
Converter = Callable[[Any, Optional[dict]], Any]


class KindMatch:
    """Result of a successful kind predicate match.
    
    Evaluates to True but can carry additional metadata about the match
    that downstream transformations might use.
    
    >>> match = KindMatch({'encoding': 'utf-8', 'analyzed': True})
    >>> bool(match)
    True
    >>> match.metadata
    {'encoding': 'utf-8', 'analyzed': True}
    """

    def __init__(self, metadata: dict | None = None):
        self.metadata = metadata or {}

    def __bool__(self):
        return True

    def __repr__(self):
        return f"KindMatch({self.metadata})"


class Kind:
    """Optional marker for explicit kind specification.
    
    A Kind wraps a hashable identifier and optionally an 'isa' predicate.
    Users are NOT required to use this class - bare hashables work fine.
    This class is for when you want to be explicit or bundle identifier + predicate.
    
    >>> text_kind = Kind('text', isa=lambda x: isinstance(x, str))
    >>> text_kind.identifier
    'text'
    >>> text_kind.isa("hello")
    True
    """

    def __init__(
        self,
        identifier: Hashable,
        isa: Callable[[Any], bool | KindMatch] | None = None,
    ):
        self.identifier = identifier
        self._isa = isa

    def isa(self, obj: Any) -> bool | KindMatch:
        """Check if obj is of this kind (predicate/recognizer function)."""
        if self._isa is None:
            # If identifier is a type, use isinstance
            if isinstance(self.identifier, type):
                return isinstance(obj, self.identifier)
            raise ValueError(f"Kind {self.identifier} has no 'isa' predicate")
        return self._isa(obj)

    def __hash__(self):
        return hash(self.identifier)

    def __eq__(self, other):
        if isinstance(other, Kind):
            return self.identifier == other.identifier
        return self.identifier == other  # Allow comparison with raw identifiers

    def __repr__(self):
        return f"Kind({self.identifier!r})"


@dataclass(frozen=True)
class Transformation:
    """An edge in the transformation graph.
    
    Represents a transformation function from one kind to another.
    """

    src: Hashable  # Source kind (any hashable)
    dst: Hashable  # Destination kind
    func: Callable[[Any, dict | None], Any]
    cost: float = 1.0  # lower = preferred


# Backward compatibility alias
@dataclass(frozen=True)
class Edge:
    """DEPRECATED: Use Transformation instead. Kept for backward compatibility."""

    src: type[Any]
    dst: type[Any]
    func: Converter
    cost: float = 1.0  # lower = preferred


class ConversionError(TypeError):
    pass


# -----------------------------
# Helpers for registration
# -----------------------------
def _normalize_converter(func: Converter) -> Converter:
    """
    Wrap a converter function to ensure it accepts (obj, context) signature.

    If func only takes one parameter, wrap it to accept and ignore context.
    If func already takes two parameters, return as-is.

    >>> def simple_converter(x): return x * 2
    >>> wrapped = _normalize_converter(simple_converter)
    >>> wrapped(5, None)
    10
    >>> wrapped(5, {'some': 'context'})
    10
    """
    sig = inspect.signature(func)
    params = list(sig.parameters.values())

    if len(params) == 1:
        # Function only takes the object parameter
        @wraps(func)
        def wrapper(obj, context):
            return func(obj)

        return wrapper
    elif len(params) >= 2:
        # Assume it already has (obj, context) or (obj, context, ...) signature
        return func
    else:
        raise ValueError(
            f"Converter function must have at least one parameter, got: {sig}"
        )


def _extract_types_from_annotations(
    func: Callable, provided_src: type | None, provided_dst: type | None
) -> tuple[type[Any], type[Any]]:
    """
    Extract src and dst types from function annotations if not provided.

    Raises
    ------
    ValueError
        If src or dst cannot be determined from either arguments or annotations.

    >>> def converter(x: int, ctx) -> str: return str(x)
    >>> _extract_types_from_annotations(converter, None, None)
    (<class 'int'>, <class 'str'>)
    """
    sig = inspect.signature(func)
    params = list(sig.parameters.values())

    # Evaluate annotations (handles from __future__ annotations)
    try:
        hints = get_type_hints(func)
    except Exception:
        hints = {}

    # Determine src type
    if provided_src is None:
        if not params:
            raise ValueError(
                f"Cannot infer src type: function {func.__name__} has no parameters"
            )
        first_param = params[0]
        annotated = hints.get(first_param.name, None)
        if annotated is None:
            raise ValueError(
                f"Cannot infer src type for {func.__name__}: "
                f"parameter '{first_param.name}' has no type annotation. "
                f"Either provide src explicitly or annotate the first parameter."
            )
        src = annotated
    else:
        src = provided_src
        # Check for conflicts
        if params:
            annotated_src = hints.get(params[0].name, None)
            if annotated_src is not None:
                # Normalize typing aliases (e.g., typing.Mapping) to their runtime
                # origins (e.g., collections.abc.Mapping) for an apples-to-apples
                # comparison. This avoids spurious warnings when the same
                # semantic type is referenced via different typing modules.
                def _canonical_type(t: type[Any]) -> type[Any]:
                    try:
                        origin = get_origin(t)
                        if origin:
                            return origin
                    except Exception:
                        pass
                    # Fallback: map typing.Mapping -> collections.abc.Mapping when
                    # typing.get_origin doesn't return an origin (older Python)
                    try:
                        import collections.abc as cabc

                        if (
                            getattr(t, "__name__", None) == "Mapping"
                            and getattr(t, "__module__", "") == "typing"
                        ):
                            return cabc.Mapping
                    except Exception:
                        pass
                    return t

                if _canonical_type(annotated_src) != _canonical_type(src):
                    warnings.warn(
                        f"Type mismatch in {func.__name__}: "
                        f"register() specifies src={getattr(src, '__name__', str(src))}, "
                        f"but first parameter is annotated as {getattr(annotated_src, '__name__', str(annotated_src))}. "
                        f"Using src={getattr(src, '__name__', str(src))} from register()."
                    )

    # Determine dst type
    if provided_dst is None:
        annotated_ret = hints.get("return", None)
        if annotated_ret is None:
            raise ValueError(
                f"Cannot infer dst type for {func.__name__}: "
                f"no return annotation found. "
                f"Either provide dst explicitly or add a return type annotation."
            )
        dst = annotated_ret
    else:
        dst = provided_dst
        annotated_dst = hints.get("return", None)
        if annotated_dst is not None:
            # Reuse the same canonicalization logic for dst comparison
            def _canonical_type(t: type[Any]) -> type[Any]:
                try:
                    origin = get_origin(t)
                    if origin:
                        return origin
                except Exception:
                    pass
                try:
                    import collections.abc as cabc

                    if (
                        getattr(t, "__name__", None) == "Mapping"
                        and getattr(t, "__module__", "") == "typing"
                    ):
                        return cabc.Mapping
                except Exception:
                    pass
                return t

            if _canonical_type(annotated_dst) != _canonical_type(dst):
                warnings.warn(
                    f"Type mismatch in {func.__name__}: "
                    f"register() specifies dst={getattr(dst, '__name__', str(dst))}, "
                    f"but return annotation is {getattr(annotated_dst, '__name__', str(annotated_dst))}. "
                    f"Using dst={getattr(dst, '__name__', str(dst))} from register()."
                )

    return src, dst


class TransformationGraph:
    """
    A graph-based registry of transformations between kinds (data representations).
    
    A "kind" is any hashable identifier for a data representation - it can be a type,
    a string, or any custom marker. The graph supports:
      - Flexible kind system (not limited to Python types)
      - Graph-oriented interface (add_node, add_edge)
      - Pluggable kind detection via predicates
      - Shortest-path (by total cost) routing
      - MRO-aware fallback for type-based kinds
      - Caching of paths and (optionally) results

    Design notes:
    - Each transformer has signature: func(obj, context) -> transformed_obj
    - Identity edges are implicit (K -> K) with cost 0
    - If multiple routes exist, the minimum total cost path is chosen
    - Kinds can be types, strings, or any hashable objects
    """

    def __init__(self) -> None:
        # Store transformations (edges) indexed by source kind
        self._transformations: dict[Hashable, list[Transformation]] = defaultdict(list)
        # Store kind predicates (isa functions) for automatic kind detection
        self._kind_predicates: dict[
            Hashable, Callable[[Any], bool | KindMatch]
        ] = {}
        # Track all registered kinds (nodes in the graph)
        self._kinds: set[Hashable] = set()
        # Optional custom kind detector function
        self._kind_detector: Callable[[Any], Hashable | None] | None = None
        # Optional result cache (obj identity-sensitive)
        self._result_cache: dict[tuple[int, Hashable], Any] = {}

    # -----------------------------
    # Graph-oriented interface (new)
    # -----------------------------
    def add_node(
        self,
        kind: Hashable | Kind,
        isa: Callable[[Any], bool | KindMatch] | None = None,
    ) -> None:
        """Add a kind (node) to the graph with optional predicate.

        Parameters
        ----------
        kind : Hashable | Kind
            The kind identifier (can be a type, string, or Kind object)
        isa : Callable[[Any], bool | KindMatch] | None
            Optional predicate function to detect if an object is of this kind

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> graph.add_node('text', isa=lambda x: isinstance(x, str))
        >>> graph.add_node(int)  # Type implies isinstance check
        """
        if isinstance(kind, Kind):
            identifier = kind.identifier
            if isa is None:
                # Use the Kind's isa method
                isa = kind.isa
        else:
            identifier = kind

        # Track this kind
        self._kinds.add(identifier)

        # Auto-generate isa for types if not provided
        if isa is None and isinstance(identifier, type):
            isa = lambda obj, t=identifier: isinstance(obj, t)

        if isa is not None:
            self._kind_predicates[identifier] = isa

    def add_edge(
        self,
        src: Hashable | Kind,
        dst: Hashable | Kind,
        func: Callable,
        *,
        cost: float = 1.0,
    ) -> None:
        """Add a transformation (edge) between two kinds.

        Automatically adds nodes if they don't exist.

        Parameters
        ----------
        src : Hashable | Kind
            Source kind
        dst : Hashable | Kind
            Destination kind
        func : Callable
            Transformation function with signature func(obj, context) -> transformed_obj
            or func(obj) -> transformed_obj (will be wrapped)
        cost : float
            Cost of this transformation (lower is preferred)

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> def text_to_int(s, ctx): return int(s)
        >>> graph.add_edge('text', int, text_to_int)
        """
        src_id = src.identifier if isinstance(src, Kind) else src
        dst_id = dst.identifier if isinstance(dst, Kind) else dst

        # Auto-add nodes if they don't exist
        if src_id not in self._kind_predicates:
            self.add_node(src)
        if dst_id not in self._kind_predicates:
            self.add_node(dst)

        normalized_func = _normalize_converter(func)
        self._transformations[src_id].append(
            Transformation(src_id, dst_id, normalized_func, cost)
        )
        self._clear_cache()

    def register_edge(
        self,
        src: Hashable | Kind | None = None,
        dst: Hashable | Kind | None = None,
        *,
        cost: float = 1.0,
    ) -> Callable:
        """Decorator to register a transformation edge.

        Can infer src/dst from function annotations if not provided.

        Parameters
        ----------
        src : Hashable | Kind | None
            Source kind (inferred from annotations if None)
        dst : Hashable | Kind | None
            Destination kind (inferred from annotations if None)
        cost : float
            Cost of this transformation

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> @graph.register_edge('text', int)
        ... def text_to_int(s, ctx): return int(s)
        """

        def deco(func: Callable) -> Callable:
            nonlocal src, dst
            # Infer from annotations if not provided
            if src is None or dst is None:
                inferred_src, inferred_dst = _extract_types_from_annotations(
                    func, src, dst
                )
                src = src if src is not None else inferred_src
                dst = dst if dst is not None else inferred_dst

            self.add_edge(src, dst, func, cost=cost)
            return func

        return deco

    def _clear_cache(self):
        """Clear path cache when graph changes."""
        try:
            type(self)._find_path_cached.cache_clear()
        except Exception:
            pass

    # -----------------------------
    # Transformation methods (new)
    # -----------------------------
    def get_transformer(
        self,
        from_kind: Hashable | Kind,
        to_kind: Hashable | Kind,
        *,
        context: dict | None = None,
    ) -> Callable[[Any], Any]:
        """Get a function that transforms from_kind → to_kind.

        Returns a composed transformer function (Pipe-like).

        Parameters
        ----------
        from_kind : Hashable | Kind
            Source kind
        to_kind : Hashable | Kind
            Destination kind
        context : dict | None
            Optional context to bake into the transformer

        Returns
        -------
        Callable[[Any], Any]
            A function that transforms objects from from_kind to to_kind

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> @graph.register_edge(str, int)
        ... def str_to_int(s, ctx): return int(s)
        >>> transformer = graph.get_transformer(str, int)
        >>> transformer("42")
        42
        """
        from_id = from_kind.identifier if isinstance(from_kind, Kind) else from_kind
        to_id = to_kind.identifier if isinstance(to_kind, Kind) else to_kind

        path = self._find_path_cached(from_id, to_id)

        def transformer(obj: Any, ctx: dict | None = context) -> Any:
            return self._apply_path(obj, path, ctx)

        return transformer

    def transform(
        self,
        obj: Any,
        to_kind: Hashable | Kind,
        *,
        from_kind: Hashable | Kind | None = None,
        context: dict | None = None,
        use_result_cache: bool = False,
    ) -> Any:
        """Transform obj to to_kind.

        If from_kind not specified, uses type(obj) with MRO fallback.

        Parameters
        ----------
        obj : Any
            Object to transform
        to_kind : Hashable | Kind
            Destination kind
        from_kind : Hashable | Kind | None
            Source kind (inferred if None)
        context : dict | None
            Optional context passed to transformation functions
        use_result_cache : bool
            If True, cache results keyed by (id(obj), to_kind)

        Returns
        -------
        Any
            Transformed object

        Raises
        ------
        ConversionError
            If no transformation path is found

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> @graph.register_edge(str, int)
        ... def str_to_int(s, ctx): return int(s)
        >>> graph.transform("42", int)
        42
        """
        to_id = to_kind.identifier if isinstance(to_kind, Kind) else to_kind

        # Explicit from_kind specified
        if from_kind is not None:
            from_id = (
                from_kind.identifier if isinstance(from_kind, Kind) else from_kind
            )
            path = self._find_path_cached(from_id, to_id)
            return self._apply_path(obj, path, context)

        # Check identity first
        if isinstance(obj, to_id) if isinstance(to_id, type) else False:
            return obj

        # Check cache
        if use_result_cache:
            key = (id(obj), to_id)
            if key in self._result_cache:
                return self._result_cache[key]

        # Try type-based lookup with MRO
        from_types = tuple(type(obj).mro())
        for from_type in from_types:
            if (
                from_type in self._transformations
                or from_type in self._kind_predicates
            ):
                try:
                    path = self._find_path_cached(from_type, to_id)
                    result = self._apply_path(obj, path, context)
                    if use_result_cache:
                        self._result_cache[key] = result
                    return result
                except ConversionError:
                    continue

        raise ConversionError(
            f"No transformation path from {type(obj).__name__} to {to_id}"
        )

    def transform_any(
        self,
        obj: Any,
        to_kind: Hashable | Kind,
        *,
        context: dict | None = None,
        use_result_cache: bool = False,
    ) -> Any:
        """Transform obj to to_kind with automatic kind detection.

        Uses configured kind detector or fallback detection strategy.

        Parameters
        ----------
        obj : Any
            Object to transform
        to_kind : Hashable | Kind
            Destination kind
        context : dict | None
            Optional context passed to transformation functions
        use_result_cache : bool
            If True, cache results

        Returns
        -------
        Any
            Transformed object

        Raises
        ------
        ConversionError
            If no transformation path is found or kind cannot be detected

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> graph.add_node('text', isa=lambda x: isinstance(x, str))
        >>> @graph.register_edge('text', int)
        ... def text_to_int(s, ctx): return int(s)
        >>> graph.transform_any("42", int)  # doctest: +SKIP
        42
        """
        detected_kind = self.detect_kind(obj)
        if detected_kind is None:
            raise ConversionError(f"Could not detect kind of {obj}")

        return self.transform(
            obj,
            to_kind,
            from_kind=detected_kind,
            context=context,
            use_result_cache=use_result_cache,
        )

    # -----------------------------
    # Kind detection methods (new)
    # -----------------------------
    def set_kind_detector(self, detector: Callable[[Any], Hashable | None]) -> None:
        """Set a custom kind detector function.

        The detector receives an object and returns a kind identifier or None.

        Parameters
        ----------
        detector : Callable[[Any], Hashable | None]
            Function that takes an object and returns its kind or None

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> def my_detector(obj):
        ...     if isinstance(obj, str) and obj.startswith('{"'):
        ...         return 'json_string'
        ...     return None
        >>> graph.set_kind_detector(my_detector)
        """
        self._kind_detector = detector

    def detect_kind(self, obj: Any) -> Hashable | None:
        """Detect the kind of an object.

        Uses custom detector if set, otherwise tries registered predicates in order.
        Returns None if no kind matches.

        Parameters
        ----------
        obj : Any
            Object to classify

        Returns
        -------
        Hashable | None
            The detected kind identifier, or None if no match

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> graph.add_node('text', isa=lambda x: isinstance(x, str))
        >>> graph.detect_kind("hello")
        'text'
        """
        # Try custom detector first
        if self._kind_detector is not None:
            result = self._kind_detector(obj)
            if result is not None:
                return result

        # Fall back to trying all registered predicates
        # Dict maintains insertion order (Python 3.7+)
        for kind, predicate in self._kind_predicates.items():
            try:
                result = predicate(obj)
                if result:
                    # Result can be True or a truthy KindMatch
                    # Either way, we found our kind
                    return kind
            except Exception:
                # Silently skip predicates that raise
                continue

        # Last resort: try type(obj)
        obj_type = type(obj)
        if obj_type in self._kind_predicates:
            return obj_type

        return None

    # -----------------------------
    # Introspection methods (new)
    # -----------------------------
    def reachable_from(self, kind: Hashable | Kind) -> set[Hashable]:
        """Get all kinds reachable from this kind via transformations.

        Parameters
        ----------
        kind : Hashable | Kind
            The starting kind

        Returns
        -------
        set[Hashable]
            Set of all reachable kind identifiers

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> # ... register transformations ...
        >>> reachable = graph.reachable_from('text')
        """
        kind_id = kind.identifier if isinstance(kind, Kind) else kind
        reachable = set()
        visited = set()
        queue = [kind_id]

        while queue:
            current = queue.pop(0)
            if current in visited:
                continue
            visited.add(current)

            for trans in self._transformations.get(current, []):
                reachable.add(trans.dst)
                queue.append(trans.dst)

        return reachable

    def sources_for(self, kind: Hashable | Kind) -> set[Hashable]:
        """Get all kinds that can be transformed to this kind.

        Parameters
        ----------
        kind : Hashable | Kind
            The destination kind

        Returns
        -------
        set[Hashable]
            Set of all source kind identifiers

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> # ... register transformations ...
        >>> sources = graph.sources_for(int)
        """
        kind_id = kind.identifier if isinstance(kind, Kind) else kind
        sources = set()

        for src, trans_list in self._transformations.items():
            for trans in trans_list:
                if trans.dst == kind_id:
                    sources.add(src)

        return sources

    def kinds(self) -> set[Hashable]:
        """Get all registered kinds (nodes in the graph).

        Returns
        -------
        set[Hashable]
            Set of all registered kind identifiers

        Examples
        --------
        >>> graph = TransformationGraph()
        >>> graph.add_node('text')
        >>> graph.add_node(int)
        >>> 'text' in graph.kinds()
        True
        """
        return self._kinds.copy()

    # -----------------------------
    # Backward compatibility methods (deprecated)
    # -----------------------------
    def register(
        self,
        src: type[Any] | None = None,
        dst: type[Any] | None = None,
        *,
        cost: float = 1.0,
    ) -> Callable[[Converter], Converter]:
        """DEPRECATED: Use register_edge() instead.

        This method is kept for backward compatibility.

        Examples
        --------
        >>> import warnings
        >>> graph = TransformationGraph()
        >>> with warnings.catch_warnings():
        ...     warnings.simplefilter("ignore")
        ...     @graph.register(str, int)
        ...     def str_to_int(s, ctx): return int(s)
        """
        warnings.warn(
            "register() is deprecated and will be removed in a future version. "
            "Use register_edge() instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        return self.register_edge(src, dst, cost=cost)

    def convert(
        self,
        obj: Any,
        to_type: type[U],
        *,
        context: dict | None = None,
        use_result_cache: bool = False,
    ) -> U:
        """DEPRECATED: Use transform() instead.

        This method is kept for backward compatibility.

        Examples
        --------
        >>> import warnings
        >>> graph = TransformationGraph()
        >>> @graph.register_edge(str, int)
        ... def str_to_int(s, ctx): return int(s)
        >>> with warnings.catch_warnings():
        ...     warnings.simplefilter("ignore")
        ...     result = graph.convert("42", int)
        >>> result
        42
        """
        warnings.warn(
            "convert() is deprecated and will be removed in a future version. "
            "Use transform() instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        return self.transform(
            obj, to_type, context=context, use_result_cache=use_result_cache
        )

    # -----------------------------
    # Internals: path finding & application
    # -----------------------------
    def _apply_path(
        self, obj: Any, path: list[Transformation], context: dict | None
    ) -> Any:
        out = obj
        for trans in path:
            out = trans.func(out, context)
        return out

    @lru_cache(maxsize=4096)
    def _find_path_cached(
        self, src: Hashable, dst: Hashable
    ) -> list[Transformation]:
        return self._find_min_cost_path(src, dst)

    def _neighbors(self, src: Hashable) -> Iterable[Transformation]:
        # Outgoing transformations from exact src
        yield from self._transformations.get(src, [])
        # Identity edge (src->src) for free; useful for uniform path logic.
        yield Transformation(src, src, lambda x, ctx: x, cost=0.0)

    def _find_min_cost_path(
        self, src: Hashable, dst: Hashable
    ) -> list[Transformation]:
        if src is dst:
            return []  # already at target

        # Dijkstra over kind-nodes
        frontier: list[tuple[float, Hashable]] = [(0.0, src)]
        dist: dict[Hashable, float] = {src: 0.0}
        prev: dict[Hashable, tuple[Hashable | None, Transformation | None]] = {
            src: (None, None)
        }

        visited: set[Hashable] = set()

        while frontier:
            frontier.sort(key=lambda t: t[0])
            cost_so_far, node = frontier.pop(0)
            if node in visited:
                continue
            visited.add(node)

            if node is dst:
                break

            for trans in self._neighbors(node):
                new_cost = cost_so_far + trans.cost
                if trans.dst not in dist or new_cost < dist[trans.dst]:
                    dist[trans.dst] = new_cost
                    prev[trans.dst] = (node, trans)
                    frontier.append((new_cost, trans.dst))

        if dst not in prev:
            # Try MRO-based widening for dst (accept subclass→superclass matches)
            # Only if dst is a type
            if isinstance(dst, type):
                original_dst = dst
                for super_dst in dst.mro():
                    if super_dst in prev and super_dst is not src:
                        dst = super_dst  # type: ignore[assignment]
                        break

        if dst not in prev:
            raise ConversionError(f"No route from {src} to {dst}")

        # Reconstruct path
        path_edges: Deque[Transformation] = deque()
        cur: Hashable | None = dst
        while cur and prev[cur][0] is not None:
            _, trans = prev[cur]
            assert trans is not None
            path_edges.appendleft(trans)
            cur = prev[cur][0]

        # Strip leading/trailing identity edges
        final_path = [t for t in path_edges if t.src is not t.dst]

        # If MRO widening happened but no conversion steps, this is invalid
        if "original_dst" in locals() and dst is not original_dst and not final_path:
            raise ConversionError(f"No route from {src} to {original_dst}")

        return final_path


class ConversionRegistry:
    """
    DEPRECATED: Use TransformationGraph instead.
    
    A graph-based registry of converters between Python types with:
      - registration decorator
      - shortest-path (by total cost) routing
      - MRO-aware fallback for source types
      - caching of paths and (optionally) results

    Design notes:
    - Each converter has signature: func(obj, context) -> converted_obj
    - Identity edges are implicit (T -> T) with cost 0.
    - If multiple routes exist, the minimum total cost path is chosen.
    """

    def __init__(self) -> None:
        self._edges: dict[type[Any], list[Edge]] = defaultdict(list)
        # Optional result cache (obj identity-sensitive). Keep simple by default.
        self._result_cache: dict[tuple[int, type[Any]], Any] = {}

    # -----------------------------
    # Registration API
    # -----------------------------
    def register(
        self,
        src: type[Any] | None = None,
        dst: type[Any] | None = None,
        *,
        cost: float = 1.0,
    ) -> Callable[[Converter], Converter]:
        """
        Decorator to register a converter function.

        If src or dst are not provided, they will be inferred from the function's
        type annotations.

        Example
        -------
        >>> reg = ConversionRegistry()
        >>> class A: ...
        >>> class B: ...
        >>> @reg.register(A, B)
        ... def a_to_b(a, ctx): return B()
        ...
        >>> isinstance(reg.convert(A(), B), B)
        True

        # Can infer types from annotations:
        >>> class X: ...
        >>> class Y: ...
        >>> @reg.register()
        ... def x_to_y(x: X, ctx) -> Y:
        ...     return Y()
        >>> isinstance(reg.convert(X(), Y), Y)
        True
        """

        def deco(func: Converter) -> Converter:
            nonlocal src, dst
            # Infer types from annotations if not provided
            src, dst = _extract_types_from_annotations(func, src, dst)
            normalized_func = _normalize_converter(func)
            self._edges[src].append(Edge(src, dst, normalized_func, cost))
            # Invalidate path cache because the graph changed
            try:
                type(self)._find_path_cached.cache_clear()
            except Exception:
                pass
            return func

        return deco

    # -----------------------------
    # Public convert()
    # -----------------------------
    def convert(
        self,
        obj: Any,
        to_type: type[U],
        *,
        context: dict | None = None,
        use_result_cache: bool = False,
    ) -> U:
        """
        Convert `obj` to `to_type`, possibly via multi-hop.

        Parameters
        ----------
        obj : Any
            Source object to convert.
        to_type : Type[U]
            Desired target type.
        context : Optional[dict]
            Arbitrary context propagated through the chain (e.g., config, flags).
        use_result_cache : bool
            If True, cache results keyed by (id(obj), to_type).

        Returns
        -------
        U
            Converted object.

        Raises
        ------
        ConversionError
            If no conversion path is found.

        Examples
        --------
        >>> reg = ConversionRegistry()
        >>> class X: ...
        >>> class Y: ...
        >>> class Z: ...
        >>> @reg.register(X, Y)
        ... def x_to_y(x, ctx): return Y()
        ...
        >>> @reg.register(Y, Z)
        ... def y_to_z(y, ctx): return Z()
        ...
        >>> isinstance(reg.convert(X(), Z), Z)
        True

        MRO fallback: if a converter is registered for a base class, it applies to a subclass.
        >>> class Base: ...
        >>> class Sub(Base): ...
        >>> class Out: ...
        >>> reg2 = ConversionRegistry()
        >>> @reg2.register(Base, Out)
        ... def base_to_out(b, ctx): return Out()
        ...
        >>> isinstance(reg2.convert(Sub(), Out), Out)
        True
        """
        if isinstance(obj, to_type):
            return obj  # Identity

        if use_result_cache:
            key = (id(obj), to_type)
            if key in self._result_cache:
                return self._result_cache[key]

        from_types = tuple(type(obj).mro())  # MRO-aware source candidates

        # Try direct or multi-hop path for the first MRO type that can route.
        for from_type in from_types:
            try:
                path = self._find_path_cached(from_type, to_type)
            except ConversionError:
                continue
            result = self._apply_path(obj, path, context)
            if use_result_cache:
                self._result_cache[(id(obj), to_type)] = result
            return result

        raise ConversionError(
            f"No conversion path from {type(obj).__name__} to {to_type.__name__}"
        )

    # -----------------------------
    # Internals: path finding & application
    # -----------------------------
    def _apply_path(self, obj: Any, path: list[Edge], context: dict | None) -> Any:
        out = obj
        for edge in path:
            out = edge.func(out, context)
        return out

    @lru_cache(maxsize=4096)
    def _find_path_cached(self, src: type[Any], dst: type[Any]) -> list[Edge]:
        return self._find_min_cost_path(src, dst)

    def _neighbors(self, src: type[Any]) -> Iterable[Edge]:
        # Outgoing edges from exact src
        yield from self._edges.get(src, [])
        # Identity edge (src->src) for free; useful for uniform path logic.
        yield Edge(src, src, lambda x, ctx: x, cost=0.0)

    def _find_min_cost_path(self, src: type[Any], dst: type[Any]) -> list[Edge]:
        if src is dst:
            return []  # already at target

        # Dijkstra over type-nodes
        frontier: list[tuple[float, type[Any]]] = [(0.0, src)]
        dist: dict[type[Any], float] = {src: 0.0}
        prev: dict[type[Any], tuple[type[Any] | None, Edge | None]] = {
            src: (None, None)
        }

        visited: set[type[Any]] = set()

        while frontier:
            frontier.sort(key=lambda t: t[0])
            cost_so_far, node = frontier.pop(0)
            if node in visited:
                continue
            visited.add(node)

            if node is dst:
                break

            for edge in self._neighbors(node):
                new_cost = cost_so_far + edge.cost
                if edge.dst not in dist or new_cost < dist[edge.dst]:
                    dist[edge.dst] = new_cost
                    prev[edge.dst] = (node, edge)
                    frontier.append((new_cost, edge.dst))

        if dst not in prev:
            # Try MRO-based widening for dst (accept subclass→superclass matches)
            original_dst = dst
            for super_dst in dst.mro():
                if super_dst in prev and super_dst is not src:
                    dst = super_dst  # type: ignore[assignment]
                    break

        if dst not in prev:
            raise ConversionError(f"No route from {src.__name__} to {dst.__name__}")

        # Reconstruct path
        path_edges: Deque[Edge] = deque()
        cur: type[Any] | None = dst
        while cur and prev[cur][0] is not None:
            _, edge = prev[cur]
            assert edge is not None
            path_edges.appendleft(edge)
            cur = prev[cur][0]

        # Strip leading/trailing identity edges
        final_path = [e for e in path_edges if e.src is not e.dst]

        # If MRO widening happened but no conversion steps, this is invalid
        if "original_dst" in locals() and dst is not original_dst and not final_path:
            raise ConversionError(
                f"No route from {src.__name__} to {original_dst.__name__}"
            )

        return final_path

    # -----------------------------
    # Registration API
    # -----------------------------
    def register(
        self,
        src: type[Any] | None = None,
        dst: type[Any] | None = None,
        *,
        cost: float = 1.0,
    ) -> Callable[[Converter], Converter]:
        """
            Decorator to register a converter function.

            Example
            -------
        >>> reg = ConversionRegistry()
        >>> class A: ...
        >>> class B: ...
        >>> @reg.register(A, B)
        ... def a_to_b(a, ctx): return B()
        ...
        >>> isinstance(reg.convert(A(), B), B)
        True

        # Can infer types from annotations:
        >>> class X: ...
        >>> class Y: ...
        >>> @reg.register()
        ... def x_to_y(x: X, ctx) -> Y:
        ...     return Y()
        >>> isinstance(reg.convert(X(), Y), Y)
        True
        """

        def deco(func: Converter) -> Converter:
            nonlocal src, dst
            # Infer types from annotations if not provided
            src, dst = _extract_types_from_annotations(func, src, dst)
            normalized_func = _normalize_converter(func)
            self._edges[src].append(Edge(src, dst, normalized_func, cost))
            # Invalidate path cache because the graph changed
            try:
                type(self)._find_path_cached.cache_clear()
            except Exception:
                pass
            return func

        return deco

    # -----------------------------
    # Public convert()
    # -----------------------------
    def convert(
        self,
        obj: Any,
        to_type: type[U],
        *,
        context: dict | None = None,
        use_result_cache: bool = False,
    ) -> U:
        """
        Convert `obj` to `to_type`, possibly via multi-hop.

        Parameters
        ----------
        obj : Any
            Source object to convert.
        to_type : Type[U]
            Desired target type.
        context : Optional[dict]
            Arbitrary context propagated through the chain (e.g., config, flags).
        use_result_cache : bool
            If True, cache results keyed by (id(obj), to_type).

        Returns
        -------
        U
            Converted object.

        Raises
        ------
        ConversionError
            If no conversion path is found.

        Examples
        --------
        >>> reg = ConversionRegistry()
        >>> class X: ...
        >>> class Y: ...
        >>> class Z: ...
        >>> @reg.register(X, Y)
        ... def x_to_y(x, ctx): return Y()
        ...
        >>> @reg.register(Y, Z)
        ... def y_to_z(y, ctx): return Z()
        ...
        >>> isinstance(reg.convert(X(), Z), Z)
        True

        MRO fallback: if a converter is registered for a base class, it applies to a subclass.
        >>> class Base: ...
        >>> class Sub(Base): ...
        >>> class Out: ...
        >>> reg2 = ConversionRegistry()
        >>> @reg2.register(Base, Out)
        ... def base_to_out(b, ctx): return Out()
        ...
        >>> isinstance(reg2.convert(Sub(), Out), Out)
        True
        """
        if isinstance(obj, to_type):
            return obj  # Identity

        if use_result_cache:
            key = (id(obj), to_type)
            if key in self._result_cache:
                return self._result_cache[key]

        from_types = tuple(type(obj).mro())  # MRO-aware source candidates

        # Try direct or multi-hop path for the first MRO type that can route.
        for from_type in from_types:
            try:
                path = self._find_path_cached(from_type, to_type)
            except ConversionError:
                continue
            result = self._apply_path(obj, path, context)
            if use_result_cache:
                self._result_cache[(id(obj), to_type)] = result
            return result

        raise ConversionError(
            f"No conversion path from {type(obj).__name__} to {to_type.__name__}"
        )

    # -----------------------------
    # Internals: path finding & application
    # -----------------------------
    def _apply_path(self, obj: Any, path: list[Edge], context: dict | None) -> Any:
        out = obj
        for edge in path:
            out = edge.func(out, context)
        return out

    @lru_cache(maxsize=4096)
    def _find_path_cached(self, src: type[Any], dst: type[Any]) -> list[Edge]:
        return self._find_min_cost_path(src, dst)

    def _neighbors(self, src: type[Any]) -> Iterable[Edge]:
        # Outgoing edges from exact src
        yield from self._edges.get(src, [])
        # Identity edge (src->src) for free; useful for uniform path logic.
        yield Edge(src, src, lambda x, ctx: x, cost=0.0)

    def _find_min_cost_path(self, src: type[Any], dst: type[Any]) -> list[Edge]:
        if src is dst:
            return []  # already at target

        # Dijkstra over type-nodes
        frontier: list[tuple[float, type[Any]]] = [(0.0, src)]
        dist: dict[type[Any], float] = {src: 0.0}
        prev: dict[type[Any], tuple[type[Any] | None, Edge | None]] = {
            src: (None, None)
        }

        visited: set[type[Any]] = set()

        while frontier:
            frontier.sort(key=lambda t: t[0])
            cost_so_far, node = frontier.pop(0)
            if node in visited:
                continue
            visited.add(node)

            if node is dst:
                break

            for edge in self._neighbors(node):
                new_cost = cost_so_far + edge.cost
                if edge.dst not in dist or new_cost < dist[edge.dst]:
                    dist[edge.dst] = new_cost
                    prev[edge.dst] = (node, edge)
                    frontier.append((new_cost, edge.dst))

        if dst not in prev:
            # Try MRO-based widening for dst (accept subclass→superclass matches)
            original_dst = dst
            for super_dst in dst.mro():
                if super_dst in prev and super_dst is not src:
                    dst = super_dst  # type: ignore[assignment]
                    break

        if dst not in prev:
            raise ConversionError(f"No route from {src.__name__} to {dst.__name__}")

        # Reconstruct path
        path_edges: Deque[Edge] = deque()
        cur: type[Any] | None = dst
        while cur and prev[cur][0] is not None:
            _, edge = prev[cur]
            assert edge is not None
            path_edges.appendleft(edge)
            cur = prev[cur][0]

        # Strip leading/trailing identity edges
        final_path = [e for e in path_edges if e.src is not e.dst]

        # If MRO widening happened but no conversion steps, this is invalid
        if "original_dst" in locals() and dst is not original_dst and not final_path:
            raise ConversionError(
                f"No route from {src.__name__} to {original_dst.__name__}"
            )

        return final_path


# ----------------------------------------------------------------------
# Backward compatibility aliases
# ----------------------------------------------------------------------

# Alias ConversionRegistry to TransformationGraph for backward compatibility
# The old ConversionRegistry class is kept but deprecated
# Users should migrate to TransformationGraph

# ----------------------------------------------------------------------
# Best-practice guidance
# ----------------------------------------------------------------------


def design_guidelines() -> str:
    """
    Returns concise guidance for organizing casting in Python.

    >>> "registry" in design_guidelines().lower()
    True
    """
    return (
        "- Define a single ConversionRegistry per bounded context; keep edges local.\n"
        "- Prefer small, testable converter functions with explicit types.\n"
        "- Use a canonical domain type as a **hub** when many formats interoperate.\n"
        "- Assign **costs** to prefer fast/accurate routes; tune with metrics.\n"
        "- Pass a **context** dict for side-channel knobs (I/O, flags, cache handles).\n"
        "- Cache paths (via lru_cache) and consider result caching for hot conversions.\n"
        "- Keep adapters at the boundaries; the core domain should consume domain types.\n"
        "- Add identity edges implicitly; avoid no-op boilerplate.\n"
        "- Write doctests on each converter to lock behavior and invariants.\n"
    )


if __name__ == "__main__":
    import doctest

    doctest.testmod()
```

## chain_map.py

```python
"""Merge mappings

Marked for deprecation.
"""

from collections import ChainMap

from collections.abc import Mapping
from itertools import chain

is_mapping = lambda x: isinstance(x, Mapping)
not_mapping = lambda x: not (is_mapping(x))

from collections.abc import Iterable


def is_iterable(x):
    """Similar in nature to :func:`callable`, ``is_iterable`` returns
    ``True`` if an object is `iterable`_, ``False`` if not.

    >>> is_iterable([])
    True
    >>> is_iterable(1)
    False"""
    return isinstance(x, Iterable)


# Note: Mark for moving --> used in only one known place
def unique_iter(src, key=None):
    """Yield unique elements from the iterable, *src*, based on *key*,
    in the order in which they first appeared in *src*.

    >>> repetitious = [1, 2, 3] * 10
    >>> list(unique_iter(repetitious))
    [1, 2, 3]

    By default, *key* is the object itself, but *key* can either be a
    callable or, for convenience, a string name of the attribute on
    which to uniqueify objects, falling back on identity when the
    attribute is not present.

    >>> pleasantries = ['hi', 'hello', 'ok', 'bye', 'yes']
    >>> list(unique_iter(pleasantries, key=lambda x: len(x)))
    ['hi', 'hello', 'bye']
    """
    if not is_iterable(src):
        raise TypeError("expected an iterable, not %r" % type(src))
    if key is None:
        key_func = lambda x: x
    elif callable(key):
        key_func = key
    elif isinstance(key, str):
        key_func = lambda x: getattr(x, key, x)
    else:
        raise TypeError('"key" expected a string or callable, not %r' % key)
    seen = set()
    for i in src:
        k = key_func(i)
        if k not in seen:
            seen.add(k)
            yield i
    return


# Note: Mark for moving to py2store --> used in only one place (py2dash).
class ChainMapTree(Mapping):
    """Combine/overlay multiple hierarchical mappings. This efficiently merges
    multiple hierarchical (could be several layers deep) dictionaries, producing
    a new view into them that acts exactly like a merged dictionary, but without
    doing any copying.
    Because it doesn't actually copy the data, it is intended to be used only
    with immutable mappings. It is safe to change *leaf* data values,
    and the results will be reflected here, but changing the structure of any
    of the trees will not work.

    >>> base1 = {
    ...     'a1': 'base1.a1',
    ...     'a2': 'base1.a2',
    ...     'a3': {
    ...         'b1': 'base1.a3.b1',
    ...         'b2': 'base1.a3.b2',
    ...     },
    ... }
    >>> base2 = {
    ...     'a2': 'base2.a2',
    ...     'a3': {
    ...         'b2': 'base2.a3.b2',
    ...         'b4': 'base2.a3.b4',
    ...     },
    ...     'a4': 'base2.a4',
    ... }
    >>>
    >>> cm = ChainMapTree(base1, base2)
    >>> cm['a1']
    'base1.a1'
    >>> cm['a2']
    'base1.a2'
    >>> cm['a4']
    'base2.a4'
    >>> cm['a3']
    ChainMapTree({'b1': 'base1.a3.b1', 'b2': 'base1.a3.b2'}, {'b2': 'base2.a3.b2', 'b4': 'base2.a3.b4'})
    >>> cm['a3']['b1']
    'base1.a3.b1'
    >>> cm['a3']['b4']
    'base2.a3.b4'
    >>> cm = ChainMapTree(base2, base1)
    >>> cm['a1']
    'base1.a1'
    >>> cm['a2']
    'base2.a2'
    >>> cm['a4']
    'base2.a4'
    >>> cm['a3']
    ChainMapTree({'b2': 'base2.a3.b2', 'b4': 'base2.a3.b4'}, {'b1': 'base1.a3.b1', 'b2': 'base1.a3.b2'})
    >>> cm['a3']['b2']
    'base2.a3.b2'
    >>> cm['a3']['b1']
    'base1.a3.b1'
    >>>
    >>> # Let's do a ChainMapTree with THREE bases now!
    >>> base3 = {
    ...     'a2': 'base3.a2',
    ...     'a3': {
    ...         'b2': 'base3.a3.b2',
    ...         'b4': 'base3.a3.b4',
    ...     },
    ...     'a4': 'base3.a4',
    ... }
    >>> cm = ChainMapTree(base3, base2, base1)
    >>> cm['a2']  # will get it from base3
    'base3.a2'
    >>> cm['a3']['b2']  # will get it from base3 (not base2)
    'base3.a3.b2'
    >>> cm['a3']['b1']  # will get it from base1 (since no one else has it)
    'base1.a3.b1'

    Based on: https://gist.github.com/Klortho/7d83975559bdcc47ac64fd7d877934f6
    """

    _max_repr_keys = (
        7  # Not used (anymore) at this point. Was to control __repr__ output length
    )

    def __init__(self, *maps):
        _maps = list(maps)

        # All keys of kids that are mappings
        kid_keys = {key for m in maps for key in m.keys() if is_mapping(m[key])}

        # This will be a dictionary of lists of mappings
        kid_maps = {}
        for key in kid_keys:
            # The list of child mappings for this key
            kmaps = [m[key] for m in maps if key in m]
            # Make sure they are *all* mappings
            if any(map(not_mapping, kmaps)):
                raise KeyError(key)
            # Recurse
            kid_maps[key] = ChainMapTree(*kmaps)

        # If non-empty, prepend it to the existing list
        if len(kid_maps.keys()) > 0:
            _maps.insert(0, kid_maps)

        self._maps = _maps

    def __getitem__(self, key):
        for _map in self._maps:
            try:
                return _map[key]
            except KeyError:
                pass
        raise KeyError(key)

    def __iter__(self):
        return unique_iter(chain(*(_map.__iter__() for _map in self._maps)))

    def __len__(self):
        return len(list(self.__iter__()))

    def _mk_keys_str(self, keys):
        first_keys = list()
        i = -1
        for i, k in enumerate(keys):
            if i < self._max_repr_keys:
                first_keys.append(k)
            else:
                break
        keys_str = ", ".join(first_keys)
        if i >= self._max_repr_keys:
            keys_str += ", ..."
        return keys_str

    def to_dict(self):
        """Convert to dict

        >>> a = {'a': {'x': 1, 'z': 3}, 'foo': "a's foo"}
        >>> b = {'a': {'y': 222, 'z': 333}, 'foo': "b's foo"}
        >>> cm = ChainMapTree(a, b)
        >>> # It acts like a dict when you ask for items, but is not a dict. If you want a dict, do this:
        >>> cm.to_dict()
        {'a': {'x': 1, 'z': 3, 'y': 222}, 'foo': "a's foo"}
        >>> # Compare to normal/flat/not-nested chaining:
        >>> dict(a, **b)   # Note the precedence is the inverse of ChainMapTree!
        {'a': {'y': 222, 'z': 333}, 'foo': "b's foo"}
        >>>
        >>> # See what you get if you specify b before a
        >>> ChainMapTree(b, a).to_dict()
        {'a': {'y': 222, 'z': 333, 'x': 1}, 'foo': "b's foo"}
        >>> # Compare to normal/flat/not-nested chaining:
        >>> dict(b, **a)  # Note the precedence is the inverse of ChainMapTree!
        {'a': {'x': 1, 'z': 3}, 'foo': "a's foo"}
        """
        d = dict()
        for k in self:
            v = self[k]
            if not isinstance(v, ChainMapTree):
                d[k] = v
            else:
                d[k] = v.to_dict()
        return d

    def __repr__(self):
        return f"{self.__class__.__name__}({', '.join(_map.__repr__() for _map in self._maps)})"
        #         map_keys_strs = self._mk_keys_str(self.keys())
        #         map_keys_strs = []
        #         for _map in self._maps:
        #             map_keys_strs.append(self._mk_keys_str(_map.keys()))
        #         map_keys_strs = ', '.join(map('({})'.format, map_keys_strs))
        # return f"{self.__class__.__name__}({map_keys_strs})"
```

## deco.py

```python
"""Decorator tools"""

import inspect
from collections import defaultdict
from contextlib import suppress
from functools import partial
from functools import wraps  # TODO: Overwritten. Remove?
from inspect import Signature, signature, Parameter
from itertools import chain
from typing import Tuple, Dict, Any, TypeVar
from collections.abc import Callable

from i2.util import mk_sentinel
from i2.signatures import Sig, kind_forgiving_func, name_of_obj

# keep the imports below here because might be referenced (or take care of refs)
# from i2.signatures import ch_signature_to_all_pk, params_of, copy_func

T = TypeVar("T")  # Can be anything


def identity(obj: T) -> T:
    return obj


# ---------------------------------------------------------------------------------------
# The following is an exact copy of the `functools.wraps`, but for some reason, it
# solves an issue of double_up_as_factory on jypyter.
# See description of problem here:
# https://stackoverflow.com/questions/73243479/mysterious-effect-a-decorator-that-breaks-juypyters-ability-to-tab-complete-a

from functools import WRAPPER_ASSIGNMENTS, WRAPPER_UPDATES, update_wrapper


# Overwrites the imported functools.wraps
def wraps(wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES):
    return partial(update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated)


def _resolve_inclusion(include, exclude, super_set):
    if isinstance(include, str):
        include = include.split()
    if isinstance(exclude, str):
        exclude = exclude.split()
    if exclude is None:
        exclude = set()
    include = [x for x in (include or super_set) if x not in exclude]
    return include


def _not_set_repr(self):
    return "NotSet"


NotSet = mk_sentinel("NotSet", repr_=_not_set_repr)


# ---------------------------------------------------------------------------------------
# TODO: https://github.com/i2mint/i2/issues/48
class FuncFactory:
    """Make a function factory.

    but more convenient and helpful (e.g. is picklable, produces functions with
    signatures, etc.)

    One can use ``functools.partials`` to fix, or change, the defaults of
    arguments of a function ``func`` thereby creating a different function.

    >>> def foo(a, b, *, c=2) -> float:
    ...     return a * b + c
    >>> foo(10, 2)
    22
    >>> foo(10, b=2, c=3)
    23
    >>> from functools import partial
    >>> new_foo = partial(foo, b=2, c=3)  # change default c=3 and add one: b=2
    >>> new_foo(10)  # now the function can be called with one argument (couldn't before)
    23

    In essence, ``FuncFactory`` is equivalent to:

    ```
    FuncFactory = lambda func: lambda *args, **kwargs: partial(func, *args, **kwargs)
    ```

    but more convenient and helpful. For one, it doesn't use ``lambda``, so is picklable.
    It also has a more helpful signature:

    >>> factory = FuncFactory(foo)
    >>> factory
    <FuncFactory(foo)>(a, b, *, c=2) -> ...Callable[..., float]

    (Note that the repr even reuses ``foo``'s return annotation to tell us that our
    factory will return a callable that returns that type (if the annotation is a type).

    An instance of ``FuncFactory`` is a factory of functions, that is, it can make
    functions for you based on the instance's underlying ``func``:

    >>> f = factory(b=2, c=3)
    >>> f(10)
    23

    Note that:

    >>> ff = factory(2, 3)  # equivalent to ``factory(a=2, b=3)``
    >>> ff(c=10)
    16

    Further, you can tell ``FuncFactory`` to ``include`` or ``exclude`` specific
    arguments, using their names or indices to specify them.

    >>> factory_no_a = FuncFactory(foo, exclude=['a'])
    >>> factory_no_a
    <FuncFactory(foo)>(b, *, c=2) -> ...Callable[..., float]
    >>> g = factory_no_a(2, 3)  # equivalent to ``factory(b=2, c=3)`` as no ``a`` here
    >>> g(10)
    23

    Recipe: Say you're normalizing some data accessor into callback functions and you
    want to create functions that provide a specific object when called (with no args).
    Sure, you can do this by specifying ``lambda: obj`` every time, but lambdas can be
    problematic (e.g. their not picklable).

    Here's another solution:

    >>> def identity(obj):
    ...     return obj
    >>> func_returning_obj = FuncFactory(identity)
    >>> get_42 = func_returning_obj(42)
    >>> get_42()
    42

    Note: A convenience property has been added to implement this recipe:

    >>> get_42, get_hello = map(FuncFactory.func_returning_obj, (42, 'hello'))
    >>> get_42()
    42
    >>> get_hello()
    'hello'

    """

    def __init__(self, func, *, include=(), exclude=()):
        self.func = func
        func_sig = Sig(func)
        include, exclude = map(func_sig.get_names, (include, exclude))
        include = _resolve_inclusion(include, exclude, func_sig.names)
        self.include = include

        actual_factory_sig = Sig(func_sig, return_annotation=Callable[..., partial])
        actual_factory_sig = actual_factory_sig[self.include]
        # previous I did the following, but don't know why:
        # factory_sig = factory_sig - sig.names[0]
        # Now I know: Because in the func(obj, **params) case it makes more sense
        # that the sig be **params
        # This led to extend to the include/exclude functionality.
        if func_sig.return_annotation is not func_sig.empty:
            try:
                actual_factory_sig = Sig(
                    actual_factory_sig,
                    return_annotation=Callable[..., func_sig.return_annotation],
                )
            except TypeError:
                pass

        self.func_sig = func_sig
        self.factory_sig = actual_factory_sig

        self.__signature__ = actual_factory_sig  # TODO: Delete when #48 solved
        # TODO: Uncomment below to resolved https://github.com/i2mint/i2/issues/48)
        ## Add NotSet default to all non-defaulted params:
        ## (To see why, go to See https://github.com/i2mint/i2/issues/48)
        # shown_factory_sig = actual_factory_sig.ch_defaults(
        #     **{name: NotSet for name in actual_factory_sig.required_names}
        # )
        # shown_factory_sig = shown_factory_sig[self.include]
        # self.__signature__ = shown_factory_sig

    @classmethod
    def wrap(cls, include=(), exclude=()):
        return partial(cls, include=include, exclude=exclude)

    def _process_args_and_kwargs(self, args, kwargs):
        _kwargs = self.factory_sig.map_arguments(
            args, kwargs, allow_partial=True, ignore_kind=True
        )
        # Uncomment below to resolved https://github.com/i2mint/i2/issues/48)
        # _kwargs = {k: v for k, v in _kwargs.items() if v is not NotSet}
        __args, __kwargs = self.func_sig.mk_args_and_kwargs(
            _kwargs, allow_partial=True, ignore_kind=False
        )
        return __args, __kwargs

    def __call__(self, *args, **kwargs):
        args, kwargs = self._process_args_and_kwargs(args, kwargs)
        return partial(self.func, *args, **kwargs)

    def __repr__(self):
        return f"<FuncFactory({name_of_obj(self.func)})>{self.__signature__}"

    @classmethod
    def func_returning_obj(cls, obj):
        return cls(identity)(obj)

    def to_jdict(self):
        return {"func": self.func}

    @classmethod
    def from_jdict(cls, jdict):
        return FuncFactory(**jdict)


def _double_up_as_factory(wrapped=None, *args, __decorator_func=None, **kwargs):
    """Util for double_up_as_factory, ``__decorator_func`` to be partialized"""
    if args:
        raise RuntimeError(
            f"You need to specify decorator arguments as keyword-only."
            f"You specified positional arguments: {args=}"
        )
    if wrapped is None:  # then we want a factory
        return partial(__decorator_func, **kwargs)
    else:
        return __decorator_func(wrapped, *args, **kwargs)


def double_up_as_factory(decorator_func):
    """Repurpose a decorator both as it's original form, and as a decorator factory.
    That is, from a decorator that is defined do ``wrapped_func = decorator(func, **params)``,
    make it also be able to do ``wrapped_func = decorator(**params)(func)``.

    Note: You'll only be able to do this if all but the first argument are keyword-only,
    and the first argument (the function to decorate) has a default of ``None`` (this is for your own good).
    This is validated before making the "double up as factory" decorator.

    >>> @double_up_as_factory
    ... def decorator(func=None, *, multiplier=2):
    ...     def _func(x):
    ...         return func(x) * multiplier
    ...     return _func
    ...
    >>> def foo(x):
    ...     return x + 1
    ...
    >>> foo(2)
    3
    >>> wrapped_foo = decorator(foo, multiplier=10)
    >>> wrapped_foo(2)
    30
    >>>
    >>> multiply_by_3 = decorator(multiplier=3)
    >>> wrapped_foo = multiply_by_3(foo)
    >>> wrapped_foo(2)
    9
    >>>
    >>> @decorator(multiplier=3)
    ... def foo(x):
    ...     return x + 1
    ...
    >>> foo(2)
    9

    Note that to be able to use double_up_as_factory, your first argument (the object to be wrapped) needs to default
    to None and be the only argument that is not keyword-only (i.e. all other arguments need to be keyword only).

    >>> @double_up_as_factory
    ... def decorator_2(func, *, multiplier=2):
    ...     '''Should not be able to be transformed with double_up_as_factory'''
    Traceback (most recent call last):
      ...
    AssertionError: First argument of the decorator function needs to default to None. Was <class 'inspect._empty'>
    >>> @double_up_as_factory
    ... def decorator_3(func=None, multiplier=2):
    ...     '''Should not be able to be transformed with double_up_as_factory'''
    Traceback (most recent call last):
      ...
    AssertionError: All arguments (besides the first) need to be keyword-only

    """

    def validate_decorator_func(decorator_func):
        first_param, *other_params = signature(decorator_func).parameters.values()
        assert first_param.default is None, (
            f"First argument of the decorator function needs to default to None. "
            f"Was {first_param.default}"
        )
        assert all(
            p.kind in {p.KEYWORD_ONLY, p.VAR_KEYWORD} for p in other_params
        ), f"All arguments (besides the first) need to be keyword-only"
        return True

    validate_decorator_func(decorator_func)

    return wraps(decorator_func)(
        partial(_double_up_as_factory, __decorator_func=decorator_func)
    )


# TODO: Review, doc, and make public.
#   Namely, it may be cleaner to have the `argname=argtrans, ...` interface
#   and offer utils to make an argtrans be the combination of a condition and a trans!
@double_up_as_factory
def _conditional_arg_trans(func=None, **condition_and_trans_of_argname):
    """

    See also: ensure_iterable_args, for a more complex example of how to use it.
    """

    @wraps(func)
    def _func(*args, **kwargs):
        sig = Sig(func)
        kwargs = sig.map_arguments(args, kwargs)
        for argname, (condition, trans) in condition_and_trans_of_argname.items():
            if argname in kwargs and condition(kwargs[argname]):
                # Note: Using tuple instead of list because quicker & safer (non-mutable)
                kwargs[argname] = trans(kwargs[argname])
        args, kwargs = sig.mk_args_and_kwargs(kwargs)
        return func(*args, **kwargs)

    return _func


def _tuplize(x):
    return (x,)


def _mk_type_condition(condition):
    """If condition is a type, make an isinstance function to check for that type"""
    curryable_isinstance = kind_forgiving_func(isinstance)
    if isinstance(condition, type) or isinstance(condition, tuple):
        type_condition = condition
        condition = partial(curryable_isinstance, class_or_tuple=type_condition)
    return condition


def ensure_iterable_args(func=None, **condition_of_argname):
    """Wrap a function so that specific arguments are assured to be iterable if
    they meet specific conditions.

    The condition, in the example below, is being a string.
    Note that in general, the condition needs to be a boolean function.
    The explicit form of our example would say `names=lambda x: isinstance(x, str)`,
    but `ensure_iterable_args` allows the convenience of just specifying the type,
    or a tuple of types, and the actually boolean function will be made for you.

    >>> @ensure_iterable_args(names=str)
    ... def greet_people(names, greeting='Hello'):
    ...     for name in names:
    ...         yield f"{greeting} {name}!"
    >>> assert list(greet_people(['Alice', 'Bob'])) == ['Hello Alice!', 'Hello Bob!']
    >>> assert list(greet_people('Alice')) == ['Hello Alice!']

    Note that to decorate a function, you can also use the form:

    >>> greet_people = ensure_iterable_args(greet_people, names=str)
    """
    condition_and_trans_of_argname = {
        argname: (_mk_type_condition(condition), _tuplize)
        for argname, condition in condition_of_argname.items()
    }
    return _conditional_arg_trans(func, **condition_and_trans_of_argname)


def transparently_wrapped(func):
    @wraps(func)
    def transparently_wrapped_func(*args, **kwargs):
        return func(args, **kwargs)

    return transparently_wrapped_func


def mk_args_kwargs_merger(func):
    """
    Make a function that will return a dict containing all {argname: argval} pairs from a function's call.
    That is, it merges all non-keyword arguments with the keyword-arguments, with the right name, so that
    the arguments can be handled more uniformly.
    :param func: The function that will be called, whose signature should be looked at to make the
        merging function
    :return: A function merge_args_and_kwargs(args, kwargs) that can be used to merge arguments

    >>> def func(a, b, c=3):
    ...     return a * (b + c)
    >>> merger = mk_args_kwargs_merger(func)
    >>> dict(merger([1], {'b': 10}))
    {'a': 1, 'b': 10}
    >>> dict(merger([], {'a': 1, 'b': 10}))
    {'a': 1, 'b': 10}
    >>> dict(merger([1, 10], {}))
    {'a': 1, 'b': 10}
    >>> dict(merger([], {}))
    {}
    >>> # Usage demo:
    >>> assert func(*[1], **{'b': 10}) == func(**merger([1], {'b': 10}))
    >>> assert func(*[], **{'a': 1, 'b': 10}) == func(**merger([], {'a': 1, 'b': 10}))
    >>> assert func(**{'a': 1, 'b': 10}) == func(**merger([], {'a': 1, 'b': 10}))
    """

    def merge_args_and_kwargs(args, kwargs):
        if len(args) > 0:
            return inspect.signature(func).bind_partial(*args, **kwargs).arguments
        else:
            return kwargs

    return merge_args_and_kwargs


def kwargs_for_func(*funcs, **kwargs):
    """
    :param funcs:
    :param kwargs:
    :return:

    >>> from i2.tests.objects_for_testing import formula1, sum_of_args, mult, add
    >>> def print_dict(d):  # just a util for this doctest
    ...     from pprint import pprint
    ...     pprint({k.__name__: d[k] for k in sorted(d, key=lambda x: x.__name__)})
    >>> print_dict(kwargs_for_func(formula1, mult, add,
    ...                           w=1, x=2, z=3, a=4, b=5)) # doctest: +NORMALIZE_WHITESPACE
    {'add': {'a': 4, 'b': 5},
     'formula1': {'w': 1, 'x': 2, 'z': 3},
     'mult': {'x': 2}}
    """
    return {func: Sig(func).source_arguments(**kwargs) for func in funcs}


def assert_attrs(attrs):
    """
    Asserts, at construction time, that the class contains a specific set of attributes
    :param attrs: An attribute name (string) or a list of attribute names whose existence needs to be enforced.
    :return: A class decorator that will enforce the existence of the attrs when an instance is made

    >>> @assert_attrs('foo')
    ... class A:
    ...     bar = 10
    ...
    >>> try:
    ...     a = A()
    ... except AttributeError:
    ...     print("AttributeError, as expected, because missing the foo attribute")
    AttributeError, as expected, because missing the foo attribute
    >>> @assert_attrs('foo')
    ... class B:
    ...     def foo(self): pass
    >>> b = B()
    >>>
    >>> class A:
    ...     bar = 10
    >>> class B:
    ...     def foo(self): pass
    >>>
    >>> @assert_attrs(['foo', 'bar'])
    ... class C(A, B):
    ...     pass
    >>> c = C()
    """
    if isinstance(attrs, str):
        attrs = [attrs]

    def _assert_attrs(klass):
        @wraps(klass)
        def get_instance(*args, **kw):
            for attr in attrs:
                if not hasattr(klass, attr):
                    raise AttributeError(
                        "class {} needs to have a {} attribute:".format(
                            klass.__name__, attr
                        )
                    )
            return klass(*args, **kw)

        return get_instance

    return _assert_attrs


def preprocess_arguments(pre):
    """Apply a function to args, kwargs and use the transformed in the decorated function"""

    def decorator(func):
        def wrapper(*args, **kwargs):
            args, kwargs = pre(*args, **kwargs)
            return func(*args, **kwargs)

        return wraps(func)(wrapper)

    return decorator


def preprocess(pre):
    def decorator(func):
        if inspect.ismethod(func):

            def wrapper(self, *args, **kwargs):
                return func(self, pre(*args, **kwargs))

        else:

            def wrapper(*args, **kwargs):
                return func(pre(*args, **kwargs))

        return wraps(func)(wrapper)

    return decorator


def _return_annotation_of(func):
    """Return annotation of callable (if type, will return type systematically)

    >>> def foo() -> bool: ...
    >>> assert _return_annotation_of(foo) == bool
    >>> assert _return_annotation_of(zip) == zip
    >>> assert _return_annotation_of(print) == Parameter.empty
    """
    if isinstance(
        func, type
    ):  # TODO: Verify rule (are there commmon enough meta tricks that need to be handled?)
        return func
    else:
        try:
            return signature(func).return_annotation
        except ValueError:  # some builtins don't have signatures
            return Parameter.empty


class OutputPostProcessingError(RuntimeError): ...


def postprocess(post, caught_post_errors=(Exception,), verbose_error_message=False):
    """Add some post-processing after a function

    :param post: The function to apply to the output

    >>> list_range = postprocess(list)(range)
    >>> list_range(4)
    [0, 1, 2, 3]
    >>> sum_range = postprocess(sum)(range)
    >>> sum_range(4)
    6

    Note: The decorator also sticks the return annotation of the post function on the wrapped one.

    Use cases:

    - Changing a generator into a container returning function
        In many situations, writing a generator is simpler than writing a function
        that accumulates a list or a dict etc.
        So here, you just write the generator and tag this decorator on top, to get the same effect.

    >>> from inspect import signature
    >>> @postprocess(dict)
    ... def bar(x):
    ...     for i in range(x):
    ...         yield str(i), i
    >>> bar(3)
    {'0': 0, '1': 1, '2': 2}
    >>> signature(bar)
    <Signature (x) -> dict>
    >>>
    >>> @postprocess(list)
    ... def foo(x):
    ...     for i in range(x):
    ...         yield i
    >>> foo(3)
    [0, 1, 2]
    >>> from inspect import signature
    >>> signature(foo)
    <Signature (x) -> list>

    - Triggering something (like logging, or forwarding) when a function returns

    >>> def log_this(x):
    ...     print(f"Logging {x}")
    ...     return x
    >>> logged_foo = postprocess(log_this)(foo)
    >>> t = logged_foo(2)
    Logging [0, 1]
    >>> assert t == [0, 1]

    - Using a function that does a lot to make several functions that do less.
        (e.g. Extracting/making a python object from a function returning a raw http response_

    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            output = func(*args, **kwargs)
            try:
                return post(output)
            except caught_post_errors as e:
                msg = f"Error when postprocessing output with post func: {func}"
                if verbose_error_message:
                    msg += "\n" + f"  output={output}"
                    if (
                        isinstance(verbose_error_message, int)
                        and verbose_error_message > 1
                    ):
                        msg += (
                            "\n"
                            + "  which was obtained by func(*args, **kwargs) where:"
                        )
                        msg += (
                            "\n" + f"    args: {args}" + "\n" + f"    kwargs: {kwargs}"
                        )
                msg += "\n" + f"Error is: {e}"
                raise OutputPostProcessingError(msg)

        return_annot = _return_annotation_of(post)
        with suppress(
            ValueError
        ):  # intended to catch cases where wrapper doesn't have a signature
            wrapper_signature = signature(wrapper)
            sig = Signature(
                wrapper_signature.parameters.values(),
                return_annotation=return_annot,
            )
            wrapper.__signature__ = sig

        return wraps(func)(wrapper)

    return decorator


def input_output_decorator(preprocess=None, postprocess=None):
    """
    Makes a decorator that preprocesses inputs and postprocesses outputs.
    Use it if you want to transform the input of a function or method before calling it, or if you want
    to transform the returned value before returning it.
    :param preprocess: Function to be applied to input
    :param postprocess: Function to be applied to output
    :return: a decorator that preprocesses inputs and postprocesses outputs
    See also: preprocess and postprocess decorators if you need only to pre or post process!

    >>> # Examples with "normal functions"
    >>> def f(x=3):
    ...     '''Some doc...'''
    ...     return x + 10
    >>> ff = input_output_decorator()(f)
    >>> print((ff(5.0)))
    15.0
    >>> ff = input_output_decorator(preprocess=int)(f)
    >>> print((ff(5.0)))
    15
    >>> ff = input_output_decorator(preprocess=int, postprocess=lambda x: "Hello {}!".format(x))(f)
    >>> print((ff('5')))
    Hello 15!
    >>> ff = input_output_decorator(postprocess=lambda x: "Hello {}!".format(x))(f)
    >>> print((ff(5.0)))
    Hello 15.0!
    >>> print((ff.__doc__))
    Some doc...
    >>>
    >>> # examples with methods (bounded, class methods, static methods
    >>> class F:
    ...     '''This is not what you'd expect: The doc of the class, not the function'''
    ...     def __init__(self, y=10):
    ...         '''Initialize'''
    ...         self.y = y
    ...     def __call__(self, x=3):
    ...         '''Some doc...'''
    ...         return self.y + x
    ...     @staticmethod
    ...     def static_method(x, y):
    ...         return "What {} {} you have".format(x, y)
    ...     @classmethod
    ...     def class_method(cls, x):
    ...         return "{} likes {}".format(cls.__name__, x)
    >>>
    >>> f = F()
    >>> ff = input_output_decorator()(f)
    >>> print((ff(5.0)))
    15.0
    >>> ff = input_output_decorator(preprocess=int)(f)
    >>> print((ff(5.0)))
    15
    >>> ff = input_output_decorator(preprocess=int, postprocess=lambda x: "Hello {}!".format(x))(f)
    >>> print((ff('5')))
    Hello 15!
    >>> ff = input_output_decorator(postprocess=lambda x: "Hello {}!".format(x))(f)
    >>> print((ff(5.0)))
    Hello 15.0!
    >>> print((ff.__doc__))
    This is not what you'd expect: The doc of the class, not the function

    # >>>
    # >>> f.static_method = input_output_decorator(preprocess=lambda x: '"' + x + '"',
    # ...                                          postprocess=lambda x: x + '!!!')(f.static_method)
    # >>> print(ff.static_method('big', 'eyes'))
    # What big "eyes" you have!!!
    """

    def decorator(func):
        if preprocess and postprocess:

            def func_wrapper(*args, **kwargs):
                return postprocess(func(preprocess(*args, **kwargs)))

        elif preprocess:  # a preprocess but no postprocess

            def func_wrapper(*args, **kwargs):
                return func(preprocess(*args, **kwargs))

        elif postprocess:  # a postprocess but no preprocess

            def func_wrapper(*args, **kwargs):
                return postprocess(func(*args, **kwargs))

        else:  # neither pre nor post processing, so leave func as is
            return func

        return wraps(func)(func_wrapper)

    decorator.preprocess = preprocess
    decorator.postprocess = postprocess

    return decorator


def transform_args(dflt_trans_func=None, /, **trans_func_for_arg):
    """
    Make a decorator that transforms function arguments before calling the function.
    Works with plain functions and bounded methods.
    For example:
        * original argument: a relative path --> used argument: a full path
        * original argument: a pickle filepath --> used argument: the loaded object
    :param rootdir: rootdir to be used for all name arguments of target function
    :param name_arg: the position (int) or argument name of the argument containing the name
    :return: a decorator

    >>> # Example with a plain function
    >>> def f(a, b, c='default_c'):
    ...     return "a={a}, b={b}, c={c}".format(a=a, b=b, c=c)
    >>> def prepend_root(x):
    ...     return 'ROOT/' + x
    >>>
    >>> def test(f):
    ...     assert f('foo', 'bar', 3) == 'a=foo, b=bar, c=3'
    ...     ff = transform_args()(f)  # no transformation specification, so function is unchanged
    ...     assert ff('foo', 'bar', c=3) == 'a=foo, b=bar, c=3'
    ...     ff = transform_args(a=prepend_root)(f)  # prepend root to a
    ...     assert ff('foo', c=3, b='bar') == 'a=ROOT/foo, b=bar, c=3'  # note: testing different order of args
    ...     ff = transform_args(b=prepend_root)(f)  # prepend root to b
    ...     assert ff(c=3, b='bar', a='foo') == 'a=foo, b=ROOT/bar, c=3'  # note: testing different order of args
    ...     ff = transform_args(a=prepend_root, b=prepend_root)(f)  # prepend root to a and b
    ...     assert ff('foo', 'bar', 3) == 'a=ROOT/foo, b=ROOT/bar, c=3'
    ...     assert ff('foo', 'bar') == 'a=ROOT/foo, b=ROOT/bar, c=default_c'  # defaults still work
    >>>
    >>> test(f)
    >>>
    >>> # Example with bounded method, wrapping from instance
    >>> class A:
    ...     def __init__(self, sep=''):
    ...         self.sep = sep
    ...     def f(self, a, b, c='default_c'):
    ...         return f"a={a}{self.sep} b={b}{self.sep} c={c}"
    >>>
    >>> a = A(sep=',')
    >>> test(a.f)
    >>>
    >>> # Example with bounded method, wrapping from class
    >>> A.f = transform_args(a=prepend_root, b=prepend_root)(A.f)
    >>> a = A(sep=',')
    >>> assert a.f('foo', 'bar', 3) == 'a=ROOT/foo, b=ROOT/bar, c=3'
    >>> assert a.f('foo', 'bar') == 'a=ROOT/foo, b=ROOT/bar, c=default_c'  # defaults still work
    """

    def transform_args_decorator(func):
        get_kwargs = mk_args_kwargs_merger(func)

        if (
            len(trans_func_for_arg) == 0 and not dflt_trans_func
        ):  # if no transformations were specified...
            return func  # just return the function itself
        elif dflt_trans_func is not None:
            assert callable(
                dflt_trans_func
            ), "The dflt_trans_func needs to be a callable"

            @wraps(func)
            def transform_args_wrapper(*args, **kwargs):
                val_of_argname = get_kwargs(args, kwargs)
                val_of_argname = {
                    argname: dflt_trans_func(val)
                    for argname, val in val_of_argname.items()
                }

                # apply transform functions to argument values
                return func(**val_of_argname)

            return transform_args_wrapper
        else:

            @wraps(func)
            def transform_args_wrapper(*args, **kwargs):
                # get a {argname: argval, ...} dict from *args and **kwargs
                # Note: Didn't really need an if/else here but I am assuming that...
                # Note: ... getcallargs gives us an overhead that can be avoided if there's only keyword args.

                val_of_argname = get_kwargs(args, kwargs)

                for argname, trans_func in trans_func_for_arg.items():
                    if argname in val_of_argname:
                        val_of_argname[argname] = trans_func(val_of_argname[argname])
                # apply transform functions to argument values
                return func(**val_of_argname)

            return transform_args_wrapper

    transform_args_decorator.dflt_trans_func = dflt_trans_func
    transform_args_decorator.trans_func_for_arg = trans_func_for_arg

    return transform_args_decorator


def wrap_method_output(wrapper_func):
    def _wrap_output(wrapped):
        @wraps(wrapped)
        def _wrapped(self, *args, **kwargs):
            return wrapper_func(wrapped(self, *args, **kwargs))

        return _wrapped

    return _wrap_output


def wrap_class_methods(
    _return_a_copy_of_the_class=True,
    _raise_error_if_non_existent_method=True,
    **wrapper_for_method,
):
    """
    Make a decorator that wraps specific methods.

    IMPORTANT: The decorator will by default return a copy of the class. This might incur some run time overhead.
    If this is desirable, for example, when you want to create several decorations of a same class.
    If you want to change the class itself (e.g. you're only loading it once in a module, and decorating it), then
    specify _return_a_copy_of_the_class=False

    Note that _return_a_copy_of_the_class=True has a side effect of building russian dolls of essentially subclasses
    of the class, which may have some undesirable results if repeated too many times.

    :param _return_a_copy_of_the_class: Specifies whether to
        return a copy of the class (_return_a_copy_of_the_class=True, the default),
        or change the actual loaded class itself (_return_a_copy_of_the_class=False)
    :param wrapper_for_method: method_name=wrapper_function pairs.
    :return: A class wrapper. That is, a decorator that takes a class and returns a decorated version of it
        (or decaorates "in-place" if _return_a_copy_of_the_class=False

    SEE ALSO:
        * wrap_method_output: The function that is called for every method we wrap.
        * transform_class_method_input_and_output: A wrap_class_methods that is specialized for input arg and output
            transformation.

    >>> from functools import wraps
    >>> class A:
    ...     def __init__(self, a=10):
    ...         self.a = a
    ...     def add(self, x):
    ...         return self.a + x
    ...     def multiply(self, x):
    ...         return self.a * x
    ...
    >>> a = A()
    >>> a.add(2)
    12
    >>> a.multiply(2)
    20
    >>>
    >>> def log_calls(func):
    ...     name = func.__name__
    ...     @wraps(func)
    ...     def _func(self, *args, **kwargs):
    ...         print("Calling {} with\\n  args={}\\n  kwargs={}".format(name, args, kwargs))
    ...         return func(self, *args, **kwargs)
    ...     return _func
    ...
    >>> AA = wrap_class_methods(**{k: log_calls for k in ['add', 'multiply']})(A)
    >>> a = AA()
    >>> a.add(x=3)
    Calling add with
      args=()
      kwargs={'x': 3}
    13
    >>> a.multiply(3)
    Calling multiply with
      args=(3,)
      kwargs={}
    30
    """

    def class_wrapper(cls):
        if _return_a_copy_of_the_class:
            _cls = type("_" + cls.__name__, cls.__bases__, dict(cls.__dict__))
            # class _cls(cls):
            #     pass
        else:
            _cls = cls
        for method, wrapper in wrapper_for_method.items():
            if hasattr(_cls, method):
                setattr(_cls, method, wrapper(getattr(_cls, method)))
            elif _raise_error_if_non_existent_method:
                raise ValueError(
                    f"{getattr(_cls, '__name__', str(cls))} has no '{method}' method!"
                )
        return _cls

    return class_wrapper


def mk_input_and_output_method_wrapper(method_output_trans=None, **arg_trans):
    def wrap_method(method_func):
        wrapped_method = transform_args(**arg_trans)(method_func)
        if method_output_trans is not None:
            return wrap_method_output(method_output_trans)(wrapped_method)
        else:
            return wrapped_method

    return wrap_method


def transform_class_method_input_and_output(
    cls, method, method_output_trans=None, **arg_trans
):
    wrapped_method = transform_args(**arg_trans)(getattr(cls, method))
    if method_output_trans is not None:
        setattr(
            cls,
            method,
            wrap_method_output(method_output_trans)(wrapped_method),
        )
    else:
        setattr(cls, method, wrapped_method)


def wrap_class_methods_input_and_output(
    _return_a_copy_of_the_class=True,
    _raise_error_if_non_existent_method=True,
    **method_trans_spec,
):
    """
    Make a decorator that wraps specific methods, transforming specific argument values a nd output values.

    IMPORTANT: The decorator will by default return a copy of the class. This might incur some run time overhead.
    If this is desirable, for example, when you want to create several decorations of a same class.
    If you want to change the class itself (e.g. you're only loading it once in a module, and decorating it), then
    specify _return_a_copy_of_the_class=False

    :param _return_a_copy_of_the_class: Specifies whether to
        return a copy of the class (_return_a_copy_of_the_class=True, the default),
        or change the actual loaded class itself (_return_a_copy_of_the_class=False)
    :param method_trans_spec: method_name=trans_specs_for_method pairs.
        The trans_specs_for_method is a dict that is understood by transform_class_method_input_and_output.
        Except for one special case, it's keys are argument names and values are callables to call on those
        arguments' values.
        The special case is method_output_trans. This specifies that the callable it points to should be called
        on output of method. Here's one recipe for outputs: If the output of a function is an iterable and you want
        to apply a function trans to each element of the output, specify method_output_trans=lambda x: map(trans, x).
    :return: A wrapped class

    SEE ALSO:
        * mk_method_trans_spec_from_methods_specs_dict: a utility to make method_trans_spec more easily
        * transform_class_method_input_and_output: The function that is called for every method we wrap.

    In the following, we will show two examples.
    - The first is a toy example to demonstrate the basic functionality.
    - The second demonstrates a more involved case, but is still a silly example.
    - The third demonstrates more the type of application we'd use wrap_class_methods_input_and_output for in real life.

    # FIRST EXAMPLE
    We make an Ops class that wraps Counter, allowing one to add items and show the counts of items added.

    >>> from collections import UserDict
    >>> import re
    >>> from collections import Counter
    >>>
    >>> class Ops:
    ...     def __init__(self):
    ...         self.counter = Counter()
    ...     def add_item(self, item):
    ...         self.counter.update({item: 1})
    ...     def show(self):
    ...         return self.counter
    >>> # Here's an example of what Ops does
    >>> ops = Ops()
    >>> for item in ['this', 'is', 'that', 'and', 'that', 'is', 'this']:
    ...     ops.add_item(item)
    ...
    >>> ops.show()
    Counter({'this': 2, 'is': 2, 'that': 2, 'and': 1})
    >>>
    >>> # But say we don't want to count actual words added, but just the first two letters of these words,
    >>> # and say we want to show() to return the dict, not the Counter.
    >>> NewOps = wrap_class_methods_input_and_output(
    ...     _return_a_copy_of_the_class=False,
    ...     add_item=dict(item=lambda x: x[:2]),  # intercept items fed to add_item and keep only 2 first letters
    ...     show=dict(method_output_trans=dict)  # intercept output of show method, converting to dict
    ... )(Ops)
    >>> # let's try it out!
    >>> ops = NewOps()
    >>> for item in ['this', 'is', 'that', 'and', 'that', 'is', 'this']:
    ...     ops.add_item(item)
    ...
    >>> ops.show()
    {'th': 4, 'is': 2, 'an': 1}
    >>> # See that we specified _return_a_copy_of_the_class=False?
    >>> # Now look at what happens if we try to use Ops, the original class, again. It behaves like NewOps.
    >>> # That's usually not the behavior we want, so be careful!
    >>> ops = Ops()
    >>> for item in ['this', 'is', 'that', 'and', 'that', 'is', 'this']:
    ...     ops.add_item(item)
    ...
    >>> ops.show()
    {'th': 4, 'is': 2, 'an': 1}
    >>>
    >>>

    # SECOND EXAMPLE

    Wrap a dict (or rather, the safer collections.UserDict), doing weird things to the input and output
    keys and values

    >>> val_in_trans = lambda x: 'hello {}'.format(x)  # prepend "hello " to incoming values
    >>> val_out_trans = lambda x: re.sub('hello', 'hi', x)  # replace "hello" by "hi" in output values
    >>> key_in_trans = lambda x: '__' + x  # prepend incoming keys with double underscore
    >>> key_out_trans = lambda x: x[2:]  # remove the first two characters (underscores) from keys when output
    >>>
    >>> methods_specs_dict = {
    ...     ('__contains__', '__getitem__', '__setitem__', '__delitem__'): dict(key=key_in_trans),
    ...     '__setitem__': dict(item=val_in_trans),
    ...     '__iter__': dict(method_output_trans=lambda x: map(key_out_trans, x)),
    ...     '__getitem__': dict(method_output_trans=val_out_trans)
    ... }
    >>>
    >>> methods_specs_dict = mk_method_trans_spec_from_methods_specs_dict(methods_specs_dict)
    >>>
    >>> @wrap_class_methods_input_and_output(**methods_specs_dict)
    ... class AA(UserDict):
    ...     pass
    ...
    >>> aa = AA()
    >>> aa['foo'] = 'shoo'  # store 'shoo' under 'foo'
    >>> # the __str__ method isn't wrapped, so we see the actual STORED keys and values
    >>> # we see that __foo, not foo is the actual key, and "hello shoo" the value:
    >>> assert str(aa) == "{'__foo': 'hello shoo'}"
    >>> assert 'foo' in aa  # yet from the interface, it looks like 'foo' is a key of aa...
    >>> assert '__foo' not in aa  # ... and '__foo' is not a key.
    >>> aa['foo'] = 'bar'  # let's replace the value of 'foo'
    >>> assert str(aa) == "{'__foo': 'hello bar'}"  # see what's stored
    >>> aa['star'] = 'wars'  # let's add another
    >>> assert list(aa) == ['foo', 'star']  # what are the keys? (this uses __iter__ under the hood)
    >>> # In the following, we'll use methods keys(), values(), and items(), none of which we wrapped.
    >>> # And yet, they work as expected, since they pass on their work to methods we wrapped.
    >>> assert list(aa.keys()) == ['foo', 'star']  # another way to get keys
    >>> # see here that when we ask for values, we don't get what we asked to store, ...
    >>> # ... nor what is actually stored, but something else
    >>> assert list(aa.values()) == ['hi bar', 'hi wars']
    >>> assert str(list(aa.items())) == "[('foo', 'hi bar'), ('star', 'hi wars')]"  # the keys and values we get from items()
    >>> assert str(aa) == "{'__foo': 'hello bar', '__star': 'hello wars'}"  # what is actually stored
    >>> del aa['foo']  # testing deletion of a key
    >>> assert str(aa) == "{'__star': 'hello wars'}"  # it worked!
    >>>
    >>>

    # THIRD EXAMPLE

    Here again, we'll wrap UserDict. But instead of being silly, we'll pretend we need to store waveforms
    in binary format (so input values will have to be wrapped), but still retrieving these waveforms as lists
    (so output values will have to be wrapped).
    Additionally, we'll pretend we're working with wav files within some root directory, but don't
    want the root dir or the '.wav' extension to appear in our keys. So we'll have to wrap input and output keys.
    Of course, this is just pretend. Don't use this with real waveforms. It won't work.

    >>> root = '/ROOT/DIR/'
    >>> abs_path_of_rel_path = lambda rel_path: root + rel_path + '.wav'  # transform a relative path to an absolute one
    >>> rel_path_of_abs_path = lambda x: x.replace(root, '').replace('.wav', '')  # transform an absolute path to a relative one
    >>> list_to_bytes = bytes
    >>> bytes_to_list = list
    >>>
    >>> methods_specs_dict = {
    ...     ('__contains__', '__getitem__', '__setitem__', '__delitem__'): dict(key=abs_path_of_rel_path),
    ...     '__setitem__': dict(item=list_to_bytes),
    ...     '__iter__': dict(method_output_trans=lambda x: map(rel_path_of_abs_path, x)),
    ...     '__getitem__': dict(method_output_trans=bytes_to_list)
    ... }
    >>>
    >>> methods_specs_dict = mk_method_trans_spec_from_methods_specs_dict(methods_specs_dict)
    >>>
    >>> @wrap_class_methods_input_and_output(**methods_specs_dict)
    ... class Wf(UserDict):
    ...     pass
    ...
    >>> year = [2, 0, 1, 9]
    >>> down = [5, 4, 3, 2, 1]
    >>>
    >>> wf = Wf()
    >>> wf['year'] = year
    >>> print(str(wf).replace("b'", "'"))
    {'/ROOT/DIR/year.wav': '\\x02\\x00\\x01\\t'}
    >>> 'year' in wf
    True
    >>> wf['down'] = down
    >>> print(str(wf).replace("b'", "'"))
    {'/ROOT/DIR/year.wav': '\\x02\\x00\\x01\\t', '/ROOT/DIR/down.wav': '\\x05\\x04\\x03\\x02\\x01'}
    >>> list(wf.keys())
    ['year', 'down']
    >>> list(wf.values())
    [[2, 0, 1, 9], [5, 4, 3, 2, 1]]
    >>> list(wf.items())
    [('year', [2, 0, 1, 9]), ('down', [5, 4, 3, 2, 1])]
    >>> len(wf)
    2
    >>> del wf['year']
    >>> len(wf)
    1
    >>> list(wf.items())
    [('down', [5, 4, 3, 2, 1])]
    """

    wrapper_for_method = {
        method: mk_input_and_output_method_wrapper(**method_trans)
        for method, method_trans in method_trans_spec.items()
    }
    return wrap_class_methods(
        _return_a_copy_of_the_class=_return_a_copy_of_the_class,
        _raise_error_if_non_existent_method=True,
        **wrapper_for_method,
    )

    # def class_wrapper(cls):
    #     if _return_a_copy_of_the_class:
    #         class _cls(cls):
    #             pass
    #     else:
    #         _cls = cls
    #     for method, method_trans in method_trans_spec.items():
    #         if hasattr(_cls, method):
    #             transform_class_method_input_and_output(_cls, method, **method_trans)
    #         elif _raise_error_if_non_existent_method:
    #             if hasattr(cls, '__name__'):
    #                 class_name = cls.__name__
    #             else:
    #                 class_name = str(cls)
    #             raise ValueError("{} has no '{}' method!".format(class_name, method))
    #     return _cls
    #
    # return class_wrapper


def add_method(obj, method_func, method_name=None, class_name=None):
    """
    Dynamically add a method to an object.

    :param obj: The object to add a method to
    :param method_func: The function to use as a method. The first argument must be the object itself
        (usually called self)
    :param method_name: The desired function name. If None, will take method_func.__name__
    :param class_name:  The desired class name. If None, will take type(obj).__name__
    :return: the object, but with the additional method (or a different function for it)

    >>> class A:
    ...     def __init__(self, x=10):
    ...         self.x = x
    >>> def times(self, y):
    ...     return self.x * y
    >>> def plus(self, y):
    ...     return self.x + y
    >>> a = A(x=10)
    >>> a = add_method(a, plus, '__call__')  # add a __call__ method, assigning it to plus
    >>> a(2)
    12
    >>> a = add_method(a, times, '__call__')  # reassign the __call__ method to times instead
    >>> a(2)
    20
    >>> a = add_method(a, plus, '__getitem__')  # assign the method __getitem__ to plus
    >>> a[2]  # see that it works
    12
    >>> a(2)  # and that we still have our __call__ method
    20
    """
    if isinstance(method_func, str):
        method_name = method_func
        method_func = getattr(obj, method_name)
    if method_name is None:
        method_name = method_func.__name__

    base = type(obj)

    if class_name is None:
        class_name = base.__name__
    bases = (base.__bases__[1:]) + (base,)
    bases_names = set(map(lambda x: x.__name__, bases))
    if class_name in bases_names:
        for i in range(6):
            class_name += "_"
            if not class_name in bases_names:
                break
        else:
            raise ValueError(
                "can't find a name for class that is not taken by bases. Consider using explicit name"
            )

    new_keys = set(dir(obj)) - set(chain(*[dir(b) for b in bases]))

    d = {a: getattr(obj, a) for a in new_keys}
    d[method_name] = method_func

    return type(class_name, bases, d)()


def transform_instance_method_input_and_output(
    obj, method, method_output_trans=None, **arg_trans
):
    from warnings import warn

    warn("Not sure transform_instance_method_input_and_output works yet")
    wrapped_method = transform_args(**arg_trans)(getattr(type(obj), method))
    if method_output_trans is not None:
        obj = add_method(
            obj,
            wrap_method_output(method_output_trans)(wrapped_method),
            method_name=method,
        )
    else:
        obj = add_method(obj, wrapped_method, method_name=method)
    return obj


def wrap_instance_methods(
    _return_a_copy_of_the_class=True,
    _raise_error_if_non_existent_method=True,
    **method_trans_spec,
):
    def obj_wrapper(obj):
        for method, method_trans in method_trans_spec.items():
            if hasattr(obj, method):
                obj = transform_instance_method_input_and_output(
                    obj, method, **method_trans
                )
            elif _raise_error_if_non_existent_method:
                if hasattr(obj.__class__, "__name__"):
                    class_name = obj.__name__
                else:
                    class_name = str(obj)
                raise ValueError(f"{class_name} has no '{method}' method!")
        return obj

    return obj_wrapper


def mk_method_trans_spec_from_methods_specs_dict(methods_specs_dict):
    """
    Utility to make inputs for wrap_class_methods_input_and_output more easily.
    :param methods_specs_dict: a dict where
        keys are method names (either a single string, or a tuple of strings)
        values are the trans_spec dicts that should be associated to those methods
    :return: A dict in the method_trans_spec (input of wrap_class_method) format.

    >>> methods_specs_dict = {}
    >>> methods_specs_dict['foo'] = {'x': str, 'y': int}
    >>> methods_specs_dict[('foo', 'bar')] = {'z': list, 'method_output_trans': float}
    >>> methods_specs_dict[('bar', )] = {'zz': int}
    >>> method_trans_spec = mk_method_trans_spec_from_methods_specs_dict(methods_specs_dict)
    >>> list(method_trans_spec.keys())
    ['foo', 'bar']
    >>> method_trans_spec['foo']
    {'x': <class 'str'>, 'y': <class 'int'>, 'z': <class 'list'>, 'method_output_trans': <class 'float'>}
    >>> method_trans_spec['bar']
    {'z': <class 'list'>, 'method_output_trans': <class 'float'>, 'zz': <class 'int'>}
    """
    method_trans_spec = defaultdict(dict)
    for methods, specs in methods_specs_dict.items():
        if isinstance(methods, str):
            methods = (methods,)
        for method in methods:
            method_trans_spec[method].update(specs)
    return dict(method_trans_spec)


Args = tuple
Kwargs = dict
WhatToLog = Callable[[Callable, Args, Kwargs], Any]


def _special_str(x: Any, max_len=100) -> str:
    """A util function for _call_signature"""
    if isinstance(x, str):
        return "'" + x + "'"
    else:
        x_str = str(x)
        if len(x_str) > max_len:
            type_str = getattr(type(x), "__name__", str(type(x)))
            if hasattr(x, "__repr__"):
                value_str = x.__repr__()
            else:
                value_str = x_str
            x_str = f"{type_str}({value_str[:20]}...)"
        return x_str


def _call_signature(func: Callable, args: Args, kwargs: Kwargs) -> str:
    """
    A util to make a string representation of a call of a function func with given args and kwargs.
    Meant to be the default mk_log_str of mk_call_logger.
    :param func: A callable
    :param args: A tuple of positional arguments
    :param kwargs: A dict of key=val arguments
    :return: A string to represent all of that.

    >>> args = (2, 'sdf', list(range(1000)))
    >>> kwargs = {'z': 'boo', 'zzz': 10}
    >>> print(_call_signature(_call_signature, args, kwargs))
    _call_signature(2, 'sdf', list([0, 1, 2, 3, 4, 5, 6...), z='boo', zzz=10)
    """
    args_signature = ", ".join(map(_special_str, args))
    kwargs_signature = ", ".join((f"{k}={_special_str(v)}" for k, v in kwargs.items()))
    return "{func_name}({signature})".format(
        func_name=func.__name__,
        signature=", ".join([args_signature, kwargs_signature]),
    )


def mk_call_logger(
    logger=print,
    what_to_log: WhatToLog = _call_signature,
    log_output=False,
    func_is_bounded=False,
):
    """
    Makes a decorator that logs each call to the wrapped function.
    :param logger: The actual function that logs stuff. Default is print. The "stuff" it logs is given by
        the what_to_log argument (a function).
    :param what_to_log: A function taking inputs (func, args, kwargs) of the call, and returning something to log
        (usually, and by default, a string)
    :param func_is_bounded: Whether the function is bounded (like a method) or not
    :return: A decorator

    >>> # Example of use on (unbounded) function, with default args
    >>> @mk_call_logger()
    ... def useless_computation(x, y=2, z='foo'):
    ...     return z * (x + y)
    ...
    >>> _ = useless_computation(3, y=1, z='ha')
    useless_computation(3, y=1, z='ha')

    The same example, but with output logging too

    >>> @mk_call_logger(log_output=True)
    ... def useless_computation(x, y=2, z='foo'):
    ...     return z * (x + y)
    >>> _ = useless_computation(3, y=1, z='ha')
    useless_computation(3, y=1, z='ha')
    -> hahahaha

    And now a bit more involved...

    >>>
    >>> # Example of use on class method, with a different what_to_log function.
    >>> class A:
    ...     def __init__(self, a=10):
    ...         self.a = a
    ...     def add(self, x):
    ...         return self.a + x
    ...     def multiply(self, x):
    ...         return self.a * x
    ...
    >>> def _name_args_kwargs(func, args, kwargs) -> str:
    ...     return "Calling {} with\\n  args={}\\n  kwargs={}".format(func.__name__, args, kwargs)
    ...
    >>>
    >>> log_calls = mk_call_logger(what_to_log=_name_args_kwargs, func_is_bounded=True)
    >>> for method in ['add', 'multiply']:
    ...     A_method = getattr(A, method)
    ...     setattr(A, method, mk_call_logger(what_to_log=_name_args_kwargs, func_is_bounded=True)(A_method))
    ...
    >>>
    >>> a = A()
    >>> a.add(x=2)
    Calling add with
      args=()
      kwargs={'x': 2}
    12
    >>> a.multiply(2)
    Calling multiply with
      args=(2,)
      kwargs={}
    20
    """
    if log_output is True:
        log_output = "-> {}".format
    assert log_output is False or callable(log_output)

    if not func_is_bounded:

        def log_calls(func):
            @wraps(func)
            def _func(*args, **kwargs):
                logger(what_to_log(func, args, kwargs))
                out = func(*args, **kwargs)
                log_output and logger(log_output(out))
                return out

            _func._logged_with = logger

            return _func

    else:

        def log_calls(func):
            @wraps(func)
            def _func(self, *args, **kwargs):
                logger(what_to_log(func, args, kwargs))
                out = func(self, *args, **kwargs)
                log_output and logger(log_output(out))
                return out

            _func._logged_with = logger

            return _func

    return log_calls


def get_callable_from_factory_if_no_arguments(func_or_factory_thereof: Callable):
    """Will return the input itself if it's a callable with at least one argument.
    If not, it will consider it to be a factory, call it to get the actual
    callable object that the user presumably is seeking to get"""
    assert callable(
        func_or_factory_thereof
    ), f"{func_or_factory_thereof} needs to be callable"
    if len(Sig(func_or_factory_thereof)) == 0:
        # if func_or_factory_thereof has no arguments, assume it's a factory
        func = func_or_factory_thereof()
        # and make sure that now we have arguments
        if not isinstance(func, Callable) or not len(Sig(func)) > 0:
            raise ValueError(
                "Your func_or_factory_thereof had no arguments, so I assumed it "
                "was a factory, called it, but that didn't produce a "
                "callable with at least one argument. So I don't know what to do."
            )
    else:
        func = func_or_factory_thereof
    return func


## This one didn't actually handle position only correctly (just signature)
# def old_ch_func_to_all_pk(func):
#     """Returns a copy of the function where all arguments are of the PK kind.
#     (PK: Positional_or_keyword)
#
#     :param func: A callable
#     :return:
#
#     >>> from py2http.decorators import signature, ch_func_to_all_pk
#     >>>
#     >>> def f(a, /, b, *, c=None, **kwargs): ...
#     ...
#     >>> print(signature(f))
#     (a, /, b, *, c=None, **kwargs)
#     >>> ff = old_ch_func_to_all_pk(f)
#     >>> print(signature(ff))
#     (a, b, c=None, **kwargs)
#     >>> def g(x, y=1, *args, **kwargs): ...
#     ...
#     >>> print(signature(g))
#     (x, y=1, *args, **kwargs)
#     >>> gg = old_ch_func_to_all_pk(g)
#     >>> print(signature(gg))
#     (x, y=1, args=(), **kwargs)
#     """
#     func = tuple_the_args(func)
#     sig = signature(func)
#     func.__signature__ = ch_signature_to_all_pk(sig)
#     return func
```

## doc_mint.py

```python
"""Meta-interfaces"""

from typing import Literal, List, Dict, Tuple, Union
from collections.abc import Callable
import re
import ast

nan = float("nan")

# --------------------------------------------------------------------------------------


def find_in_params(
    query: str, params: Callable | str, *, search_in=("name", "description")
) -> list[dict]:
    """
    Find parameters in a list of parameter specifications.

    :param query: The query to search for.
    :param params: The list of parameter specifications.
        Params can be provided in two formats:
        - A function, from which the params will be extracted from the docstring.
        - A list of dictionaries where each dictionary specifies a parameter, containing:
            - name: The name of the parameter (str).
            - default: The default value of the parameter (any, optional).
            - annotation: The type annotation for the parameter (str, optional).
            - description: A description of the parameter (str).
        If a callable is provided, it will be used to generate the list of parameter specifications.
    :param search_in: The fields to search in each parameter specification.
    :return: A list of parameter specifications that match the query.

    Examples:

    >>> params = [
    ...     {"name": "x", "default": 1, "annotation": "int", "description": "An integer value."},
    ...     {"name": "y", "default": None, "annotation": "str", "description": "An optional string."},
    ... ]
    >>> find_in_params('int', params)
    [{'name': 'x', 'default': 1, 'annotation': 'int', 'description': 'An integer value.'}]

    """
    if isinstance(search_in, str):
        search_in = (search_in,)
    if isinstance(params, str) or callable(params):
        params = docstring_to_params(params)
    if isinstance(query, str):
        query = re.compile(query, re.IGNORECASE)
    return list(
        filter(lambda x: any(query.search(x.get(key, "")) for key in search_in), params)
    )


def indent_lines(string: str, indent: str) -> str:
    r"""
    Indent each line of a string.

    :param string: The string to indent.
    :param indent: The string to use for indentation.
    :return: The indented string.

    Examples:

    >>> print(indent_lines('This is a test.\nAnother line.', ' ' * 8))
            This is a test.
            Another line.
    """
    return "\n".join(indent + line for line in string.split("\n"))


def most_common_indent(string: str, ignore_first_line=True) -> str:
    r"""
    Find the most common indentation in a string.

    :param string: The string to analyze.
    :param ignore_first_line: Whether to ignore the first line when determining the
        indentation. Default is True since the first line often has no indentation
        because of the way python strings appear in code.
    :return: The most common indentation string.

    Examples:

    >>> most_common_indent('    This is a test.\n    Another line.')
    '    '
    """
    indents = re.findall(r"^( *)\S", string, re.MULTILINE)
    n_lines = len(indents)
    if ignore_first_line and n_lines > 1:
        # if there's more than one line, ignore the indent of the first
        # (This is is because of the way docstrings are often formatted)
        indents = indents[1:]
    return max(indents, key=indents.count)


def inject_docstring_content(to_inject, *, position=-1, indent=True):
    r"""
    Inject content into the docstring of a function.

    Note: If you use the decorator on a string, it will assume that string is the doc
    string you want to transform and return the modified string directly.

    :param to_inject: The content to inject.
    :param position: The position in the docstring to inject the content.
        If an integer, the content is injected at that line number (pushing the rest down).
        If a string, will consider it as a regex pattern to match the line to inject after.
        Default is -1, to inject at the end.
    :param indent: Control on indent.
        If True, will use the most common indent of the input docstrings.
        If a string, it will use that specific string.
    :return: A decorator that injects the content into the docstring of the decorated function.

    Examples:

    >>> @inject_docstring_content('This is a test.')
    ... def test_func():
    ...     '''This is the docstring.'''
    ...     pass
    >>> test_func.__doc__
    'This is the docstring.\nThis is a test.'
    >>> @inject_docstring_content('This is a test.', position='###INSERT HERE###')
    ... def test_func():
    ...     '''This is the docstring.
    ...     ###INSERT HERE###
    ...     More blah.
    ...     '''
    ...     pass
    >>> test_func.__doc__
    'This is the docstring.\n    ###INSERT HERE###\n    More blah.\n    '
    """

    if indent is True:
        indent = most_common_indent(to_inject)
    else:
        indent = indent or ""

    def decorator(func):
        input_was_docstr_itself = False

        if isinstance(func, str):
            doc = func
            input_was_docstr_itself = True
        else:
            doc = func.__doc__ or ""

        lines = doc.split("\n")
        # TODO: Could figure out indent (ignoring first line), to inform the injected content indent

        if isinstance(position, int):
            if position == -1:
                lines.append(to_inject)
            else:
                lines.insert(position, to_inject)
        elif isinstance(position, str):
            for i, line in enumerate(lines):
                if re.match(position, line):
                    lines.insert(i + 1, to_inject)
                    break

        new_doc = "\n".join(lines)

        if input_was_docstr_itself:
            return new_doc
        else:
            func.__doc__ = new_doc
            return func

    return decorator


# TODO: params_to_docstring and docstring_to_params are a parse/generate pair, with echoes of embody and routing techniques.
def params_to_docstring(
    params: list[dict],
    *,
    doc_style: str = "numpy",
    take_name_of_types: bool = False,
    quote_string_defaults: bool = True,
) -> str:
    """
    Generate a docstring from a list of parameter specifications.

    :param params: A list of dictionaries where each dictionary specifies a parameter.
        Each dictionary should contain:
          - name: The name of the parameter (str).
          - default: The default value of the parameter (any, optional).
          - annotation: The type annotation for the parameter (str, optional).
          - description: A description of the parameter (str).
    :param doc_style: The style of the docstring to generate. One of 'numpy', 'google', or 'rest'.
    :param take_name_of_types: Whether to use the name of the type as the annotation (bool).
    :param quote_string_defaults: Whether to quote string defaults (bool).

    :return: A formatted docstring (str).

    Examples:

    >>> params = [
    ...     {"name": "x", "default": 1, "annotation": "int", "description": "An integer value."},
    ...     {"name": "y", "default": None, "annotation": "str", "description": "An optional string."},
    ... ]
    >>> print(params_to_docstring(params))  # doctest: +NORMALIZE_WHITESPACE
    Parameters
    ----------
    x : int, default=1
        An integer value.
    y : str, default=None
        An optional string.
    <BLANKLINE>
    >>> print(params_to_docstring(params, doc_style='google'))  # doctest: +NORMALIZE_WHITESPACE
    Args:
        x (int, optional): An integer value. Defaults to 1.
        y (str, optional): An optional string. Defaults to None.
    <BLANKLINE>
    >>> print(params_to_docstring(params, doc_style='rest'))  # doctest: +NORMALIZE_WHITESPACE
    :param x: An integer value. (Default: 1)
    :type x: int
    :param y: An optional string. (Default: None)
    :type y: str
    <BLANKLINE>
    """

    # Preprocess parameters to a uniform structure
    def processed_params():
        for p in params:
            name = p["name"]
            annotation = p.get("annotation", "")
            if (
                take_name_of_types
                and isinstance(annotation, type)
                and hasattr(annotation, "__name__")
            ):
                annotation = annotation.__name__
            description = p.get("description", "")
            if not description or isinstance(description, float):  # float to catch nans
                description = ""
            default = p.get("default", None)
            if quote_string_defaults and isinstance(default, str):
                default = f'"{default}"'
            optional = "default" in p

            yield {
                "name": name,
                "annotation": annotation,
                "description": description,
                "default": default,
                "optional": optional,
            }

    def numpy_lines(params):
        yield "Parameters"
        yield "----------"
        for param in params:
            line = param["name"]
            if param["annotation"]:
                line += f" : {param['annotation']}"
            if param["optional"]:
                line += f", default={param['default']}"
            yield line
            yield f"    {param['description']}"

    def google_lines(params):
        yield "Args:"
        for param in params:
            line = f"    {param['name']}"
            if param["annotation"]:
                line += f" ({param['annotation']}"
                if param["optional"]:
                    line += ", optional"
                line += ")"
            elif param["optional"]:
                line += " (optional)"
            desc = param["description"]
            if param["optional"]:
                desc += f" Defaults to {param['default']}."
            line += f": {desc}"
            yield line

    def rest_lines(params):
        for param in params:
            desc = param["description"]
            if param["optional"]:
                desc += f" (Default: {param['default']})"
            yield f":param {param['name']}: {desc}"
            if param["annotation"]:
                yield f":type {param['name']}: {param['annotation']}"

    formatters = {
        "numpy": numpy_lines,
        "google": google_lines,
        "rest": rest_lines,
    }

    if doc_style not in formatters:
        raise ValueError(f"Unsupported doc_style: {doc_style}")

    lines = formatters[doc_style](processed_params())
    return "\n".join(lines) + "\n"


def string_param_to_obj(string_to_object_mapping: dict, string=None):
    """
    Convert a string representation of a parameter to an object.

    Use Case: When parsing a docstring, you get values as strings, but these values
    may need to be converted to their actual object types
    (in the case of default and annotatios for example).
    This is a helper function to do that conversion.

    :param string: The string representation of the parameter.
    :param string_to_object_mapping: A mapping from string representations to objects.
    :return: The object corresponding to the string representation.

    Examples:

    >>> string_to_object_mapping = {
    ...     'None': None,
    ...     'list': list,
    ...     'int': int,
    ... }
    >>> string_to_obj = string_param_to_obj(string_to_object_mapping)
    >>> string_to_obj('None')
    >>> string_to_obj('list')
    <class 'list'>
    >>> string_to_obj('int')
    <class 'int'>
    >>> string_to_obj('not something listed')
    'not something listed'
    """
    if string is None:
        return partial(string_param_to_obj, string_to_object_mapping)

    if string in string_to_object_mapping:
        return string_to_object_mapping[string]
    else:
        # TODO: Should handle strings that are numeric values...

        # if all else fails, return the string itself
        return string


dflt_str_to_obj_mapping = {
    "None": None,
    "True": True,
    "False": False,
    "list": list,
    "int": int,
    "float": float,
    "str": str,
    "dict": dict,
    "tuple": tuple,
    "set": set,
    "bool": bool,
    "complex": complex,
    "nan": nan,
    "inf": float("inf"),
    "-inf": float("-inf"),
}

_MAX_LENGTH_FOR_LITERAL_EVAL = 1000


def literal_eval_converter(s: str, max_length=_MAX_LENGTH_FOR_LITERAL_EVAL):
    if len(s) > max_length:  # Restrict long strings for extra safety
        return None
    elif "\n" in s or "\r" in s or ";" in s:  # extra safety
        return None
    try:
        return ast.literal_eval(s)
    except (ValueError, SyntaxError):
        return None


def register_converter(converter):
    """
    Register a new converter. A converter can be:
      - A dict: { "None": None, "int": int, ... }
      - A callable: lambda s: attempt to parse s and return object or None
    """
    dflt_str_to_obj_converters.append(converter)


dflt_str_to_obj_converters = []
register_converter(dflt_str_to_obj_mapping)
register_converter(literal_eval_converter)


def convert_string(s: str, converters: list[dict | Callable]) -> object:
    for converter in converters:
        # If converter is a dict
        if isinstance(converter, dict):
            if s in converter:
                return converter[s]

        # If converter is a callable
        else:
            try:
                result = converter(s)
                if result is not None:
                    return result
            except Exception:
                # If a callable fails, just skip it
                pass

    # If no converter matched
    return s


def docstring_to_params(
    docstring: str,
    *,
    doc_style: Literal["numpy", "google", "rest"] = "numpy",
    converters: list = dflt_str_to_obj_converters,
) -> list[dict]:
    """
    Parse a docstring and extract parameter specifications.

    :param docstring: The docstring to parse.
    :param doc_style: The style of the docstring to parse. One of 'numpy', 'google', or 'rest'.

    :return: A list of parameter specifications, where each specification is a dictionary containing:
        - name: The name of the parameter (str).
        - default: The default value of the parameter (str, optional).
        - annotation: The type annotation for the parameter (str, optional).
        - description: A description of the parameter (str).

    Examples:

    >>> docstring = '''
    ... Parameters
    ... ----------
    ... x : int, default=1
    ...     An integer value.
    ... y : str, default=None
    ...     An optional string.
    ... '''
    >>> params = docstring_to_params(docstring)
    >>> params  == [
    ...     {'name': 'x', 'default': 1, 'annotation': int, 'description': 'An integer value.'},
    ...     {'name': 'y', 'default': None, 'annotation': str, 'description': 'An optional string.'}
    ... ]
    True


    >>> docstring = '''
    ... Args:
    ...     x (int, optional): An integer value. Defaults to 1.
    ...     y (str, optional): An optional string. Defaults to None.
    ... '''
    >>> params = docstring_to_params(docstring, doc_style='google')
    >>> params == [
    ...     {"name": "x", "default": 1, "annotation": int, "description": "An integer value."},
    ...     {"name": "y", "default": None, "annotation": str, "description": "An optional string."},
    ... ]
    True

    >>> docstring = '''
    ... :param x: An integer value. (Default: 1)
    ... :type x: int
    ... :param y: An optional string. (Default: None)
    ... :type y: str
    ... '''
    >>> params = docstring_to_params(docstring, doc_style='rest')
    >>> params == [
    ...     {"name": "x", "default": 1, "annotation": int, "description": "An integer value."},
    ...     {"name": "y", "default": None, "annotation": str, "description": "An optional string."},
    ... ]
    True
    """
    if not isinstance(docstring, str) and callable(docstring):
        docstring = docstring.__doc__ or ""

    # Extract parameter lines
    param_lines = _extract_params_section(docstring, doc_style)

    # Parse parameter lines
    if doc_style == "numpy":
        params = _parse_numpy_param_lines(param_lines)
    elif doc_style == "google":
        params = _parse_google_param_lines(param_lines)
    elif doc_style == "rest":
        params = _parse_rest_param_lines(param_lines)
    else:
        raise ValueError(f"Unsupported doc_style: {doc_style}")

    def convert_param(param):
        param["default"] = convert_string(param["default"], converters)
        param["annotation"] = convert_string(param["annotation"], converters)
        return param

    params = list(map(convert_param, params))

    return params


def _extract_params_section(docstring: str, doc_style: str) -> list[str]:
    lines = docstring.strip().split("\n")
    if doc_style == "numpy":
        # Find "Parameters" section
        try:
            param_index = lines.index("Parameters")
            # Next line should be a separator like '----------'
            separator_line = lines[param_index + 1]
            if not re.match(r"-{3,}", separator_line.strip()):
                raise ValueError(
                    "Expected a separator after 'Parameters' in Numpy docstring"
                )
            param_lines = lines[param_index + 2 :]
        except ValueError:
            # "Parameters" section not found
            return []
    elif doc_style == "google":
        # Find "Args:" section
        try:
            param_index = lines.index("Args:")
            param_lines = lines[param_index + 1 :]
        except ValueError:
            # "Args:" section not found
            return []
    elif doc_style == "rest":
        param_lines = lines
    else:
        raise ValueError(f"Unsupported doc_style: {doc_style}")
    return param_lines


def _collect_indented_lines(
    param_lines: list[str], start_index: int, indent_level: int
) -> tuple[str, int]:
    description_lines = []
    i = start_index
    while i < len(param_lines) and (
        param_lines[i].startswith(" " * indent_level) or param_lines[i].strip() == ""
    ):
        description_lines.append(param_lines[i].strip())
        i += 1
    description = " ".join(description_lines).strip()
    return description, i


def _parse_numpy_param_lines(param_lines: list[str]) -> list[dict]:
    params = []
    i = 0
    while i < len(param_lines):
        line = param_lines[i]
        if not line.strip():
            i += 1
            continue
        # Match parameter line: name : type, default=value
        param_match = re.match(
            r"^(\w+)\s*:\s*([^,]+)(?:,\s*default=(.+))?", line.strip()
        )
        if param_match:
            name = param_match.group(1)
            annotation = param_match.group(2).strip()
            default = param_match.group(3).strip() if param_match.group(3) else None
            i += 1
            # Collect description lines
            description, i = _collect_indented_lines(param_lines, i, indent_level=4)
            param = {
                "name": name,
                "default": default,
                "annotation": annotation,
                "description": description,
            }
            params.append(param)
        else:
            i += 1
    return params


def _parse_google_param_lines(param_lines: list[str]) -> list[dict]:
    params = []
    i = 0
    while i < len(param_lines):
        line = param_lines[i]
        if not line.strip():
            i += 1
            continue
        # Match parameter line: name (type, optional): description
        param_match = re.match(r"^ {4}(\w+)\s*(?:\(([^)]+)\))?:\s*(.+)", line)
        if param_match:
            name = param_match.group(1)
            type_default = param_match.group(2)
            description = param_match.group(3)
            annotation = None
            default = None
            if type_default:
                # Parse type and optional
                type_parts = type_default.split(",")
                annotation = type_parts[0].strip()
                if "optional" in [part.strip() for part in type_parts[1:]]:
                    # Extract default from description if present
                    default_match = re.search(r"Defaults to (.+)\.", description)
                    if default_match:
                        default = default_match.group(1).strip()
                        description = re.sub(
                            r"Defaults to .+\.", "", description
                        ).strip()
            i += 1
            # Collect additional description lines
            extra_description, i = _collect_indented_lines(
                param_lines, i, indent_level=8
            )
            if extra_description:
                description += " " + extra_description
            param = {
                "name": name,
                "default": default,
                "annotation": annotation,
                "description": description.strip(),
            }
            params.append(param)
        else:
            i += 1
    return params


def _parse_rest_param_lines(param_lines: list[str]) -> list[dict]:
    params_dict = {}
    i = 0
    while i < len(param_lines):
        line = param_lines[i].strip()
        # Match ':param name: description'
        param_match = re.match(r"^:param (\w+):\s*(.+)", line)
        if param_match:
            name = param_match.group(1)
            description = param_match.group(2)
            # Check for default value in description
            default_match = re.search(r"\(Default:\s*(.+)\)", description)
            default = default_match.group(1).strip() if default_match else None
            if default:
                description = re.sub(r"\(Default:\s*.+\)", "", description).strip()
            # Initialize parameter entry
            params_dict[name] = {
                "name": name,
                "default": default,
                "annotation": None,
                "description": description,
            }
            i += 1
        # Match ':type name: type'
        elif line.startswith(":type"):
            type_match = re.match(r"^:type (\w+):\s*(.+)", line)
            if type_match:
                name = type_match.group(1)
                annotation = type_match.group(2).strip()
                if name in params_dict:
                    params_dict[name]["annotation"] = annotation
                else:
                    # Type without param, create entry
                    params_dict[name] = {
                        "name": name,
                        "default": None,
                        "annotation": annotation,
                        "description": "",
                    }
                i += 1
            else:
                i += 1
        else:
            i += 1
    params = list(params_dict.values())
    return params


# --------------------------------------------------------------------------------------
# Older stuff. TODO: Clean up and remove what's not needed

import doctest
from collections.abc import Callable
import re
from itertools import groupby
from inspect import getdoc

MAX_LINE_LENGTH = 72  # https://en.wikipedia.org/wiki/Characters_per_line


def _prefix_lines(s: str, prefix: str = "# ", even_if_empty: bool = False) -> str:
    r"""
    Prefix every line of s with given prefix.

    :param s: String whose lines you want to prefix.
    :param prefix: Desired prefix string. Default is '# ', to have the effect of "commenting out" line
    :param even_if_empty: Whether to prefix empty strings or not.
    :return: A string whose lines have been prefixed.

    >>> _prefix_lines('something to comment out')
    '# something to comment out'
    >>> _prefix_lines('A line you want to prefix', prefix='PREFIX: ')
    'PREFIX: A line you want to prefix'
    >>> print(_prefix_lines('What happens\nif the thing to comment out\nhas multiple lines?'))
    # What happens
    # if the thing to comment out
    # has multiple lines?
    >>> _prefix_lines('')  # see that an empty string is returned as is
    ''
    >>> _prefix_lines('', even_if_empty=True)  # unless you ask for it
    '# '
    """
    if not even_if_empty:
        if len(s) == 0:
            return s
    return "\n".join(map(lambda x: prefix + x, s.split("\n")))


class ExampleX(doctest.Example):
    """doctest.Example eXtended to have more convenient methods"""

    source_prefix = ">>> "
    source_continuation = "... "

    def __init__(
        self,
        source,
        want=None,
        exc_msg=None,
        lineno=0,
        indent=0,
        options=None,
    ):
        # if source is already a doctest.Example instance, use its properties as args
        if isinstance(source, doctest.Example):
            o = source
            source, want, exc_msg, lineno, indent, options = (
                o.source,
                o.want,
                o.exc_msg,
                o.lineno,
                o.indent,
                o.options,
            )
        super().__init__(source, want, exc_msg, lineno, indent, options)

    @property
    def indent_str(self):
        return self.indent * " "

    @property
    def _source_str(self):
        indented_continuation = f"{self.indent_str}{self.source_continuation}"
        t = self.source.replace("\n", f"\n{indented_continuation}")
        # remove the trailing '...' if present
        if t.endswith(indented_continuation):
            t = t[: -len(indented_continuation)]
        return t

    def __str__(self):
        want = self.want
        if self.want:
            want = self.indent_str + want
        return self.indent_str + self.source_prefix + self._source_str + want

    def __repr__(self):
        return str(self)


from collections.abc import Sequence


class DoctestBlock(list):
    """A list that (should) contain doctest Example instances"""

    def __init__(self, seq=()):
        super().__init__(map(ExampleX, seq))

    def __str__(self):
        return "".join(map(str, self))

    def __repr__(self):
        return f"<DoctestBlock with {len(self)} examples>\n" + str(self)


parse_doctest = doctest.DocTestParser().parse


# TODO: Newer, using doctest parser. See what can be merged
#  with non_doctest_lines etc.
def split_text_and_doctests(doc_string: str):
    r"""
    Generates alternating blocks of "text" (string) and "doctest blocks"
    (``DoctestBlock`` instances, which are essentially a list of ``ExampleX`` instances).

    >>> example = '''
    ...     This is to test the doctest splitter.
    ...     Until now, we're in a text block.
    ...     The following is a doctest block:
    ...
    ...     >>> 2 + 3
    ...     5
    ...     >>> t = 5
    ...     >>> tt = 10
    ...
    ...     This is another text block, followed with another doctest block:
    ...
    ...     >>> def foo():
    ...     ...     return 42
    ...     >>> foo()
    ...     42
    ...
    ... '''
    >>>
    >>> blocks = list(split_text_and_doctests(example))

    There are 5 blocks:

    >>> len(blocks)
    5

    The first block is a string, corresponding to explanatory text of the doc string:

    >>> isinstance(blocks[0], str)
    True
    >>> print(blocks[0])
    <BLANKLINE>
    This is to test the doctest splitter.
    Until now, we're in a text block.
    The following is a doctest block:
    <BLANKLINE>
    <BLANKLINE>

    The next block is a ``DoctestBlock`` instance.

    >>> block = blocks[1]
    >>> isinstance(block, DoctestBlock)
    True

    This block has 3 elements (``ExampleX`` instances)

    >>> len(block)
    3

    If you ask for the string representation of this block, you'll get a doctest string:

    >>> str(block)
    '    >>> 2 + 3\n    5\n    >>> t = 5\n    >>> tt = 10\n'

    """

    def is_string(item):
        if isinstance(item, str):
            if item == "":
                return False
            else:
                return True
        else:
            return False

    t = parse_doctest(doc_string)
    g = groupby(t, key=is_string)
    for _is_string, group_data in g:
        if _is_string:
            yield "\n".join(list(group_data))
        else:
            yield DoctestBlock(filter(None, group_data))


comment_strip_p = re.compile(r"(?m)^ *#.*\n?")
doctest_line_p = re.compile(r"\s*>>>")
empty_line = re.compile(r"\s*$")


def non_doctest_lines(doc):
    r"""Generator of lines of the doc string that are not in a doctest scope.

    >>> def _test_func():
    ...     '''Line 1
    ...     Another
    ...     >>> doctest_1
    ...     >>> doctest_2
    ...     line_after_a_doc_test
    ...     another_line_that_is_in_the_doc_test scope
    ...
    ...     But now we're out of a doctest's scope
    ...
    ...     >>> Oh no, another doctest!
    ...     '''
    >>> from inspect import getdoc
    >>>
    >>> list(non_doctest_lines(getdoc(_test_func)))
    ['Line 1', 'Another', "But now we're out of a doctest's scope", '']

    :param doc:
    :return:
    """
    last_line_was_a_doc_test = False
    for line in doc.splitlines():
        if not doctest_line_p.match(line):
            if not last_line_was_a_doc_test:
                yield line
                last_line_was_a_doc_test = False
            else:
                if empty_line.match(line):
                    last_line_was_a_doc_test = False
        else:
            last_line_was_a_doc_test = True


def strip_comments(code):
    code = str(code)
    return comment_strip_p.sub("", code)


def mk_example_wants_callback(source_want_func: Callable[[str, str], Callable]):
    def example_wants_callback(example, *args, **kwargs):
        want = example.want.strip()
        if want:
            source = example.source.strip()
            return source_want_func(source, want, *args, **kwargs)
        else:
            return example.source

    return example_wants_callback


def split_line_comments(s):
    t = s.split("#")
    if len(t) == 1:
        comment = ""
    else:
        s, comment = t
    return s, comment


def _assert_wants(source, want, wrap_func_name=None):
    is_a_multiline = len(source.split("\n")) > 1

    if not is_a_multiline:
        source, comment = split_line_comments(source)
        if wrap_func_name is None:
            t = f"({source}) == {want} #{comment}"
        else:
            t = f"{wrap_func_name}({source}) == {wrap_func_name}({want}) #{comment}"
        if "'" in t and not '"' in t:
            strchr = '"'
            return "assert {t}, {strchr}{t}{strchr}".format(t=t, strchr=strchr)
        elif '"' in t and not "'" in t:
            strchr = "'"
            return "assert {t}, {strchr}{t}{strchr}".format(t=t, strchr=strchr)
        else:
            return f"assert {t}"
    else:  # if you didn't return before
        if wrap_func_name is None:
            return f"actual = {source}\nexpected = {want}\nassert actual == expected"
        else:
            return (
                f"actual = {wrap_func_name}({source})\nexpected = {wrap_func_name}({want})\n"
                "assert actual == expected"
            )


def _output_prefix(source, want, prefix="# OUTPUT: "):
    return source + "\n" + prefix + want + "\n"


output_prefix = mk_example_wants_callback(_output_prefix)
assert_wants = mk_example_wants_callback(_assert_wants)

# def example_to_doctest_string(source, want):
#     want.replace()
#     return source +


def doctest_string_trans_lines(
    doctest_obj: doctest.DocTest, example_callback=assert_wants
):
    for example in doctest_obj.examples:
        yield example_callback(example)


def _doctest_string_gen(obj, example_callback, recurse=True):
    doctest_finder = doctest.DocTestFinder(verbose=False, recurse=recurse)
    doctest_objs = doctest_finder.find(obj)
    for doctest_obj in doctest_objs:
        yield from doctest_string_trans_lines(doctest_obj, example_callback)


def doctest_string(obj, example_callback=assert_wants, recurse=True):
    """
    Extract the doctests found in given object.

    :param obj: Object (module, class, function, etc.) you want to extract doctests from.
    :params output_prefix:
    :param recurse: Whether the process should find doctests in the attributes of the object, recursively.
    :return: A string containing the doctests, with output lines prefixed by '# Output:'
    """
    return "\n".join(_doctest_string_gen(obj, example_callback, recurse=recurse))


from functools import partial

doctest_string.for_output_prefix = partial(
    doctest_string, example_callback=output_prefix
)
doctest_string.for_assert_wants = partial(doctest_string, example_callback=assert_wants)


def doctest_string_print(obj, example_callback=assert_wants, recurse=True):
    """
    Extract the doctests found in given object.
    :param obj: Object (module, class, function, etc.) you want to extract doctests from.
    :param recurse: Whether the process should find doctests in the attributes of the object, recursively.
    :return: A string containing the doctests, with output lines prefixed by '# Output:'
    """
    return print(doctest_string(obj, example_callback, recurse=recurse))


def old_doctest_string(
    obj,
    output_prefix="# OUTPUT: ",
    include_attr_without_doctests=False,
    recurse=True,
):
    """
    Extract the doctests found in given object.
    :param obj: Object (module, class, function, etc.) you want to extract doctests from.
    :param output_prefix:
    :param recurse: Whether the process should find doctests in the attributes of the object, recursively.
    :return: A string containing the doctests, with output lines prefixed by '# Output:'
    """
    doctest_finder = doctest.DocTestFinder(verbose=False, recurse=recurse)
    r = doctest_finder.find(obj)
    s = ""
    for rr in r:
        header = f"# {rr.name} "
        header += "#" * max(0, MAX_LINE_LENGTH - len(header)) + "\n"
        ss = ""
        for example in rr.examples:
            want = example.want
            want = want.strip()
            ss += "\n" + example.source + _prefix_lines(want, prefix=output_prefix)
        if include_attr_without_doctests:
            s += header + ss
        elif len(ss) > 0:  # only append this attr if ss is non-empty
            s += header + ss
    return s


# import sphinx

if __name__ == "__main__":
    print(doctest_string(_prefix_lines))
# # _prefix_lines ########################################################
#
# _prefix_lines('something to comment out')
# # OUTPUT: '# something to comment out'
# _prefix_lines('A line you want to prefix', prefix='PREFIX: ')
# # OUTPUT: 'PREFIX: A line you want to prefix'
# print(_prefix_lines('What happens\nif the thing to comment out\nhas multiple lines?'))
# # OUTPUT: # What happens
# # OUTPUT: # if the thing to comment out
# # OUTPUT: # has multiple lines?
# _prefix_lines('')  # see that an empty string is returned as is
# # OUTPUT: ''
# _prefix_lines('', even_if_empty=True)  # unless you ask for it
# # OUTPUT: '# '
```

## errors.py

```python
"""Error objects"""

import dataclasses
from typing import Union, Any, Type
from collections.abc import Callable, Mapping
from functools import partial
from contextlib import AbstractContextManager


class DataError(Exception):
    pass


class DuplicateRecordError(DataError):
    pass


class NotFoundError(DataError):
    pass


class AuthorizationError(Exception):
    pass


class OverwritesNotAllowed(AuthorizationError):
    """To raise when writes are only allowed if the item doesn't already exist"""

    def __init__(self, *args, forbidden_keys=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.forbidden_keys = forbidden_keys

    @classmethod
    def for_key(cls, key):
        return OverwritesNotAllowed(
            f"You're not allowed to overwrite to the value of {key}",
            forbidden_keys={key},
        )

    @classmethod
    def for_keys(cls, keys):
        return OverwritesNotAllowed(
            f"You're not allowed to overwrite to the values of {', '.join(keys)}",
            forbidden_keys=keys,
        )


class ForbiddenError(AuthorizationError):
    pass


class InputError(Exception):
    pass


class ModuleNotFoundIgnore:
    """Context manager meant to ignore import errors.
    The use case in mind is when we want to condition some code on the existence of some package.
    """

    def __enter__(self):
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is ModuleNotFoundError:
            pass
        return True


OnErrorCallback = Union[None, str, Callable[[], Any]]
ExceptionType = type[BaseException]


def log_and_return(msg, logger=print):
    logger(msg)
    return msg


class InterruptWithBlock(BaseException):
    """To be used to interrupt the march of a with"""


# Note: Can be extended to have more precise handled conditions and callbacks
#  (involving exc_val and exc_tb)
# Note: Efforts towards a more general version here:
#   https://github.com/thorwhalen/ut/blob/33c20ce76fe0f9dcc6aa197b0a2dbbf3d7b1d5be/errors.py#L90
@dataclasses.dataclass
class HandleExceptions(AbstractContextManager):
    """A context manager that catches and (specifically) handles specific exceptions.

    It takes one argument: A dict (or mapping) of exception type keys and callback
    values. If within a with block, the particular (listed) exception happens,
    the callback is called and it's returned value is assigned to the
    `HandleExceptions` instance's `.exit_value` attribute.
    That attribute will only exist if the with block existed with an exception
    caught by `HandleExceptions`.

    A callback is an argument-less function. If you need to specify arguments, you can
    envoke the command pattern, using `functools.partial` to make a argument-less
    function. See in the example below how we ask `HandleExceptions` to print a
    specific string if a `ZeroDivisionError` happens:

    >>> from functools import partial
    >>> def print_and_return(msg):
    ...     print(msg)
    ...     return msg
    >>> with HandleExceptions({
    ...     ZeroDivisionError: partial(print_and_return, "You interrupted me"),
    ...     KeyboardInterrupt: lambda: 'imagine this is code to notify someone'
    ... }) as he:
    ...     print('This works')
    ...     0 / 0
    ...
    This works
    You interrupted me

    You can check if the context exited with a handled exception, and if so, what
    the callback returned value was.

    >>> he.exited_with_handled_exception()
    True
    >>> he.exit_value
    'You interrupted me'

    Also available, whether the exception was a handled one or not: The exception
    instance itself:

    >>> he.exited_with_exception
    ZeroDivisionError('division by zero')

    If all you want to do though is print a string (and have the same string
    available in the `exit_value` attribute, we got you covered!
    Just specify a string and we'll make that printer callaback for you!

    >>> from functools import partial
    >>>
    >>> with HandleExceptions({ZeroDivisionError: "You interrupted me again!"}):
    ...     print('This also works')
    ...     0 / 0
    This also works
    You interrupted me again!

    Note that specifying `partial(print, "some message")` will work as a
    "printing callback", but the string won't be available in `.exit_value` since
    `print` returns None.

    A few recipes now...

    You can also use your own custom exception types to do things like interrupt
    a with block early given some condition(s).

    >>> with HandleExceptions(
    ...     {InterruptWithBlock: "The with block was interrupted early."}
    ... ):
    ...     print('before condition')
    ...     x = 5 % 2
    ...     if x:
    ...         raise InterruptWithBlock()
    ...     print('after condition')
    ...
    ...
    before condition
    The with block was interrupted early.

    Tip: If you need to do stuff with an exception, but reraise it, you can
    still do that in your callback. Just say `raise` at the end of the callback!

    >>> def print_and_raise(msg):
    ...     print(msg)
    ...     raise
    >>>
    >>> with HandleExceptions({  # doctest: +SKIP
    ...     ZeroDivisionError: partial(print_and_raise, "That again!"),
    ... }):
    ...     print('This also works')
    ...     0 / 0
    This also works
    That again!
    Traceback (most recent call last):
        ...
    ZeroDivisionError: division by zero
    """

    on_error: Mapping[ExceptionType, OnErrorCallback] = dataclasses.field(
        default_factory=dict
    )
    exited_with_exception = None

    def __post_init__(self):
        self.on_error = dict(self.on_error)
        for handled_exc_type, callback in self.on_error.items():
            if isinstance(callback, str):
                msg = callback
                self.on_error[handled_exc_type] = partial(log_and_return, msg)

    def __enter__(self):
        self.initialize()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            self.exited_with_exception = exc_val
            for handled_exc_type, callback in self.on_error.items():
                if issubclass(exc_type, handled_exc_type):
                    # if the exc_type is a subtype of handled_exc_type, call the callback
                    self.exit_value = callback()  # storing the result in exit_value
                    # and exit with True
                    return True  # suppress the exception
            # if there was an exception, but a handled one, raise it!
            raise

    def exited_with_handled_exception(self):
        return hasattr(self, "exit_value")

    def initialize(self):
        if hasattr(self, "exit_value"):
            delattr(self, "exit_value")
        self.exited_with_exception = None
```

## examples/__init__.py

```python
"""i2 usage examples"""
```

## examples/castgraph_examples.py

```python
"""Examples using castgraph."""

from typing import Optional
from collections.abc import MutableMapping
from i2.castgraph import ConversionRegistry


# ----------------------------------------------------------------------
# Example usage and optional "canonical" hub
# ----------------------------------------------------------------------


class FilePath(str):
    """Marker type for file paths (to distinguish from arbitrary strings)."""


class Text(str):
    """Marker type for text payloads."""


class JSONDict(dict):
    """Marker type for dicts with JSON semantics."""


class CanonicalRecord(dict):
    """Canonical in-memory record format."""


def example_registry() -> ConversionRegistry:
    """
    Build a registry that can:
      - FilePath -> Text
      - Text -> JSONDict
      - JSONDict -> CanonicalRecord
      - Direct Text -> CanonicalRecord (cheaper path)

    The route chosen will be the minimum total cost.

    >>> reg = example_registry()
    >>> # Multi-hop via cheaper direct Text->CanonicalRecord
    >>> data = Text('{"a": 1, "b": 2}')
    >>> out = reg.convert(data, CanonicalRecord)
    >>> isinstance(out, CanonicalRecord) and out["a"] == 1
    True

    >>> # FilePath -> Text -> CanonicalRecord
    >>> fp = FilePath("/tmp/dummy.json")
    >>> out = reg.convert(fp, CanonicalRecord, context={"fs": {" /tmp/dummy.json": '{"x":42}'}})
    >>> isinstance(out, CanonicalRecord) and out["x"] == 42
    True
    """
    import json

    reg = ConversionRegistry()

    @reg.register(FilePath, Text, cost=1.0)
    def filepath_to_text(fp: FilePath, ctx: dict | None) -> Text:
        # Example: pretend-read from an injected "fs" dict for testability
        fs: MutableMapping[str, str] = (ctx or {}).get("fs", {})
        # Try direct lookup first
        content = fs.get(str(fp))
        if content is None:
            # Some doctests or callers may have accidental whitespace in keys
            # (e.g. " /tmp/dummy.json"). Try matching by stripping keys.
            for k, v in fs.items():
                try:
                    if isinstance(k, str) and k.strip() == str(fp):
                        content = v
                        break
                except Exception:
                    # Ignore problematic keys and continue
                    continue
            if content is None:
                # Fallback to real file read (commented to keep doctests pure)
                # with open(fp, "r", encoding="utf-8") as f:
                #     content = f.read()
                # For doctest, simulate missing file:
                content = ""
        return Text(content)

    @reg.register(Text, JSONDict, cost=1.0)
    def text_to_json(t: Text, ctx: dict | None) -> JSONDict:
        return JSONDict(json.loads(str(t) or "{}"))

    @reg.register(JSONDict, CanonicalRecord, cost=1.0)
    def json_to_canonical(d: JSONDict, ctx: dict | None) -> CanonicalRecord:
        # Simple normalization step
        return CanonicalRecord(d)

    @reg.register(Text, CanonicalRecord, cost=0.5)  # cheaper direct route
    def text_to_canonical(t: Text, ctx: dict | None) -> CanonicalRecord:
        return CanonicalRecord(json.loads(str(t) or "{}"))

    return reg
```

## examples/signature_calculus.py

```python
"""
# Signature Calculus

`Sig` is an extension of the `inspect.Signature` object that puts more
goodies at your fingertips.

First of all, a `Sig` instance can be made from a variety of types


>>> from i2.signatures import *
>>> Sig()
<Sig ()>
>>> Sig('self')
<Sig (self)>
>>> Sig(lambda a, b, c=0: None)
<Sig (a, b, c=0)>

>>> Sig(Parameter('foo', Parameter.POSITIONAL_ONLY, default='foo', annotation=int))
<Sig (foo: int = 'foo', /)>
>>> Sig() + 'self' + (lambda a, b, c=0: None) - 'c' + P('c', default=1)
<Sig (self, a, b, c=1)>


Note a difference between `Sig.extract_args_and_kwargs` and binding by hand.

>>> def formula1(w, /, x: float, y=1, *, z: int = 1):
...     return ((w + x) * y) ** z
...
>>>
>>> sig = Sig(formula1)
>>> b = sig.bind(1,2,y=10)
>>> b.apply_defaults()
>>> b.args, b.kwargs
((1, 2, 10), {'z': 1})
>>>
>>> args, kwargs = sig.extract_args_and_kwargs(1, 2, y=10, _apply_defaults=True)
>>> assert args == (1,)
>>> assert kwargs == {'x': 2, 'y': 10, 'z': 1}

bind method seems to favor args over kwargs.

"""
```

## footprints.py

```python
"""Analyzing what attributes of an input object a function actually uses"""

# --------------------------------------------------------------------------------------
# Tools to trace operations on an object.
# See https://github.com/i2mint/i2/issues/56.

import operator
from functools import partial, cached_property
import ast
from textwrap import dedent
from typing import List, Literal, Union
from collections.abc import Callable, Container

from i2.util import ConditionalExceptionCatcher
from i2.multi_object import Pipe
from i2.signatures import Sig, name_of_obj, is_signature_error

# TODO: Maybe we should just use an explicit list of dunders instead of this dynamic
#  introspective approach.
dunder_filt = partial(filter, lambda xx: xx.startswith("__"))

_dunders = Pipe(dir, dunder_filt, set)


def module_if_string(x):
    if isinstance(x, str):
        return __import__(x)
    else:
        return x


dunders = Pipe(module_if_string, _dunders)


def dunders_diff(x, y):
    return dunders(x) - dunders(y)


def _method_sig(func, instance_arg="self"):
    """Replace the first argument of a function signature with an instance argument"""
    sig = Sig(func)
    first_param = sig.names[0]
    return Sig(func).ch_names(**{first_param: instance_arg})


exclude = {"__class_getitem__"}

# operator dunders not dunders of all modules (as represented by `typing` module)
_operator_dunders = {
    k: _method_sig(getattr(operator, k)) for k in dunders_diff("operator", "typing")
}
# dict dunders that aren't dunders of all objects (as represented by `object` object)
_dict_dunders = {
    k: _method_sig(getattr(dict, k)) for k in (dunders_diff(dict, object) - exclude)
}
_rops = (
    set(
        "__radd__, __rsub__, __rmul__, __rdiv__, __rtruediv__, __rfloordiv__, __rmod__, "
        "__rdivmod__, __rpow__, __rlshift__, __rrshift__, __rand__, __rxor__, "
        "__ror__".split()
    )
    - _dict_dunders.keys()
    - _operator_dunders.keys()
)
_rops = {k: Sig(lambda self, other: None) for k in _rops}
_dflt_methods = dict(_operator_dunders, **_dict_dunders, **_rops)


def trace_class_decorator(cls):
    def create_trace_method(name, signature=None):
        def method(self, *args):
            self.trace.append((name, *args))
            return self

        if signature is not None:
            method.__signature__ = signature


# TODO: Handle *args and **kwargs
def _dflt_method_factory(name, signature=None):
    """A factory for methods that trace the operations that are performed on an.
    The methods made here are specifically meant to be operator methods that have only
    positional arguments.
    """

    def method(self, *args):
        self.trace.append((name, *args))
        return self

    method.__name__ = name

    if signature is not None:
        method.__signature__ = signature

    return method


# TODO: Add method factory argument
def trace_class_decorator(
    cls,
    names_and_sigs=tuple(_dflt_methods.items()),
    method_factory=_dflt_method_factory,
):
    """A decorator that adds methods to a class that trace the operations that are
    performed on an instance of that class.
    """
    for name, sig in dict(names_and_sigs).items():
        setattr(cls, name, method_factory(name, sig))

    return cls


@trace_class_decorator
class MethodTrace:
    """A class that can be used to trace the methods that are called on it.

    See: https://github.com/i2mint/i2/issues/56 for more details.

    >>> t = MethodTrace()
    >>> ((t + 3) - 2) * 5 / 10  # doctest: +ELLIPSIS
    <MethodTrace with .trace = ('__add__', 3), ... ('__truediv__', 10)>
    >>> assert t.trace == [
    ...     ('__add__', 3), ('__sub__', 2), ('__mul__', 5), ('__truediv__', 10)
    ... ]
    >>>
    >>>
    >>> w = t[42]
    >>> t[42] = 'mol'  # an operation with two arguments
    >>> # ... and now an operation with no arguments:
    >>> ~t  # doctest: +ELLIPSIS
    <MethodTrace with .trace = ... ('__setitem__', 42, 'mol'), ('__invert__',)>
    >>>
    >>> assert t.trace == [
    ... ('__add__', 3), ('__sub__', 2), ('__mul__', 5), ('__truediv__', 10),
    ... ('__getitem__', 42), ('__setitem__', 42, 'mol'), ('__invert__',)
    ... ]
    >>>

    """

    def __init__(self):
        self.trace = []

    def __repr__(self):
        trace_str = ", ".join(map(lambda x: f"{x}", self.trace))
        return f"<{name_of_obj(type(self))} with .trace = {trace_str}>"

    # TODO: The following is a means to be able to trace all non-dunder methods.
    #  Not sure if we want this as a default, or an option.
    #  Make this work (works, but recursion error when unpickling.
    # def __getattr__(self, operation):
    #     def traced_operation(*args):
    #         self.trace.append((operation, *args))
    #         return self
    #
    #     return traced_operation


# --------------------------------------------------------------------------------------
# Tools to statically

import inspect


def get_class_that_defined_method(method):
    """
    Get class for unbound/bound method.
    """
    if inspect.ismethod(method):
        for cls in inspect.getmro(method.__self__.__class__):
            if cls.__dict__.get(name_of_obj(method)) is method:
                return cls
        method = method.__func__  # fallback to __qualname__ parsing
    if inspect.isfunction(method):
        cls = getattr(
            inspect.getmodule(method),
            method.__qualname__.split(".<locals>", 1)[0].rsplit(".", 1)[0],
        )
        if isinstance(cls, type):
            return cls
    return getattr(method, "__objclass__", None)


def cls_and_method_name_of_method(method):
    if isinstance(method, property):
        return get_class_that_defined_method(method.fget), name_of_obj(method.fget)
    return get_class_that_defined_method(method), name_of_obj(method)


def _process_duplicates(a, remove_duplicates=True):
    if remove_duplicates:
        return set(a)
        ## remove duplicates conserving order
        # return list(dict.fromkeys(a))
    else:
        return a


def get_class_that_defined_method(method):
    """
    Get class for unbound/bound method.
    """
    if inspect.ismethod(method):
        for cls in inspect.getmro(method.__self__.__class__):
            if cls.__dict__.get(name_of_obj(method)) is method:
                return cls
        method = method.__func__  # fallback to __qualname__ parsing
    if inspect.isfunction(method):
        cls = getattr(
            inspect.getmodule(method),
            method.__qualname__.split(".<locals>", 1)[0].rsplit(".", 1)[0],
        )
        if isinstance(cls, type):
            return cls
    return getattr(method, "__objclass__", None)


def cls_and_method_name_of_method(method):
    if isinstance(method, property):
        return get_class_that_defined_method(method.fget), name_of_obj(method.fget)
    return get_class_that_defined_method(method), name_of_obj(method)


# --------------------------------------------------------------------------------------
# Static footprints

import ast
import dis
from functools import reduce
from importlib import import_module
import os
from collections import namedtuple
from inspect import getsource, getsourcefile

Import = namedtuple("Import", ["module", "name", "alias"])


def _get_ast_root_from(o):
    source_str = None
    source_filepath = None
    if isinstance(o, str) and os.path.isfile(o):
        source_filepath = o
        with open(source_filepath) as fh:
            source_str = fh.read()
    elif not isinstance(o, ast.AST):  # not an AST node...
        source_filepath = getsourcefile(o)
        source_str = getsource(o)
        if not isinstance(source_filepath, str) and isinstance(source_str, str):
            raise ValueError("Unrecognized object format")

    return ast.parse(source=source_str, filename=source_filepath)


def _get_imports_from_ast_root(ast_root, recursive=False):
    for node in ast.iter_child_nodes(ast_root):
        module = None
        if isinstance(node, ast.Import):
            module = []
        elif isinstance(node, ast.ImportFrom):
            module = node.module.split(".")

        if module is not None:
            for n in node.names:
                yield Import(module, n.name.split("."), n.asname)

        if recursive:
            yield from _get_imports_from_ast_root(node, recursive=recursive)


def get_imports_from_obj(o, recursive=False):
    """Getting imports for an object (usually, module)"""
    root = _get_ast_root_from(o)
    yield from _get_imports_from_ast_root(root, recursive)


def _alt_cls_and_method_name_of_method(
    method,
):  # TODO: Delete when determined to be of no additional value
    method_path = method.__qualname__.split(".")
    name = method_path[-1]
    cls = reduce(getattr, method_path[:-1], import_module(method.__module__))
    return cls, name


def list_func_calls(fn):
    """
    Extracts functions and methods called from fn
    :param fn: reference to function or method
    :return: a list of functions or methods names
    """
    funcs = []
    if isinstance(fn, cached_property):
        fn = fn.func
    bytecode = dis.Bytecode(fn)
    instrs = list(reversed([instr for instr in bytecode]))
    for ix, instr in enumerate(instrs):
        if instr.opname == "LOAD_METHOD" or instr.opname == "LOAD_GLOBAL":
            funcs.append(instr.argval)
    return [funcname for funcname in reversed(funcs)]


def attr_list(root, func_name):
    """
    Extracts attributes from ast tree processing only func_name function or method
    :param root: root node of ast tree
    :param func_name: name of the function
    :return: a list of attributes names
    """
    atts = []
    functions = sorted(
        {node.name for node in ast.walk(root) if isinstance(node, ast.FunctionDef)}
    )
    for root in ast.walk(root):
        if isinstance(root, ast.FunctionDef) and root.name == func_name:
            for child in ast.walk(root):
                if (
                    isinstance(child, ast.Attribute)
                    and isinstance(child.ctx, ast.Load)
                    and child.attr not in functions
                ):
                    atts.append(child.attr)

    return atts


# TODO: Generalize attrs_used_by_method to attrs_used_by_func.


class _DefinitionFinder(ast.NodeVisitor):
    def __init__(self, object_name):
        self.object_name = object_name
        self.definition_node = None

    # def visit_Assign(self, node):
    #     # Check if the target of assignment matches the object name
    #     for target in node.targets:
    #         if isinstance(target, ast.Name) and target.id == self.object_name:
    #             self.definition_node = node
    #     self.generic_visit(node)

    def visit_FunctionDef(self, node):
        # Check if the function name matches the object name
        if node.name == self.object_name:
            self.definition_node = node
        self.generic_visit(node)

    def visit_ClassDef(self, node):
        # Check if the class name matches the object name
        if node.name == self.object_name:
            self.definition_node = node
        self.generic_visit(node)


def _get_definition_node(object_name, src_code):
    tree = ast.parse(dedent(src_code))
    finder = _DefinitionFinder(object_name)
    finder.visit(tree)
    return finder.definition_node


def _get_source_segment_from_node(node, src_code):
    """
    Extracts the source code segment for a given AST node from the source code string.
    """
    if not node:
        return None

    # Extract start and end line numbers from the node
    start_line = node.lineno
    end_line = node.end_lineno if hasattr(node, "end_lineno") else start_line

    # Split the source code into lines
    lines = src_code.splitlines()

    # Extract the lines corresponding to the node (adjusting for 0-based indexing)
    segment_lines = lines[start_line - 1 : end_line]

    # Join the lines back into a single string
    return "\n".join(segment_lines)


def _get_definition_source(object_name: str, src_code: str):
    """
    Returns the source code string for the definition of the specified object within
    the given source code.

    >>> src_code = '''
    ... class MyClass:
    ...     x = 1
    ...     def my_method(self):
    ...         return self.x + 1
    ...
    ... def my_function():
    ...     pass
    ...
    ... a = 10
    ... '''
    >>> object_name = "MyClass"
    >>> assert _get_definition_source('MyClass', src_code).strip() == (
    ... '''
    ... class MyClass:
    ...     x = 1
    ...     def my_method(self):
    ...         return self.x + 1
    ... '''.strip()
    ... )
    """
    object_name, *more_names = object_name.split(".")

    node = _get_definition_node(object_name, src_code)
    if node is None:
        raise ValueError(f"Could not find definition for object {object_name}")
    src = _get_source_segment_from_node(node, src_code)
    src = dedent(src)

    if not more_names:
        return src
    else:
        return _get_definition_source(".".join(more_names), src)


def _unwrap_object(o):
    if isinstance(o, cached_property):
        return o.func
    elif isinstance(o, property):
        return o.fget
    return o


from operator import attrgetter

qualname_of_obj = partial(name_of_obj, base_name_of_obj=attrgetter("__qualname__"))


def _get_source(o, src_code=None) -> str:
    if src_code is None:
        if isinstance(o, str):
            if os.path.isfile(o):
                with open(o) as f:
                    source_str = f.read()
            else:
                source_str = o  # consider the string to be the source code
            return source_str
        else:  # it's an object
            o = _unwrap_object(o)
            # get's it's source code
            try:
                with open(getsourcefile(o)) as f:
                    source_str = f.read()
                return source_str
                # TODO: (somehow, simply return getsource(o) doesn't lead to passing tests)
            except OSError as e:
                try:
                    return getsource(o)
                except Exception as e:

                    raise ValueError(
                        f"Could not get source code for object {o}. "
                        "(This can happen if your object was defined in jupyter notebooks, "
                        "for example.)"
                        "Please provide the source code explicitly via the "
                        "src_code argument. "
                        "For example, if you are in a jupyter notebook or ipython, "
                        "you can specify `src_code=In[cell_index]` where `cell_index` "
                        "is the index of the cell where the object was defined."
                    )
    else:
        # The reason for this case is that the source code of an object is not always
        # accessible via normal means (for example, in jupyter notebooks) so we need to pass it in.
        if isinstance(o, str):
            object_name = o
        else:
            object_name = qualname_of_obj(o)
        source_str = _get_definition_source(object_name, src_code)
        return source_str


def ensure_ast(o, src_code=None) -> ast.AST:
    """
    Casts input object `o` to a AST node.

    If the input is an AST node, it is returned as is.
    If the input is a filepath, the file is read and parsed as source code.
    If the input is a string, it is parsed as source code.
    If the input is an object, the source code is extracted and parsed.

    Let's get an AST for the ensure_ast function itself:

    >>> assert isinstance(ensure_ast(ensure_ast), ast.AST)

    Note that sometimes the source code of an object cannot be accessed via normal
    means (for example, in REPLs like jupyter notebooks) so we need to pass it in.

    >>> src_code = '''
    ... class MyClass:
    ...     x = 1
    ...     def my_method(self):
    ...         return self.x + 1
    ... a = 10
    ... '''
    >>> assert isinstance(ensure_ast('MyClass', src_code), ast.AST)

    """

    if isinstance(o, ast.AST):
        return o
    else:
        source_str = _get_source(o, src_code)
        return ast.parse(dedent(source_str))


class AttributeVisitor(ast.NodeVisitor):
    def __init__(self, object_name):
        self.object_name = object_name
        self.attributes = set()

    def visit_Attribute(self, node):
        # Check if the attribute access is for the target object
        if isinstance(node.value, ast.Name) and node.value.id == self.object_name:
            self.attributes.add(node.attr)
        # Continue traversing to find more attributes
        self.generic_visit(node)


# TODO: Clean this all up, it's horrible!
# TODO: Write teests for accessed_attributes using i2.tests.footprints_test -> A, B
def accessed_attributes(func, object_name=None):
    """
    Extracts the attributes accessed by a function or method.

    (This is a simpler, but narrow, version of `attrs_used_by_method`).

    >>> def func(a, b, c):
    ...     return a + b.bar + c
    ...
    >>> # Commenting out the testing code, as execution is not performed in the PCI
    >>> def foo(self):
    ...     a = 2
    ...     self.method(x)
    ...     y = self.prop
    ...     return a + func(x, self, y)
    ...
    >>> assert accessed_attributes(foo, 'self') == {'method', 'prop'}

    """
    if object_name is None:
        object_name = next(iter(Sig(func)), None)
        if object_name is None:
            raise ValueError(
                "Could not determine the object name. Please provide it explicitly."
            )

    # Convert the function source to an AST
    func_name = qualname_of_obj(func)
    src = _get_definition_source(func_name, _get_source(func))
    node = ensure_ast(src)
    # func_src = ast.parse(inspect.getsource(func))
    # Initialize the visitor with the target object name
    visitor = AttributeVisitor(object_name)
    # Visit the AST to find accessed attributes
    visitor.visit(node)
    # Return the set of accessed attributes
    return visitor.attributes


def _is_method_like(
    name,
    obj,
    *,
    no_dunders=True,
    include=("__post_init__",),
    include_types=(Callable, property, cached_property),
    exclude_types=(staticmethod,),
):
    if name in include:
        return True
    elif no_dunders and name.startswith("__") and name.endswith("__"):
        return False
    return isinstance(obj, include_types) and not isinstance(obj, exclude_types)


def init_argument_names(cls, *, no_error_action=None) -> list[str]:
    """
    Get the list of argument names

    >>> from dataclasses import dataclass
    >>> @dataclass
    ... class DataClass:
    ...     x: str
    ...     y: float = 2
    ...     z = 3
    ...
    >>> init_argument_names(DataClass)
    ['x', 'y']

    Note: Some builtin types don't have signatures, so we get:

    ```
    ValueError: no signature found for builtin type ...
    ```

    By default, we handle this by returning an empty list, but a callable
    no_error_action will call that function and return its result.
    Anything else will result in raising the error.

    """
    try:
        return Sig(cls).names
    except ValueError as e:
        # Some builtin types don't have signatures, so we get:
        #   ValueError: no signature found for builtin type ...
        # By default, we handle this by returning an empty list.
        if no_error_action is None:
            return []
        elif callable(no_error_action):
            return no_error_action()
        else:
            raise e


ExcludeNames = Union[Container, Callable]


def _get_class_attributes(
    cls: type,
    filt=_is_method_like,
    *,
    exclude_names: ExcludeNames = init_argument_names,
):
    if isinstance(exclude_names, Callable):
        exclude_names = exclude_names(cls)

    for name, obj in cls.__dict__.items():
        if filt(name, obj) and name not in exclude_names:
            yield obj


skip_signature_errors = ConditionalExceptionCatcher(ValueError, is_signature_error)


def attribute_dependencies(
    cls: type,
    filt=_is_method_like,
    *,
    name_of_obj=name_of_obj,
    exclude_names: ExcludeNames = init_argument_names,
):
    """
    Extracts (method_name, accessed_attributes) pairs for a class or instance thereof.

    :param cls: The class or instance to analyze
    :param filt: A function that filters the attributes to consider
    :param name_of_obj: A function that returns the name of an object
    :param exclude_names: A list of names to exclude from the analysis or a function that
        returns such a list given the class
    :return: A generator of (method_name, accessed_attributes) pairs

    """
    for func in _get_class_attributes(cls, filt=filt, exclude_names=exclude_names):
        with skip_signature_errors:
            yield name_of_obj(func), accessed_attributes(func)


def _attrs_used_by_method(cls, method_name, *, src_code=None):
    """
    Util for attrs_used_by_method. Same output as attrs_used_by_method, but intput is (cls, method_name)
    """
    root = ensure_ast(cls, src_code)
    funcs = list_func_calls(getattr(cls, method_name))
    attrs = []
    for func in funcs:
        attrs = attrs + attr_list(root, func)

    return _process_duplicates(
        attrs + attr_list(root, method_name), remove_duplicates=True
    )


def attrs_used_by_method(method, *, src_code=None):
    """
    Extracts a list of cls attributes which are used by a method or method_name function

    Note: The function tries to analyzed the source code deeply, gathering indirect
    references to the instance attributes. As a result, it is not very robust.
    You may want to check out the simpler (but narrow) function: `accessed_attributes`.

    Args:
        method: The method (object) to analyze

    Returns:
        A list of attribute names (of the class or instance thereof) that are accessed in the code of the said method.

    Example:

    Consider the method `A.target_method` coming from the following code in
    `i2.tests.footprints_test`:
    ```python
    def func(obj):
        \"\"\"Accesses attributes 'a' and 'b' of obj\"\"\"
        return obj.a + obj.b

    class A:
        e = 2

        def __init__(self, a=1, b=0, c=1, d=10):
            self.a = a
            self.b = b
            self.c = c
            self.d = d

        def target_method(self, x):
            \"\"\"Accesses ['a', 'b', 'c', 'e']\"\"\"
            t = func(self)  # and this function will access some attributes!
            tt = self.other_method(t)
            return x * tt / self.e

        def other_method(self, x=1):
            \"\"\"Accesses ['c', 'e']\"\"\"
            w = self.c * 2  # c is accessed first
            return self.e + self.c * x - w  # and c is accessed again

        @classmethod
        def a_class_method(cls, y):
            \"\"\"Accesses ['e']\"\"\"
            return cls.e + y
    ```

    # TODO: Stopped working in 3.12 (worked in 3.10). See why
    # >>> from i2.tests.footprints_test import A
    # >>> assert attrs_used_by_method(A.target_method) == {'a', 'b', 'c', 'e'}

    """
    return _attrs_used_by_method(
        *cls_and_method_name_of_method(method), src_code=src_code
    )


# --------------------------------------------------------------------------------------
# Newer static footprints
# TODO: Merge with older attrs_used_by_method one above

import inspect
from i2.signatures import Sig, resolve_function


def get_source(obj: object) -> str:
    """Get source string of a python object"""
    return inspect.getsource(resolve_function(obj))


# TODO: Break into two functions (one doing the work of the loop for a single method)
# TODO: Routing pattern. Extract conditional logic to make it parametrizable
def object_dependencies(obj, *, get_source=get_source):
    import ast
    import inspect
    import textwrap
    from functools import cached_property

    dependency_dict = {}

    if inspect.isclass(obj):
        members = inspect.getmembers(obj)
    elif hasattr(obj, "__class__"):
        members = inspect.getmembers(obj.__class__)
    else:
        return "Invalid input. Please provide a class or an instance."

    for name, method in members:
        try:
            source_code = get_source(method)
            first_arg = Sig(resolve_function(method)).names[0]
        except TypeError:
            continue

        source_code = textwrap.dedent(source_code)
        tree = ast.parse(source_code)

        called_methods = []
        assigned_attributes = set()

        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Attribute):
                        if getattr(target.value, "id", None) == first_arg:
                            assigned_attributes.add(target.attr)

            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Attribute):
                    if getattr(node.func.value, "id", None) == first_arg:
                        called_methods.append(node.func.attr)

            elif isinstance(node, ast.Attribute):
                if isinstance(node.value, ast.Name):
                    if node.value.id == first_arg:
                        called_methods.append(node.attr)

        # Remove attributes that are only assigned to
        called_methods = set(called_methods) - assigned_attributes

        dependency_dict[name] = called_methods

    return dependency_dict


def _dict_to_graph(
    graph: dict,
    *,
    edge_connector: str,
    graph_template: str,
    display: Callable = None,
    indent: str = "    ",
    prefix: str = "",
    suffix: str = "",
) -> str:
    """Helper for dict_to_graph"""
    if not display:
        display = lambda x: x
    elif display is True:
        display = print
    assert callable(display), f"display should be callable, boolean or None: {display=}"
    lines = []
    for from_node, to_nodes in graph.items():
        for to_node in to_nodes:
            lines.append(f"{indent}{from_node}{edge_connector}{to_node};")
    graph_code = "\n".join(lines)
    return display(graph_template.format(code=f"{prefix}{graph_code}{suffix}"))


def dict_to_graph(
    graph: dict,
    from_key_to_values: bool = True,
    *,
    kind: Literal["graphviz", "mermaid"] = "graphviz",
    indent: str = "    ",
    prefix: str = "",
    suffix: str = "",
    display: bool | Callable = False,
) -> str:
    """A function to convert a dictionary to a graphviz string.

    You privide a graph in the form of a ``{from_node: to_nodes, ...`` or
    ``{to_node: from_nodes, ...``` dictionary, and will get a graphviz string.
    You can use this to visualize a graph (e.g. a dependency graph) in a jupyter notebook.

    :param graph: The graph, in the form of a to convert to graphviz.
    :param from_key_to_values: Whether the keys of the graph are from nodes or to nodes.
    :param kind: The kind of graphviz string to return. Either "graphviz" or "mermaid".
    :param indent: The indent to use for the graphviz string.
    :param graphviz_template: The template to use for the graphviz string.
    :param display: Whether to display the graphviz string as a graph in a jupyter notebook. Requires graphviz.
    :return: The graphviz string.

    Example (but bear in mind the order of the nodes in graphviz_str may be different):

    >>> graph_dict = {
    ...     "A": ["B", "C"],
    ...     "B": ["D"],  # note that "D" is not mentioned as a key
    ...     "C": ["D", "E", "F"],
    ...     "E": [],
    ... }
    >>> # Keys are from nodes
    >>> graphviz_str = dict_to_graph(graph_dict)
    >>> print(graphviz_str)  # doctest: +SKIP
    digraph G {
        "A" -> "B";
        "A" -> "C";
        "B" -> "D";
        "C" -> "D";
        "C" -> "E";
        "C" -> "F";
    }

    >>> # Keys are to nodes
    >>> graphviz_str = dict_to_graph(graph_dict, from_key_to_values=False)
    >>> print(graphviz_str)  # doctest: +SKIP
    digraph G {
        "B" -> "A";
        "C" -> "A";
        "D" -> "B";
        "D" -> "C";
        "E" -> "C";
    }

    The default ``kind`` is graphviz, but you can also use mermaid:

    >>> graphviz_str = dict_to_graph(graph_dict, kind="mermaid")
    >>> print(graphviz_str)  # doctest: +SKIP
    graph TD
        A --> B;
        A --> C;
        B --> D;
        C --> D;
        C --> E;
        C --> F;

    """
    # TODO: Could make these specs open-closed (routing pattern)
    if kind == "graphviz":
        graph_template = f"digraph G {{{{\n{{code}}\n}}}}"
        edge_connector = " -> "
        if display is True:
            try:
                from graphviz import Source

                display = Source
            except ImportError:
                ImportError("You need to `pip install graphviz` to display the graph")

    elif kind == "mermaid":
        graph_template = f"graph TD\n{{code}}\n"
        edge_connector = " --> "
        if display is True:
            try:
                from kroki import diagram_image  # pip install kroki

                display = lambda x: diagram_image(x, diagram_type="mermaid")
            except ImportError:
                ImportError("You need to `pip install kroki` to display the graph")

    else:
        raise ValueError(f"Invalid kind specified: {kind}")

    if not from_key_to_values:
        graph = {
            to_node: [from_node for from_node in graph if to_node in graph[from_node]]
            for to_node in {val for vals in graph.values() for val in vals}
        }

    return _dict_to_graph(
        graph,
        edge_connector=edge_connector,
        graph_template=graph_template,
        indent=indent,
        prefix=prefix,
        suffix=suffix,
        display=display,
    )


# --------------------------------------------------------------------------------------
# Dynamic footprints

from contextlib import contextmanager


class Tracker:
    """
    Tracks the attribute access right after `start_track` is set to True.

    Add this to __metaclass__ for any class that you need to track attributes for given a
    target method.
    """

    start_track = False

    def __init__(self, *args, **kwargs):
        self.attr_used = []
        self.attrs_to_ignore = []
        super().__init__(*args, **kwargs)

    def __getattribute__(self, item):
        """
        Inspect getter for tracking the attributes accessed.
        """
        if item not in ["on_access"]:
            self.on_access(item)
        return super().__getattribute__(item)

    def __setattr__(self, key, value):
        """
        Inspect setter for tracking the attributes accessed.
        """
        if self.start_track:
            self.on_access(key)

        super().__setattr__(key, value)

    def on_access(self, key):
        """
        on attribute access, record attribute if and only if its not from
        core attribute or `attrs_to_ignore` set to class.
        """
        if (
            key in ["start_track", "attr_used", "on_access", "attrs_to_ignore"]
            or key in self.attrs_to_ignore
        ):
            return
        if self.start_track:
            self.attr_used.append(key)


@contextmanager
def start_tracking(tracker_instance):
    """
    Ctx manager to gracefully start/stop tracking.
    """
    tracker_instance.start_track = True
    yield tracker_instance
    tracker_instance.start_track = False


def attrs_used_by_method_computation(
    cls_method, init_kwargs=None, method_kwargs=None, remove_duplicates=True
):
    """
    Tracks the access to attributes within an execution.
    """
    if init_kwargs is None:
        init_kwargs = {}
    if method_kwargs is None:
        method_kwargs = {}

    method_class, method_name = cls_and_method_name_of_method(cls_method)
    tracker = type(
        "Tracker",
        (Tracker, method_class),
        dict(method_class.__dict__, **Tracker.__dict__),
    )(**init_kwargs)
    tracker.attrs_to_ignore = [
        func for func in dir(tracker) if callable(getattr(tracker, func))
    ]

    if hasattr(tracker, method_name):
        # Now, we want to track attributes.
        with start_tracking(tracker):
            if isinstance(cls_method, property):
                candidate_method = cls_method.fget
                candidate_method(tracker)
            else:
                candidate_method = getattr(tracker, method_name)
                candidate_method(**method_kwargs)

        return _process_duplicates(
            tracker.attr_used, remove_duplicates=remove_duplicates
        )

    else:
        # Error class/obj do not have that method.
        return 1


# def test_attrs_used_by_method():
#     def func(obj):
#         return obj.a + obj.b
#
#     class A:
#         e = 2
#
#         def __init__(self, a=0, b=0, c=1, d=10):
#             self.a = a
#             self.b = b
#             self.c = c
#             self.d = d
#
#         def target_func(self, x=3):
#             t = func(self)
#             tt = self.other_method(t)
#             return x * tt / self.e
#
#         target_method = target_func
#
#         def other_method(self, x=1):
#             return self.c * x
#
#     from i2.footprints import attrs_used_by_method
#
#     print(attrs_used_by_method(A.target_func))
```

## io_trans.py

```python
"""
Tools to make input and output transforming decorators.

Input value transformers can be conditioned on argument value and name, as well as the
wrapped function itself.

Output value tranformers can be conditioned on argument value and the wrapped function.

"""

from dataclasses import dataclass
from typing import (
    Optional,
    TypedDict,
)
from collections.abc import Mapping, Callable
from inspect import signature, Parameter
from pickle import dumps

from i2.signatures import Sig

import functools

# from typing import _TypedDictMeta
# Note: When doing the above, lint complains, but TypedDict alone doesn't do the trick.
# _TypedDictMeta = TypedDict  # to show that TypedDict doesn't work
# Raises     TypeError: TypedDict does not support instance and class checks
# So we do the following hack:
_TypedDictMeta = type(TypedDict)


# monkey patching WRAPPER_ASSIGNMENTS to get "proper" wrapping (adding defaults and kwdefaults
wrapper_assignments = (
    "__module__",
    "__name__",
    "__qualname__",
    "__doc__",
    "__annotations__",
    "__defaults__",
    "__kwdefaults__",
)

update_wrapper = functools.update_wrapper
update_wrapper.__defaults__ = (
    functools.WRAPPER_ASSIGNMENTS,
    functools.WRAPPER_UPDATES,
)
wraps = functools.wraps
wraps.__defaults__ = (functools.WRAPPER_ASSIGNMENTS, functools.WRAPPER_UPDATES)


def identity_func(x):
    return x


@dataclass
class IoTrans:
    def in_val_trans(self, argval, argname, func):
        return argval  # default is the value as is

    def out_trans(self, argval, func):
        return argval  # default is the value as is

    def __call__(self, func):
        sig = Sig(func)

        @wraps(
            func
        )  # Todo: Want empty mapping as default (use frozendict or __post_init__?)
        def wrapped_func(*args, **kwargs):
            original_kwargs = sig.map_arguments_from_variadics(*args, **kwargs)
            new_kwargs = {
                argname: self.in_val_trans(argval, argname, func)
                for argname, argval in original_kwargs.items()
            }
            new_args, new_kwargs = sig.mk_args_and_kwargs(new_kwargs)
            return self.out_trans(func(*new_args, **new_kwargs), func)

        return wrapped_func


@dataclass
class ArgnameIoTrans(IoTrans):
    """Transforms argument values according to their names

    >>> def foo(a, b=1.0):
    ...     return a + b
    >>>
    >>> input_trans = ArgnameIoTrans({'a': int, 'b': float})
    >>> foo2 = input_trans(foo)
    >>> assert foo2(3) == 4.0
    >>> assert foo2(-2, 2) == 0.0
    >>> assert foo2("3") == 4.0
    >>> assert foo2("-2", "2") == 0.0
    >>> assert signature(foo) == signature(foo2)
    """

    argname_2_trans_func: Mapping

    def in_val_trans(self, argval, argname, func):
        trans_func = self.argname_2_trans_func.get(argname, None)
        if trans_func is not None:
            return trans_func(argval)
        else:
            return super().in_val_trans(argval, argname, func)


empty = Parameter.empty


@dataclass
class AnnotAndDfltIoTrans(IoTrans):
    """Transforms argument values using annotations and default type

    >>> def foo(a: int, b=1.0):
    ...     return a + b
    >>>
    >>> input_trans = AnnotAndDfltIoTrans()
    >>> foo3 = input_trans(foo)
    >>> assert foo3(3) == 4.0
    >>> assert foo3(-2, 2) == 0.0
    >>> assert foo3("3") == 4.0
    >>> assert foo3("-2", "2") == 0.0
    >>> assert signature(foo) == signature(foo3)
    """

    annotations_handled = frozenset([int, float, str])
    dflt_types_handled = frozenset([int, float, str])

    def in_val_trans(self, argval, argname, func):
        param = signature(func).parameters[argname]
        if param.annotation in self.annotations_handled:
            return param.annotation(argval)
        else:
            dflt_type = type(param.default)
            if dflt_type in self.dflt_types_handled:
                return dflt_type(argval)
        return super().in_val_trans(argval, argname, func)


@dataclass
class JSONAnnotAndDfltIoTrans(AnnotAndDfltIoTrans):
    """Transforms argument values using annotations and default type,
    including lists, iterables, dicts, and booleans

    >>> def foo(a: dict, b=['dflt'], c=False):
    ...     return dict({}, a=a, b=b, c=c)
    >>>
    >>> input_trans = JSONAnnotAndDfltIoTrans()
    >>> foo4 = input_trans(foo)
    >>> assert foo4('{}') == {'a': {}, 'b': ['dflt'], 'c': False}
    >>> assert foo4({'d': 'e'}, '["some_value"]', 'true') == {'a': {'d': 'e'}, 'b': ['some_value'], 'c': True}
    >>> complex_types_result = foo4('{"None": null, "True": true, "False": false}', '[null, true, false]', 'false')
    >>> assert complex_types_result == {'a': {'None': None, 'True': True, 'False': False}, 'b': [None, True, False], 'c': False}
    >>> assert signature(foo) == signature(foo4)
    """

    def in_val_trans(self, argval, argname, func):
        param = signature(func).parameters[argname]
        if param.annotation != str and not isinstance(param.default, str):
            if (
                param.annotation == dict
                or isinstance(param.annotation, _TypedDictMeta)
                or isinstance(param.default, dict)
                or isinstance(type(param.default), _TypedDictMeta)
                or param.annotation == bool
                or isinstance(param.default, bool)
            ):
                return cast_to_jdict(argval)
            if hasattr(param.annotation, "__iter__") or hasattr(
                param.default, "__iter__"
            ):
                return cast_to_list(argval)
        return super().in_val_trans(argval, argname, func)


# TODO: Move this doctest to tests suite that ignores the test if pandas not there!
# >>> import pandas as pd
# >>> out_trans = TypedBasedOutIoTrans({
# ...     (list, tuple, set): ', '.join,
# ...     pd.DataFrame: pd.DataFrame.to_csv
# ... })
# >>>
# >>>
# >>> @out_trans
# ... def repeat(a: int, b: list):
# ...     return a * b
# ...
# >>> assert repeat(2, ['repeat', 'it']) == 'repeat, it, repeat, it'
# >>>
# >>> @out_trans
# ... def transpose(df):
# ...     return df.T
# ...
# >>> df = pd.DataFrame({'a': [1,2,3], 'b': [10, 20, 30]})
# >>> print(df.to_csv())  # doctest: +NORMALIZE_WHITESPACE
# ,a,b
# 0,1,10
# 1,2,20
# 2,3,30
# >>> print(transpose(df))  # doctest: +NORMALIZE_WHITESPACE
# ,0,1,2
# a,1,2,3
# b,10,20,30
# TODO: Write doctest (that only needs builtins)
@dataclass
class TypedBasedOutIoTrans(IoTrans):
    """Transform output according to it's type."""

    trans_func_for_type: (
        Mapping
    ) = ()  # Todo: Want empty mapping as default (use frozendict or __post_init__?)
    dflt_trans_func: Callable | None = None

    def out_trans(self, argval, func):
        for typ in self.trans_func_for_type:
            if isinstance(argval, typ):
                return self.trans_func_for_type[typ](argval)
        if isinstance(
            self.dflt_trans_func, Callable
        ):  # Question: use callable() instead? What's the difference?
            return self.dflt_trans_func(argval)


def pickle_out_trans(self, argval, func):
    return dumps(argval)


PickleFallbackTypedBasedOutIoTrans = functools.partial(
    TypedBasedOutIoTrans, dflt_trans_func=dumps
)

import json
import os


def cast_to_jdict(value):
    """Tries to cast to a json-friendly dictionary.

    >>> cast_to_jdict('3')
    [3]
    >>> cast_to_jdict("[3]")
    [3]
    >>> cast_to_jdict("[4,2]")
    [4, 2]
    >>> cast_to_jdict('[4, "string", ["another", "list"], {"nested": 10.2}]')
    [4, 'string', ['another', 'list'], {'nested': 10.2}]
    >>> cast_to_jdict('{"here": "is", "a": {"nested": "json"}, "with": [null, true, false, 1, 2.3]}')
    {'here': 'is', 'a': {'nested': 'json'}, 'with': [None, True, False, 1, 2.3]}

    And csvs too:

    >>> cast_to_jdict('1,2,3.4, "string" ,  null, true, false, ["a", "list"]')
    [1, 2, 3.4, 'string', None, True, False, ['a', 'list']]
    """
    if isinstance(value, str):
        value = value.strip()
        if value:
            first_char = value[0]
            if first_char in {"[", "{"}:
                return json.loads(value)
            elif value in ["true", "false", "null"]:
                return json.loads(value)
            elif os.path.isfile(value):
                return json.load(value)
            else:
                return json.loads(
                    "[" + value + "]"
                )  # wrap in brackets and call json.loads
        else:
            return ""
    else:
        return value


def cast_to_list(value):
    """Tries to case to a list (with json friendly elements)

    >>> cast_to_list('3')
    [3]
    >>> cast_to_list("[3]")
    [3]
    >>> cast_to_list("[4,2]")
    [4, 2]
    >>> cast_to_list('[4, "string", ["another", "list"], {"nested": 10.2}]')
    [4, 'string', ['another', 'list'], {'nested': 10.2}]

    And csvs too:

    >>> cast_to_list('1,2,3.4, "string" ,  null, true, false, ["a", "list"]')
    [1, 2, 3.4, 'string', None, True, False, ['a', 'list']]
    """
    if isinstance(value, str):
        value = cast_to_jdict(value)
        assert isinstance(value, list)
        return value
    elif hasattr(value, "tolist"):  # meant for numpy arrays
        # what other potential attributes to check for?
        return value.tolist()
    else:
        return list(
            value
        )  # will work with set, tuple, and other iterables (not recursively though: just level 0)


# @dataclass
# class PickleFallbackTypedBasedOutIoTrans(TypedBasedOutIoTrans):
#     dflt_trans_func = pickle_out_trans
```

## itypes.py

```python
"""Types"""

from typing import (
    NewType,
    Optional,
    Protocol,
    Any,
    runtime_checkable,
    get_args,
    Literal,
    get_origin,
)
from collections.abc import Iterable


from inspect import signature
from functools import wraps


# Note: Simplified func-applied version of i2.Sig.kwargs_for_args_and_kwargs
def _arg_name_and_val_dict(func, *args, **kwargs):
    """

    :param func:
    :param args:
    :param kwargs:
    :return:

    >>> def foo(x, /, y, *, z=3): ...
    >>> _arg_name_and_val_dict(foo, 1, 2, z=4)
    {'x': 1, 'y': 2, 'z': 4}

    """
    b = signature(func).bind(*args, **kwargs)
    b.apply_defaults()
    return dict(b.arguments)


def _annotation_is_literal(typ):
    return get_origin(typ) is Literal


def _literal_values(literal):
    """
    Get the values of a Literal type.

    :param literal:
    :return:

    >>> from typing import Literal
    >>> _literal_values(Literal[1, 2, 3])
    (1, 2, 3)
    """
    return literal.__args__


def _value_is_in_literal(value, literal):
    return value in _literal_values(literal)


def _validate_that_value_is_in_literal(name, value, literal):
    if not _value_is_in_literal(value, literal):
        error = ValueError(
            f"{value} is an invalid value for {name}. "
            f"Values should be one of the following: {literal.__args__}"
        )
        error.allowed_values = literal.__args__
        error.input_value = value
        raise error


# TODO: Not picklable. Make with wrapper module, or let this module be independent?
# TODO: Could use Sig. Should we, or let this module be independent?
# TODO: Add control over error message/type?
def validate_literal(func):
    """
    Decorator to validate (Literal-annotated) argument values at call time.

    Wraps a function to add validation of the input arguments annotated with Literal
    against the values listed by the literal. If the input argument is not one of the
    literal values, a ValueError is raised.

    >>> @validate_literal
    ... def f(x: Literal[1, 2, 3]):
    ...     return x
    >>> f(1)
    1
    >>> f(4)  # doctest: +NORMALIZE_WHITESPACE
    Traceback (most recent call last):
        ...
    ValueError: 4 is an invalid value for x. Values should be one of the following: (1, 2, 3)

    """

    @wraps(func)
    def wrapper(*args, **kwargs):
        _kwargs = _arg_name_and_val_dict(func, *args, **kwargs)
        for arg_name, arg_type in func.__annotations__.items():
            if _annotation_is_literal(arg_type):
                arg_val = _kwargs[arg_name]
                _validate_that_value_is_in_literal(arg_name, arg_val, arg_type)
        return func(*args, **kwargs)

    return wrapper


def iterable_to_literal(iterable: Iterable):
    """
    Convert an iterable to a Literal type.

    >>> iterable_to_literal([1, 2, 3])
    typing.Literal[1, 2, 3]

    """
    return Literal.__getitem__(tuple(iterable))


def new_type(
    name,
    tp,
    doc: str | None = None,
    aka: Iterable | None = None,
    assign_to_globals=False,
):
    """
    Make a new type with (optional) doc and (optional) aka, set of var names it often
    appears as

    Args:
        name: Name to give the variable
        tp: type (see typing module)
        doc: Optional string to put in __doc__ attribute
        aka: Optional set (or any iterable) to put in _aka attribute,
            meant to list names the variables of this type often appear as.

    Returns: None

    >>> from typing import Any, Union, List
    >>> MyType = new_type('MyType', int)
    >>> # TODO: Skipping the next part because outputs <class 'typing.NewType'> in 3.10
    >>> type(MyType)  # doctest: +SKIP
    <class 'function'>
    >>> Key = new_type('Key', Any, aka=['key', 'k'])
    >>> sorted(Key._aka)
    ['k', 'key']
    >>> Val = new_type(
    ... 'Val', Union[int, float, List[Union[int, float]]],
    ... doc="A number or list of numbers.")
    >>> Val.__doc__
    'A number or list of numbers.'
    """
    new_tp = NewType(name, tp)
    if doc is not None:
        setattr(new_tp, "__doc__", doc)
    if aka is not None:
        setattr(new_tp, "_aka", set(aka))
    if assign_to_globals:
        globals()[
            name
        ] = new_tp  # not sure how kosher this is... Should only use at top level of module, for sure!
    return new_tp


class HasAttrs:
    """
    Make a protocol to express the existence of specific attributes.

    >>> SizedAndAppendable = HasAttrs["__len__", "append"]
    >>> assert isinstance([1, 2, 3], SizedAndAppendable)  # lists have both a length and an append
    >>> assert not isinstance((1, 2, 3), SizedAndAppendable)  # tuples don't have an append

    [Python Protocols](https://www.python.org/dev/peps/pep-0544/) are a way to be able to do
    "behavior typing" (my bad terminology).
    Basically, if you want your static analyzer
    (the swingles in your IDE, or linter validation process...)
    to check if you're manipulating the expected types, except the types
    (classes, subclasses, ABCs, abstract classes...) are too restrictive (they are!),
    you can use Protocols to fill the gap.

    Except writing them can sometimes be verbose.

    With HasAttrs you can have the basic "does it have these attributes" cases covered.

    >>> assert isinstance(dict(), HasAttrs["items"])
    >>> assert not isinstance(list(), HasAttrs["items"])
    >>> assert not isinstance(dict(), HasAttrs["append"])
    >>>
    >>> class A:
    ...     prop = 2
    ...
    ...     def method(self):
    ...         pass
    >>>
    >>> a = A()
    >>> assert isinstance(a, HasAttrs["method"])
    >>> assert isinstance(a, HasAttrs["method", "prop"])
    >>> assert not isinstance(a, HasAttrs["method", "prop", "this_attr_does_not_exist"])

    """

    def __class_getitem__(self, attr_names):
        if isinstance(attr_names, str):
            attr_names = [attr_names]
        assert all(map(str.isidentifier, attr_names)), (
            f"The following are not valid python 'identifiers' "
            f"{', '.join(a for a in attr_names if not a.isidentifier())}"
        )

        annotations = {attr: Any for attr in attr_names}

        @runtime_checkable
        class HasAttrs(Protocol):
            __annotations__ = annotations

            # def __repr__(self):  # TODO: This is for the instance, need it for the class
            #     return "HasAttrs[{', '.join(annotations)}]"

        return HasAttrs

        # return type(
        #     "HasAttrs",
        #     (Protocol,),
        #     {"__annotations__": {attr: Any for attr in attr_names}},
        # )


# TODO: Complete scary hack: Find another way (see uses)
def is_a_new_type(typ):
    return (
        callable(typ)
        and getattr(typ, "__qualname__", "").startswith("NewType")
        and hasattr(typ, "__supertype__")
    )


def typ_name(typ):
    if is_a_new_type(typ):
        return typ.__name__
    else:
        return typ._name


def is_callable_kind(typ):
    """
    >>> from typing import Callable, Tuple
    >>> is_callable_kind(Callable)
    True
    >>> is_callable_kind(Callable[[int, float], str])
    True
    >>> is_callable_kind(Tuple[int, float, str])
    False
    """
    if is_a_new_type(typ):
        return is_callable_kind(typ.__supertype__)
    return typ_name(typ) == "Callable"
    # Also possible: typ.mro()[0] == __import__('collections.abc').Callable


def input_and_output_types(typ: type):
    """Returns the input and output types

    >>> from typing import Callable, Tuple
    >>> input_types, output_type = input_and_output_types(Callable[[float, int], str])
    >>> assert input_types == [float, int] and output_type == str
    >>> input_types, output_type = input_and_output_types(Callable[[], str])
    >>> assert input_types == [] and output_type == str

    But will fail if `typ` isn't a `Callable`:

    >>> input_and_output_types(Tuple[float, int, str])  # doctest: +NORMALIZE_WHITESPACE
    Traceback (most recent call last):
      ...
    AssertionError: Is not a typing.Callable kind: typing.Tuple[float, int, str]

    Will also fail if `typ` is a Callable but not "parametrized".

    >>> input_and_output_types(Callable)  # doctest: +NORMALIZE_WHITESPACE
    Traceback (most recent call last):
      ...
    AssertionError: Can only be used on a Callable[[...],...] kind: typing.Callable

    """
    if is_a_new_type(typ):
        return input_and_output_types(typ.__supertype__)
    assert is_callable_kind(typ), f"Is not a typing.Callable kind: {typ}"
    typ_args = get_args(typ)
    assert len(typ_args) > 0, f"Can only be used on a Callable[[...],...] kind: {typ}"
    return typ_args[0], typ_args[1]


def dot_string_of_callable_typ(typ):
    input_types, output_type = input_and_output_types(typ)
    return (
        ",".join(map(typ_name, input_types))
        + f" -> {typ_name(typ)} -> "
        + typ_name(output_type)
    )


def dot_strings_of_callable_types(*typs, func_shape="box"):
    for typ in typs:
        yield dot_string_of_callable_typ(typ)
        yield f'{typ_name(typ)} [shape="{func_shape}"]'


# --------------------------------------------------------------------------------------
# Misc

from typing import Any, Dict, Optional, KT
from collections.abc import Callable, Iterator, Iterable

# Define a type alias for clarity in the code
ObjectType = Any


class ObjectClassifier:
    """
    A general-purpose classifier for objects based on a set of verifying functions.

    Each "verifier" checks whether an object belongs to a certain kind (category).

    Example usage:

    >>> from typing import Mapping, Iterable
    >>>
    >>> obj = "test"
    >>> isa = lambda typ: lambda obj: isinstance(obj, typ)
    >>> verifiers = {
    ...     'str': isa(str),
    ...     'mapping': isa(Mapping),
    ...     'iterable': isa(Iterable)
    ... }
    >>> classifier = ObjectClassifier(verifiers)

    Check if the object matches any kind

    >>> classifier.matches(obj)
    True

    Check if the object matches a specific kind

    >>> classifier.matches(obj, 'str')
    True
    >>> classifier.matches(obj, 'mapping')
    False

    Get all matches

    >>> classifier.all_matches(obj)
    {'str': True, 'mapping': False, 'iterable': True}

    Find all matching kinds

    >>> list(classifier.matching_kinds(obj))
    ['str', 'iterable']

    Find the first matching kind (default is to ensure uniqueness, which will fail here)

    >>> classifier.matching_kind(obj)
    Traceback (most recent call last):
      ...
    ValueError: Multiple matches found: ['str', 'iterable']

    Find the first matching kind without uniqueness check

    >>> classifier.matching_kind(obj, assert_unique=False)
    'str'

    """

    def __init__(self, verifiers: dict[KT, Callable[[ObjectType], bool]]):
        """
        Initialize with a dictionary of verifiers. Each verifier is a function
        that returns True or False based on the object's classification.

        :param verifiers: A dictionary mapping kind names (keys) to verifying functions (values).
        """
        self.verifiers = verifiers

    def matches(self, obj: ObjectType, kind: KT | None = None) -> bool:
        """
        Returns True if the object matches the given kind, or matches any kind
        if kind is None.

        :param obj: The object to classify.
        :param kind: The specific kind (verifier key) to check.
        :return: True if the object matches the given or any kind.
        """
        if kind is None:
            # Check against all kinds if kind is not specified
            return any(verifier(obj) for verifier in self.verifiers.values())
        # Check for the specific kind
        return self.verifiers.get(kind, lambda x: False)(obj)

    def all_matches(self, obj: ObjectType) -> dict[KT, bool]:
        """
        Returns a dictionary indicating if the object matches each kind.

        :param obj: The object to classify.
        :return: A dictionary with kind names as keys and True/False as values.
        """
        return {kind: verifier(obj) for kind, verifier in self.verifiers.items()}

    def matching_kind(
        self, obj: ObjectType, *, assert_unique: bool = True
    ) -> KT | None:
        """
        Returns the first kind that matches the object. If assert_unique is True,
        it asserts that only one match exists. Optionally, it can return the value instead of the key.

        :param obj: The object to classify.
        :param assert_unique: Ensures only one kind matches, if True.
        :return: The key of the first matching kind, or None if no match.
        """
        matches = list(self.matching_kinds(obj))
        if assert_unique and len(matches) > 1:
            raise ValueError(f"Multiple matches found: {matches}")
        return matches[0] if matches else None

    def matching_kinds(self, obj: ObjectType) -> Iterator[KT]:
        """
        Returns an iterator of kinds that match the object.

        :param obj: The object to classify.
        :return: An iterator of matching kinds.
        """
        for kind, verifier in self.verifiers.items():
            if verifier(obj):
                yield kind
```

## key_path.py

```python
"""Flattening maps and manipulating key paths"""

from operator import attrgetter
import importlib
from itertools import chain


def obj_to_str_path(
    obj,
    *,
    sep=".",
    name_of_obj=attrgetter("__qualname__"),
    path_of_module=attrgetter("__module__"),
):
    """Get the dotpath reference for an object

    >>> from inspect import Signature
    >>> obj_to_str_path(Signature.replace)
    'inspect.Signature.replace'

    ``obj_to_str_path`` is the inverse of ``str_path_to_obj``

    >>> assert str_path_to_obj(obj_to_str_path(Signature.replace)) == Signature.replace

    Let's try with a different separator.

    >>> path = obj_to_str_path(Signature.replace, sep='/')
    >>> path
    'inspect/Signature.replace'

    Remember to specify the same ``sep`` when you do the inverse!

    >>> assert str_path_to_obj(path, sep='/') == Signature.replace

    You can also pass in your own ``name_of_obj`` and ``path_of_module`` functions.
    For example you want a more permissive version of `name_of_obj` you may consider
    ``i2.name_of_obj``.
    Note, thought, that ``str_path_to_obj`` might not work as an inverse for
    custom ``name_of_obj`` and ``path_of_module`` functions.
    You may have to write your own inverse function in this case.

    """
    return sep.join((path_of_module(obj), name_of_obj(obj)))


def str_path_to_obj(str_path: str, *, sep="."):
    """Loads and returns the object referenced by the string DOTPATH_TO_MODULE.OBJ_NAME"""

    path_parts = iter(str_path.split(sep))
    path_parts = chain.from_iterable(map(lambda x: x.split("."), path_parts))
    module_name = next(path_parts)
    obj = importlib.import_module(module_name)  # assume it's a module
    for item in path_parts:
        new_module_name = module_name + "." + item
        try:
            obj = importlib.import_module(new_module_name)
            module_name = new_module_name
        except ModuleNotFoundError:
            while item:
                try:
                    obj = getattr(obj, item)
                    item = next(path_parts, None)
                except AttributeError:
                    break
    return obj


# --------------------------------------------------------------------------------------

from functools import reduce, wraps
import operator
from collections.abc import MutableMapping


def trans_generator_output(trans):
    def decorator(func):
        @wraps(func)
        def wrapped(*args, **kwargs):
            yield from (trans(x) for x in func(*args, **kwargs))

        return wrapped

    return decorator


class NoDefault:
    pass


NO_DFLT = NoDefault()


# TODO: Could move this functionality to dol.paths, using the tools that are there.
def flatten_dict(d, sep=None, prefix=""):
    """
    Computes a "flat" dict from a nested one. A flat dict's keys are the paths of the input dict.
    These paths will be expressed as tuples of the original keys by defaults.
    If these keys are strings though, you can use sep and prefix to get string representations of the paths.

    :param d: a nested dict
    :param sep: The separator character (or string) in a string representation of the paths.
    :param prefix: A string to prepend on all the paths
    :return: A flat dict

    >>> d = {'a': {
    ...         'a': '2a',
    ...         'c': {'a': 'aca', 'u': 4}
    ...         },
    ...      'c': 3
    ...     }
    >>> flatten_dict(d)
    {('a', 'a'): '2a', ('a', 'c', 'a'): 'aca', ('a', 'c', 'u'): 4, ('c',): 3}
    >>> flatten_dict(d, sep='.')
    {'a.a': '2a', 'a.c.a': 'aca', 'a.c.u': 4, 'c': 3}
    >>> flatten_dict(d, sep='/', prefix='/ROOT/')
    {'/ROOT/a/a': '2a', '/ROOT/a/c/a': 'aca', '/ROOT/a/c/u': 4, '/ROOT/c': 3}
    """
    if sep is None:
        kp = KeyPathMap(d)
    else:
        kp = StrKeyPath(d, sep=sep, prefix=prefix)
    return kp.flat_dict()


def rollout_dict(d, sep=None, prefix=""):
    """
    Get the nested path of a flat (key path) dict. This is the inverse of flatten_dict.

    :param d: a flat dict (i.e. one whose keys are paths of a nested dict)
    :param sep: If None (default), the paths should be key tuples. If a string, it it assumed to be
        the separator of string representations of the path
    :param prefix: A string that has be prepended to all each key (path) of the input dict
        (and therefore should be removed)
    :return: The corresponding nested path

    >>> flat_d = {('a', 'a'): '2a', ('a', 'c', 'a'): 'aca', ('a', 'c', 'u'): 4, ('c',): 3}
    >>> rollout_dict(flat_d)
    {'a': {'a': '2a', 'c': {'a': 'aca', 'u': 4}}, 'c': 3}
    >>> flat_d = {'a.a': '2a', 'a.c.a': 'aca', 'a.c.u': 4, 'c': 3}
    >>> rollout_dict(flat_d, sep='.')
    {'a': {'a': '2a', 'c': {'a': 'aca', 'u': 4}}, 'c': 3}
    >>> flat_d = {'/ROOT/a/a': '2a', '/ROOT/a/c/a': 'aca', '/ROOT/a/c/u': 4, '/ROOT/c': 3}
    >>> rollout_dict(flat_d, sep='/', prefix='/ROOT/')
    {'a': {'a': '2a', 'c': {'a': 'aca', 'u': 4}}, 'c': 3}
    """
    if sep is None:
        kp = KeyPathMap()
    else:
        kp = StrKeyPath(sep=sep, prefix=prefix)
    return kp.rollout(d)


# Note: Might need to extend control of setitem: overwrites, leaf->node, node->leaf
# Note: Could also control deletes more.
class KeyPathMap(MutableMapping):
    """
    Provides a key-path view to a nested mapping (by default, a dict).
    A nested mapping can be see as a tree, where if a value is itself a mapping, it is a non-terminal node,
    leaves (or terminal) holding the "actual values".

    When wrapping a mapping in KeyPathMap, you can pretend that you have a flat mapping from (root to leaf) paths
    instead of a nested structure, and do your mapping CRUD with that view.

    >>> d = {'a': {
    ...         'a': '2a',
    ...         'b': {'a': 'aba',
    ...               'b': 3}
    ...         },
    ...      'c': 3.14
    ...     }
    >>> kp = KeyPathMap(d)
    >>> list(kp.items())
    [(('a', 'a'), '2a'), (('a', 'b', 'a'), 'aba'), (('a', 'b', 'b'), 3), (('c',), 3.14)]
    >>> list(kp)
    [('a', 'a'), ('a', 'b', 'a'), ('a', 'b', 'b'), ('c',)]
    >>> len(kp)
    4
    >>> assert list(kp) == list(kp.keys())
    >>> list(kp.values())
    ['2a', 'aba', 3, 3.14]
    >>> kp['a']
    {'a': '2a', 'b': {'a': 'aba', 'b': 3}}
    >>> kp[('a',)]
    {'a': '2a', 'b': {'a': 'aba', 'b': 3}}
    >>> kp['a', 'a']
    '2a'
    >>> kp['a', 'b', 'b']
    3
    >>> ('a', 'new_key') in kp
    False
    >>> kp['a', 'new_key'] = 'new val'
    >>> ('a', 'new_key') in kp
    True
    >>> kp['a', 'new_key']
    'new val'
    >>> len(kp)
    5
    >>> del kp['a', 'b', 'a']
    >>> len(kp)
    4
    >>> list(kp.items())
    [(('a', 'a'), '2a'), (('a', 'b', 'b'), 3), (('a', 'new_key'), 'new val'), (('c',), 3.14)]
    >>>
    >>> # By default, you can only write on already created nodes. But if auto_node_writes=True, you can do this:
    >>> kp = KeyPathMap(auto_node_writes=True)
    >>> kp
    {}
    >>> kp['a', 'b', 'c'] = 'hi world!'
    >>> kp
    {'a': {'b': {'c': 'hi world!'}}}
    """

    def __init__(
        self,
        store=dict,
        key_type: type = None,
        node_type: type = None,
        auto_node_writes=False,
    ):
        """
        Initialize a KeyPathMap.
        :param store: Your mapping, or the type of your mapping.
        :param key_type: The type of the keys
        :param node_type: The node type (typically the same as the store type
        :param auto_node_writes: False by default, which means you can only write on already created nodes.
            But if True, you can assign to paths with prefixes that don't yet exist. See examples.
        """
        if isinstance(store, type):
            store = store()
        store_type = type(store)
        if node_type is None:
            node_type = store_type
        if key_type is None:
            try:  # consider the type of the first key found in the store as the type for all
                key_type = type(next(iter(store.keys())))
            except StopIteration:
                key_type = str  # default to string keys

        def is_node(x):
            return isinstance(x, node_type)

        def is_key(x):
            return isinstance(x, key_type)

        self.store = store
        self.is_node = is_node
        self.is_key = is_key
        self.mk_new_node = node_type
        self.auto_node_writes = auto_node_writes

    def __getitem__(self, k):
        if not self.is_key(k):
            d = self.store.__getitem__(k[0])
            for kk in k[1:]:
                d = d[kk]
            return d
        else:
            return self.store.__getitem__(k)

    def __setitem__(self, k, v):
        if self.auto_node_writes:
            self._recursive_setitem(k, v)
        else:
            if not self.is_key(k) and len(k) > 1:
                d = self.store.__getitem__(k[0])
                for kk in k[1:-1]:
                    d = d[kk]
                d[k[-1]] = v
            else:
                self.store.__setitem__(k, v)

    def __delitem__(self, k):
        if not self.is_key(k):
            if len(k) == 1:
                self.store.__delitem__(k[0])
            else:
                d = self.store.__getitem__(k[0])
                for kk in k[1:-1]:
                    d = d[kk]
                d.__delitem__(k[-1])
        else:
            self.store.__delitem__(k)

    def items(self):
        return self._items(self.store)

    def __iter__(self):
        return map(lambda x: x[0], self.items())

    def __contains__(self, k):
        if self.is_key(k):
            return self.store.__contains__(k)
        else:
            d = self.store
            for kk in k:
                if not d.__contains__(kk):
                    return False
                else:
                    d = d[kk]
            return True  # if you got so far, you have all the keys in the path

    def __len__(self):
        c = 0
        for _ in self.items():
            c += 1
        return c

    def _recursive_setitem(self, k, v):
        if not self.is_key(k):
            d = self.store
            for kk in k[:-1]:
                if kk not in d:
                    d[kk] = self.mk_new_node()
                d = d[kk]
                # else:
                #     d = d[kk]
                #     assert self.is_node(d), f"Trying to set a value for path {k}, but {kk} was a leaf"
            d[k[-1]] = v
        else:
            self.store.__setitem__(k, v)

    @trans_generator_output(lambda x: (tuple(x[0]), x[1]))
    def _items(self, d, key_path_prefix=None):
        if key_path_prefix is None:
            for k, v in d.items():
                if not self.is_node(v):
                    yield [k], v
                else:
                    for kk, vv in self._items(v, [k]):
                        yield kk, vv
        else:
            for k, v in d.items():
                if not self.is_node(v):
                    yield key_path_prefix + [k], v
                else:
                    if self.is_key(k):
                        k = [k]
                    for kk, vv in self._items(v, k):
                        yield key_path_prefix + list(kk), vv

    def __repr__(self):
        return self.store.__repr__()

    def flat_dict(self):
        return {k: v for k, v in self.items()}

    def rollout(self, d):
        target = self.__class__(auto_node_writes=True)
        for k, v in d.items():
            target[k] = v
        return target.store

    def extract(self, key_paths=None, default_val=NO_DFLT, target=dict):
        if key_paths is None:
            key_paths = self.keys()
        if isinstance(target, type):
            target = target()
        for kp in key_paths:
            try:
                target[kp] = self[kp]
            except KeyError:
                if default_val is not NO_DFLT:
                    target[kp] = default_val
                else:
                    raise
        return target


class StrKeyPath(KeyPathMap):
    """
    A KeyPathMap, but where the key paths are expressed as string with a separator.
    If sep = '.', then instead of using ('a', 'b', 'c') as a key, you can use 'a.b.c'.

    >>> d = {'a': {
    ...         'a': '2a',
    ...         'b': {'a': 'aba',
    ...               'b': 3}
    ...         },
    ...      'c': 3.14
    ...     }
    >>> # Example with sep='/'
    >>> kp = StrKeyPath(d, sep='/')
    >>> list(kp.items())
    [('a/a', '2a'), ('a/b/a', 'aba'), ('a/b/b', 3), ('c', 3.14)]
    >>> # You can also add a prefix to the keys
    >>> kp = StrKeyPath(d, sep='/', prefix="http://")
    >>> list(kp.items())
    [('http://a/a', '2a'), ('http://a/b/a', 'aba'), ('http://a/b/b', 3), ('http://c', 3.14)]
    >>>
    >>> # Default sep is '.', so we'll work with that:
    >>> kp = StrKeyPath(d)
    >>> kp
    {'a': {'a': '2a', 'b': {'a': 'aba', 'b': 3}}, 'c': 3.14}
    >>> list(kp.items())
    [('a.a', '2a'), ('a.b.a', 'aba'), ('a.b.b', 3), ('c', 3.14)]
    >>> list(kp)
    ['a.a', 'a.b.a', 'a.b.b', 'c']
    >>> len(kp)
    4
    >>> assert list(kp) == list(kp.keys())
    >>> list(kp.values())
    ['2a', 'aba', 3, 3.14]
    >>> kp['a']
    {'a': '2a', 'b': {'a': 'aba', 'b': 3}}
    >>> kp['a.a']
    '2a'
    >>> kp['a.b.b']
    3
    >>> ('a.new_key') in kp
    False
    >>> kp['a.new_key'] = 'new val'
    >>> 'a.new_key' in kp
    True
    >>> kp['a.new_key']
    'new val'
    >>> len(kp)
    5
    >>> del kp['a.b.a']
    >>> len(kp)
    4
    >>> list(kp.items())
    [('a.a', '2a'), ('a.b.b', 3), ('a.new_key', 'new val'), ('c', 3.14)]
    >>>
    >>> # By default, you can only write on already created nodes. But if auto_node_writes=True, you can do this:
    >>> kp = StrKeyPath(auto_node_writes=True)
    >>> kp
    {}
    >>> kp['a.b.c'] = 'hi world!'
    >>> kp
    {'a': {'b': {'c': 'hi world!'}}}
    >>>
    """

    def __init__(
        self,
        store=dict,
        key_type: type = None,
        node_type: type = None,
        auto_node_writes=False,
        sep: str = ".",
        prefix: str = "",
    ):
        prefix_length = len(prefix)
        self.sep = sep
        self.prefix = prefix
        self._id_of_key = lambda k: tuple(k[prefix_length:].split(sep))
        self._key_of_id = lambda _id: prefix + sep.join(_id)
        super().__init__(
            store=store,
            key_type=key_type,
            node_type=node_type,
            auto_node_writes=auto_node_writes,
        )

    def __getitem__(self, k):
        return super().__getitem__(self._id_of_key(k))

    def __setitem__(self, k, v):
        return super().__setitem__(self._id_of_key(k), v)

    def __delitem__(self, k):
        return super().__delitem__(self._id_of_key(k))

    def __contains__(self, k):
        return super().__contains__(self._id_of_key(k))

    def items(self):
        yield from ((self._key_of_id(_id), v) for _id, v in super().items())

    def rollout(self, d):
        target = self.__class__(sep=self.sep, prefix=self.prefix, auto_node_writes=True)
        for k, v in d.items():
            target[k] = v
        return target.store


# TODO: Not what is below should remain. Probably better to just use the above.
class KeyPathTrans:
    """
    Doing what StrKeyPath but where the store that is being operated on is not included in the object, but
    given to the method as input.
    """

    def __init__(self, sep: str = ".", node_type: type = dict, mk_new_node=None):
        """

        :param sep:
        :param node_type:
        """
        self.sep = sep
        self.node_type = node_type
        if mk_new_node is None:
            mk_new_node = node_type
        self.mk_new_node = mk_new_node

    def items(self, d, key_path_prefix=None):
        """
        iterate through items of store recursively, yielding (key_path, val) pairs for all nested values that are not
        store types.
        That is, if a value is a store_type, it won't generate a yield, but rather, will be iterated through
        recursively.
        :param d: input store
        :param key_path_so_far: string to be prepended to all key paths (for use in recursion, not meant for direct use)
        :return: a (key_path, val) iterator

        >>> kp = KeyPathTrans()
        >>> input_dict = {
        ...     'a': {
        ...         'a': 'a.a',
        ...         'b': 'a.b',
        ...         'c': {
        ...             'a': 'a.c.a'
        ...         }
        ...     },
        ...     'b': 'b',
        ...     'c': 3
        ... }
        >>> list(kp.items(input_dict))
        [('a.a', 'a.a'), ('a.b', 'a.b'), ('a.c.a', 'a.c.a'), ('b', 'b'), ('c', 3)]
        """
        if key_path_prefix is None:
            for k, v in d.items():
                if not isinstance(v, self.node_type):
                    yield k, v
                else:
                    for kk, vv in self.items(v, k):
                        yield kk, vv
        else:
            for k, v in d.items():
                if not isinstance(v, self.node_type):
                    yield key_path_prefix + self.sep + k, v
                else:
                    for kk, vv in self.items(v, k):
                        yield key_path_prefix + self.sep + kk, vv

    def keys(self, d):
        for k, v in d.items():
            if not isinstance(v, self.node_type):
                yield k
                # key_path_list.append(k)
            else:
                yield from (k + self.sep + x for x in self.keys(v))

    def getitem(self, d, key_path, default_val=None):
        """
        getting with a key list or "."-separated string
        :param d: dict-like
        :param key_path: list or "."-separated string of keys
        :return:
        """
        if isinstance(key_path, str):
            key_path = key_path.split(self.sep)
        try:
            return reduce(operator.getitem, key_path, d)
        except (TypeError, KeyError):
            return default_val

    def setitem(self, d, key_path, val):
        """
        setting with a key list or "."-separated string
        :param d: dict
        :param key_path: list or "."-separated string of keys
        :param val: value to assign
        :return:
        """
        if isinstance(key_path, str):
            key_path = key_path.split(self.sep)
        self.getitem(d, key_path[:-1])[key_path[-1]] = val

    def setitem_recursive(self, d, key_path, val):
        """

        :param d:
        :param key_path:
        :param val:
        :return:

        >>> kp = KeyPathTrans()
        >>> input_dict = {
        ...   "a": {
        ...     "c": "val of a.c",
        ...     "b": 1,
        ...   },
        ...   "10": 10,
        ...   "b": {
        ...     "B": {
        ...       "AA": 3
        ...     }
        ...   }
        ... }
        >>>
        >>> kp.setitem_recursive(input_dict, 'new.key.path', 7)
        >>> input_dict
        {'a': {'c': 'val of a.c', 'b': 1}, '10': 10, 'b': {'B': {'AA': 3}}, 'new': {'key': {'path': 7}}}
        >>> kp.setitem_recursive(input_dict, 'new.key.old.path', 8)
        >>> input_dict
        {'a': {'c': 'val of a.c', 'b': 1}, '10': 10, 'b': {'B': {'AA': 3}}, 'new': {'key': {'path': 7, 'old': {'path': 8}}}}
        >>> kp.setitem_recursive(input_dict, 'new.key', 'new val')
        >>> input_dict
        {'a': {'c': 'val of a.c', 'b': 1}, '10': 10, 'b': {'B': {'AA': 3}}, 'new': {'key': 'new val'}}
        """
        if isinstance(key_path, str):
            key_path = key_path.split(self.sep)
        first_key = key_path[0]
        if len(key_path) == 1:
            d[first_key] = val
        else:
            if first_key in d:
                self.setitem_recursive(d[first_key], key_path[1:], val)
            else:
                d[first_key] = self.node_type()
                self.setitem_recursive(d[first_key], key_path[1:], val)

    def extract_key_paths(
        self,
        d,
        key_paths,
        field_naming="full",
        use_default=False,
        default_val=None,
    ):
        """
        getting with a key list or "."-separated string
        :param d: dict-like
        :param key_path: list or "."-separated string of keys
        :param field_naming: 'full' (default) will use key_path strings as is, leaf will only use the last dot item
            (i.e. this.is.a.key.path will result in "path" being used)
        :return:

        >>> kp = KeyPathTrans()
        >>> d = {
        ...     'a': {
        ...         'a': 'a.a',
        ...         'b': 'a.b',
        ...         'c': {
        ...             'a': 'a.c.a'
        ...         }
        ...     },
        ...     'b': 'b',
        ...     'c': 3
        ... }
        >>> kp.extract_key_paths(d, 'a')
        {'a': {'a': 'a.a', 'b': 'a.b', 'c': {'a': 'a.c.a'}}}
        >>> kp.extract_key_paths(d, 'a.a')
        {'a.a': 'a.a'}
        >>> kp.extract_key_paths(d, 'a.c')
        {'a.c': {'a': 'a.c.a'}}
        >>> kp.extract_key_paths(d, ['a.a', 'a.c'])
        {'a.a': 'a.a', 'a.c': {'a': 'a.c.a'}}
        >>> kp.extract_key_paths(d, ['a.a', 'something.thats.not.there'])  # missing key just won't be included
        {'a.a': 'a.a'}
        >>> kp.extract_key_paths(d, ['a.a', 'something.thats.not.there'], use_default=True, default_val=42)
        {'a.a': 'a.a', 'something.thats.not.there': 42}
        """
        dd = self.mk_new_node()
        if isinstance(key_paths, str):
            key_paths = [key_paths]
        if isinstance(key_paths, self.node_type):
            key_paths = [k for k, v in key_paths.items() if v]

        for key_path in key_paths:
            if isinstance(key_path, str):
                field = key_path
                key_path = key_path.split(self.sep)
            else:
                field = self.sep.join(key_path)

            if field_naming == "leaf":
                field = key_path[-1]
            else:
                field = field

            try:
                dd.update({field: reduce(operator.getitem, key_path, d)})
            except (TypeError, KeyError):
                if use_default:
                    dd.update({field: default_val})

        return dd


# from collections.abc import MutableMapping
#
#
# class AttrStore(MutableMapping):
#     def __getitem__(self, k):
#         if not hasattr(self, k):
#             setattr(self, k, AttrStore())
#         return getattr(self, k)
#
#     def __setitem__(self, k, v):
#         return setattr(self, k, v)
#
#     def __delitem__(self, k):
#         return delattr(self, k)
#
#     def __iter__(self):
#         return iter(self.__dict__)
#
#     def keys(self):
#         return self.__dict__.keys()
#
#     def __contains__(self, k):
#         return hasattr(self, k)
#
#     def __len__(self):
#         return len(list(self))
#
#
# d = AttrStore()
# kp = KeyPathMap(d)
# kp['a'] = '2a'
# print(kp.store.a)
# print(list(kp))
# kp['c']['d'] = 3
# print(list(kp))
# kp['c', 'z'] = 'hi world'
# print(list(kp.items()))

## Not working yet:
# d = AttrStore()
# kp = StrKeyPath(d, sep='.')
# kp['a'] = '2a'
# print(kp.store.a)
# print(list(kp))
# kp['c']['d'] = 3
# print(list(kp))
# kp['c.z'] = 'hi world'
# list(kp.items())
```

## multi_object.py

```python
"""A few fundamental tools to operate on a fixed collection of objects (e.g. functions).

For functions you have:

- ``Pipe``: To compose functions (output of one fed as the input of the next)

- ``FuncFanout``: To apply multiple functions to the same inputs

- ``FlexFuncFanout``: Like `FuncFanout` but where the application of inputs is flexible.
That is, the functions "draw" their inputs from the a common pool, but don't choke
if there are extra unrecognized arguments.

- ``ParallelFuncs``: To make a dict-to-dict function, applying a specific function for
each input key (putting the result in that key in the output.

For context managers you have:

- ``ContextFanout``: To hold multiple context managers as one (entering and exiting
together)

![image](https://user-images.githubusercontent.com/1906276/138004878-bfe17115-c25f-4d22-9740-0fef983507c0.png)

"""

from typing import Union, Any, TypeVar
from collections.abc import Mapping, Iterable, Callable
from inspect import Signature, signature


# Note: For a more powerful, and customizable implementation,
#  see `i2.signatures.name_of_obj`.
# TODO: Analyse usages and reroute to i2.signatures.name_of_obj instead.
#   Reason this duplicate exists might be because we wanted to keep multi_object
#   completely independent from other modules (self-contained).
def name_of_obj(o: object, default=None) -> str | None:
    """
    Tries to find the (or "a") name for an object, even if `__name__` doesn't exist.

    This is a basic implementation, and it's not guaranteed to work for all objects.
    For a more powerful, and customizable implementation,
    see `i2.signatures.name_of_obj`.

    >>> name_of_obj(map)
    'map'
    >>> name_of_obj([1, 2, 3])
    'list'
    >>> name_of_obj(print)
    'print'
    >>> name_of_obj(lambda x: x)
    '<lambda>'
    >>> from functools import partial
    >>> name_of_obj(partial(print, sep=","))
    'print'
    """
    if hasattr(o, "__name__"):
        return o.__name__
    elif hasattr(o, "__class__"):
        name = name_of_obj(o.__class__)
        if name == "partial":
            if hasattr(o, "func"):
                return name_of_obj(o.func)
        return name
    else:
        return default


# Note: Vendored in dol.util and lkj.strings
def truncate_string_with_marker(
    s, *, left_limit=15, right_limit=15, middle_marker="..."
):
    """
    Return a string with a limited length.

    If the string is longer than the sum of the left_limit and right_limit,
    the string is truncated and the middle_marker is inserted in the middle.

    If the string is shorter than the sum of the left_limit and right_limit,
    the string is returned as is.

    >>> truncate_string_with_marker('1234567890')
    '1234567890'

    But if the string is longer than the sum of the limits, it is truncated:

    >>> truncate_string_with_marker('1234567890', left_limit=3, right_limit=3)
    '123...890'
    >>> truncate_string_with_marker('1234567890', left_limit=3, right_limit=0)
    '123...'
    >>> truncate_string_with_marker('1234567890', left_limit=0, right_limit=3)
    '...890'

    If you're using a specific parametrization of the function often, you can
    create a partial function with the desired parameters:

    >>> from functools import partial
    >>> truncate_string = partial(truncate_string_with_marker, left_limit=2, right_limit=2, middle_marker='---')
    >>> truncate_string('1234567890')
    '12---90'
    >>> truncate_string('supercalifragilisticexpialidocious')
    'su---us'

    """
    middle_marker_len = len(middle_marker)
    if len(s) <= left_limit + right_limit:
        return s
    elif right_limit == 0:
        return s[:left_limit] + middle_marker
    elif left_limit == 0:
        return middle_marker + s[-right_limit:]
    else:
        return s[:left_limit] + middle_marker + s[-right_limit:]


def ensure_iterable_of_callables(x):
    """Assert that the input is an iterable of callables,
    or wrap a single callable in an iterable.
    """
    if isinstance(x, Iterable):
        if not all(callable(xx) for xx in x):
            non_callables = filter(lambda xx: not callable(xx), x)
            raise TypeError(f"These were not callable: {list(non_callables)}")
        return x
    else:
        assert callable(x)
        return (x,)


Obj = TypeVar("Obj")


def uniquely_named_objects(
    objects: Iterable[Obj],
    exclude_names: Iterable[str] = (),
    obj_to_name: Callable[[Obj], str] = name_of_obj,
    *,
    name_for_position: dict = (),
):
    """Generate (name, object) pairs from an iterable of objects

    :param objects: Objects to be named
    :param exclude_names: Names that can't be used
    :param obj_to_name: Function that tries to get/make a name from an object
    :param name_for_position: A ``{position_idx: name,...}`` mapping that instructs
    ``uniquely_named_objects`` to use a specific name for a given position.

    >>> from functools import partial
    >>> objects = [map, [1], [1, 2], lambda x: x, partial(print, sep=",")]
    >>> g = uniquely_named_objects(objects)
    >>> names_and_objects = dict(g)
    >>> list(names_and_objects)
    ['map', 'list', '_2', '_3', 'print']

    That ``'_2'`` is there because both ``[1]`` and ``[1, 2]`` would be named `'list'`,
    so to avoid that, a default name (revealing the position of the object in the
    input `objects`) is given.
    The '_3' comes from the fact that the ``lambda`` function doesn't have a proper
    name (one that is a python identifier).

    If we wanted the name for ``[1]`` to revert to the default positional name ``'_1'``,
    we can achieve this by forbidding the name ``'list'``:

    >>> list(dict(uniquely_named_objects(objects, exclude_names={'list'})))
    ['map', '_1', '_2', '_3', 'print']

    You could also acheive this by specifying this ``'_1'`` explicitly in the
    ``name_for_position`` argument:

    >>> list(dict(uniquely_named_objects(objects, name_for_position={1: '_1'})))
    ['map', '_1', 'list', '_3', 'print']

    The reason this `list` reappears as a name is that we didn't exclude it, and
    the name is not taken by the ``[1]`` argument anymore.
    To get the desired effect with ``name_for_position`` we could therefore do this:

    >>> list(dict(uniquely_named_objects(objects, name_for_position={1: '_1', 2: '_2'})))
    ['map', '_1', '_2', '_3', 'print']

    Obviously, ``exclude_names`` is the right argument for the problem above, but
    what ``name_for_position`` does give you is the ability to explicitly chose the
    names you want to assign to all or some of the elements of your iterable.

    >>> list(dict(uniquely_named_objects(
    ...     objects, name_for_position={1: 'first_list', 2: 'second_list', 3: 'lambda'}))
    ... )
    ['map', 'first_list', 'second_list', 'lambda', 'print']

    Extra notes:

    See what ``uniquely_named_objects`` offers as parametrization:

    - You can provide an exclusion list (though the handing of a conflict is hardcoded
    and questionable)

    - You can provide a ``obj_to_name`` function to control the naming of objects.
    One trick to be aware of if objects have unique hashes: Make a
    ``d = {obj: name,...}`` mapping and specify ``obj_to_name=d.get``.

    - Any controllable way to decide on a name based on the position of the function
    in the iterable (this could be useful!)

    What you DO NOT have:

    - Any way to choose names non-myopically: An object’s name cannot “see” the
    objects around it to decide on a name (it can only see the names use by those behind
    it through ``exclude_names``).

    - Any “retries” or “alternative naming logic” if a chosen name conflicts with
    ``exclude_names``
    """
    _exclude_names = set(exclude_names)
    for i, obj in enumerate(objects):
        if i in name_for_position:
            name = name_for_position[i]
        else:
            name = obj_to_name(obj)
        # if name == '<lambda>':
        # name = f'lambda_{i}'
        if name is None or not name.isidentifier() or name in _exclude_names:
            name = f"_{i}"
            assert (
                name not in _exclude_names
            ), "{name} already used in {_exclude_names}!"
        yield name, obj
        _exclude_names.add(name)


def merge_unnamed_and_named(*unnamed, **named) -> dict:
    """To merge unnamed and named arguments into a single (named) dict of arguments

    >>> merge_unnamed_and_named(10, 20, thirty=30, fourty=40)
    {'_0': 10, '_1': 20, 'thirty': 30, 'fourty': 40}
    """
    # TODO: Could do what is done in meshed (try to get names of actual functions)
    #  and use _0, _1, _2... as fallback only?
    named_unnamed = {f"_{i}": obj for i, obj in enumerate(unnamed)}
    if not named_unnamed.keys().isdisjoint(named):
        raise ValueError(
            f"Some of your objects' names clashed: "
            f"{named_unnamed.keys() & named.keys()}"
        )
    return dict(named_unnamed, **named)


# TODO: Refactoring: Remove once not used anymore
def _multi_func_init(self, *unnamed_funcs, **named_funcs) -> None:
    if len(unnamed_funcs) == 1 and isinstance(unnamed_funcs[0], Mapping):
        self.funcs = unnamed_funcs[0]
        expected_n_funcs = len(self.funcs)
    else:
        expected_n_funcs = len(unnamed_funcs) + len(named_funcs)
        self.funcs = merge_unnamed_and_named(*unnamed_funcs, **named_funcs)
    if len(self.funcs) != expected_n_funcs:
        raise ValueError(
            "Some of your func names clashed. Your unnamed funcs were: "
            f"{unnamed_funcs} and your named ones were: {named_funcs}"
        )
    ensure_iterable_of_callables(self.funcs.values())


_dflt_signature = Signature.from_callable(lambda *args, **kwargs: None)


class MultiObj(Mapping):
    """A base class that holds several named objects

    >>> from functools import partial

    Let's make a `MultiObj` with some miscellaneous objects.
    (Note that `MultiObj` will usually be used for specific kinds of objects such as
    callables or context managers. Here we chose the objects to demo what the
    auto-naming does.)

    >>> mo = MultiObj([1], [1, 2], partial(print, sep=","), i='hi', ident=lambda x: x)

    You now have a mapping and can do mapping things such as being able to list keys,
    getting the length, seeing if a key is present, and getting the value for a key.

    Note that the first and second list cannot be assigned the same name without creating a conflict.
    In general one of the following will happen:
    -you give it a name
    -it tries to figure out a non conflicting name (if the object has a dunder name, etc)
    -it falls back to a naming that is just the stringification of the argument's positional index

    >>> list(mo) # not that the second item cannot be 'list', so a different name is given
    ['list', '_1', 'print', 'i', 'ident']
    >>> len(mo)
    5
    >>> mo['_1']
    [1, 2]
    >>> 'list' in mo
    True
    >>> 'not a key of mo' in mo
    False

    When a key (always a string) is also a valid identifier, and in-so-far as
    it doesn't clash with other attributes, `MultiObj` will also give
    you access to the names/keys of your objects via attributes.
    (Note, this is similar to what `pandas.DataFrame` does with it's columns names.)

    >>> mo.list
    [1]
    >>> mo.print
    functools.partial(<built-in function print>, sep=',')

    You can also specify an object mapping directly through a mapping:

    >>> mo = MultiObj({'this': [1], 'that': [1, 2]})
    >>> dict(mo)
    {'this': [1], 'that': [1, 2]}

    You can specify an instance name and/or doc with the special (reserved) argument
    names ``__name__`` and ``__doc__`` (which therefore can't be used as object names:

    >>> mo = MultiObj(
    ... this=[1], that=[1, 2], __name__='this_and_that', __doc__='Nothing much'
    ... )
    >>> dict(mo)
    {'this': [1], 'that': [1, 2]}
    >>> mo.__name__
    'this_and_that'
    >>> mo.__doc__
    'Nothing much'
    """

    # Placing as staticmethod so that subclasses can overwrite if necessary
    auto_namer = staticmethod(uniquely_named_objects)

    def __init__(self, *unnamed, **named):
        named = self._process_reserved_names(named)

        if len(unnamed) == 1 and isinstance(unnamed[0], Mapping) and len(named) == 0:
            # Special case where a single input is given: a mapping between names and obj
            self.objects = unnamed[0]
        else:
            # Normal case: Some named and unnamed objects are given,
            # so we need to get unique names for the unnamed
            named_unnamed = dict(self.auto_namer(unnamed, exclude_names=set(named)))
            # The uniquely_name_objects should take care of this, but extra precaution:
            if not named_unnamed.keys().isdisjoint(named):
                raise ValueError(
                    f"Some of your objects' names clashed: "
                    f"{named_unnamed.keys() & named.keys()}"
                )
            # Merge these
            self.objects = dict(named_unnamed, **named)

    _reserved_names = ("__name__", "__doc__")

    def _process_reserved_names(self, named_funcs):
        for name in self._reserved_names:
            if (value := named_funcs.pop(name, None)) is not None:
                setattr(self, name, value)
        return named_funcs

    def __iter__(self):
        yield from self.objects

    def __getitem__(self, k):
        return self.objects[k]

    def __contains__(self, k):
        return k in self.objects

    def __len__(self):
        return len(self.objects)

    def __getattr__(self, item):
        """Access to those objects that have proper identifier names"""
        if item in self and item.isidentifier():
            return self.objects[item]
        else:
            raise AttributeError(f"Not an attribute: {item}")

    def __repr__(self):
        return f"<{type(self).__name__} containing: {', '.join(self)}>"

    def __getstate__(self):
        return self.__dict__

    def __setstate__(self, state):
        for k, v in state.items():
            setattr(self, k, v)


def iterable_of_callables_validation(funcs: Iterable[Callable]):
    if not isinstance(funcs, Iterable):
        raise TypeError(f"Not an iterable: {funcs}")
    elif not all(callable(xx) for xx in funcs):
        non_callables = filter(lambda f: not callable(f), funcs)
        raise TypeError(f"These were not callable: {list(non_callables)}")


class MultiFunc(MultiObj):
    """A MultiObj, but specialized to contain callable objects only"""

    def __init__(self, *unnamed_funcs, **named_funcs):
        # The basic MultiObj initialization
        super().__init__(*unnamed_funcs, **named_funcs)
        # The extra stuff we want to do with functions
        iterable_of_callables_validation(self.funcs.values())

    @property
    def funcs(self):
        """Alias of .objects, for better readability in the context of MultiFunc"""
        return self.objects


def _signature_from_first_and_last_func(first_func, last_func):
    try:
        input_params = signature(first_func).parameters.values()
    except ValueError:  # function doesn't have a signature, so take default
        input_params = _dflt_signature.parameters.values()
    try:
        return_annotation = signature(last_func).return_annotation
    except ValueError:  # function doesn't have a signature, so take default
        return_annotation = _dflt_signature.return_annotation
    return Signature(input_params, return_annotation=return_annotation)


# Note: There is a similar function in dol.util and meshed, of the same name
class Pipe(MultiFunc):
    """Simple function composition. That is, gives you a callable that implements

    input -> f_1 -> ... -> f_n -> output.

    >>> def foo(a, b=2):
    ...     return a + b
    >>> f = Pipe(foo, lambda x: print(f"x: {x}"))
    >>> f(3)
    x: 5

    You can name functions, but this would just be for documentation purposes.
    The names are completely ignored.

    >>> g = Pipe(
    ...     add_numbers = lambda x, y: x + y,
    ...     multiply_by_2 = lambda x: x * 2,
    ...     stringify = str
    ... )
    >>> g(2, 3)
    '10'

    Notes:
        - Pipe instances don't have a __name__ etc. So some expectations of normal functions are not met.
        - Pipe instance are pickalable (as long as the functions that compose them are)

    You can specify a single functions:

    >>> Pipe(lambda x: x + 1)(1)
    2

    but

    >>> Pipe()
    Traceback (most recent call last):
      ...
    ValueError: You need to specify at least one function!

    You can specify an instance name and/or doc with the special (reserved) argument
    names ``__name__`` and ``__doc__`` (which therefore can't be used as function names):

    >>> f = Pipe(map, add_it=sum, __name__='map_and_sum', __doc__='Apply func and add')
    >>> f(lambda x: x * 10, [1, 2, 3])
    60
    >>> f.__name__
    'map_and_sum'
    >>> f.__doc__
    'Apply func and add'
    """

    def __init__(self, *unnamed_funcs, **named_funcs):
        # The basic MultiFunc initialization
        super().__init__(*unnamed_funcs, **named_funcs)

        # The extra initialization for pipelines
        callables = list(self.funcs.values())
        n_funcs = len(callables)
        if n_funcs == 0:
            raise ValueError("You need to specify at least one function!")

        elif n_funcs == 1:
            other_funcs = ()
            first_func = last_func = callables[0]
        else:
            first_func, *other_funcs = callables
            *_, last_func = other_funcs

        self.__signature__ = _signature_from_first_and_last_func(first_func, last_func)
        self.first_func = first_func
        self.other_funcs = other_funcs

    def __call__(self, *args, **kwargs):
        out = self.first_func(*args, **kwargs)
        for i, func in enumerate(self.other_funcs, 1):
            try:
                out = func(out)
            except Exception as original_error:
                new_error = self._mk_pipe_call_error(
                    original_error, i, out, args, kwargs
                )
                raise new_error from original_error
        return out

    def _mk_pipe_call_error(self, error_obj, i, out, args, kwargs):
        msg = f"Error calling function {self._func_info_str(i)}\n"
        out_str = f"{out}"
        msg += f"on input {truncate_string_with_marker(out_str)}\n"
        msg += "which was the output of previous function "
        msg += f"\t{self._func_info_str(i - 1)}\n"
        args_str = ", ".join(map(str, args))
        kwargs_str = ", ".join(f"{k}={v}" for k, v in kwargs.items())
        msg += f"The error was cause by calling {self} on ({args_str}, {kwargs_str})\n"
        msg += f"Error was: {error_obj}"
        new_error_obj = type(error_obj)(msg)
        new_error_obj.error_context = {
            "Pipe": self,
            "args": args,
            "kwargs": kwargs,
            "func_index": i,
            "func_key": list(self.funcs.keys())[i],
            "func": list(self.funcs.values())[i],
            "func_input": out,
        }
        return new_error_obj

    def _func_info_str(self, i):
        key = list(self.funcs.keys())[i]
        func = self.funcs[key]
        func_name = name_of_obj(func, default=f"")
        return f"(name={func_name}, key={key}, index={i})"

    def __len__(self):
        return len(self.funcs)

    def __eq__(self, other):
        return pipes_are_equal(self, other)


from operator import eq


def _flatten_pipe(pipe):
    for func in pipe.funcs.values():
        if isinstance(func, Pipe):
            yield from _flatten_pipe(func)
        else:
            yield func


def flatten_pipe(pipe):
    """Unravel nested Pipes to get a flat 'sequence of functions' version of input.

    >>> def f(x): return x + 1
    >>> def g(x): return x * 2
    >>> def h(x): return x - 3
    >>> a = Pipe(f, g, h)
    >>> b = Pipe(f, Pipe(g, h))
    >>> len(a)
    3
    >>> len(b)
    2
    >>> c = flatten_pipe(b)
    >>> len(c)
    3
    >>> assert a(10) == b(10) == c(10) == 19
    """
    return Pipe(*_flatten_pipe(pipe))


def pipes_are_equal(p1, p2, *, func_equality=eq, verbose=False):
    """Determine if two pipelines are equal.

    Pipelines are equal if their flattened versions have equal functions.
    Function equality can be controlled by the ``func_equality`` argument.
    The ``verbose`` argument will print some more information about why the pipelines
    are not equal.

    >>> def f(x): return x + 1
    >>> def g(x): return x * 2
    >>> def h(x): return x - 3
    >>> a = Pipe(f, g, h)
    >>> b = Pipe(f, g, h)
    >>> c = Pipe(f, Pipe(g, h))
    >>> assert a(10) == b(10) == c(10) == 19
    >>> pipes_are_equal(a, b)
    True
    >>> pipes_are_equal(a, c)
    True
    >>> pipes_are_equal(Pipe(f, g), Pipe(g, h))
    False
    >>> pipes_are_equal(Pipe(f, g), Pipe(f, g, h))
    False

    Get more information when pipes are not equal.

    >>> pipes_are_equal(Pipe(f, g), Pipe(f, g, h), verbose=True)
    --> Flattened pipes do not have the same number of functions: len(p1)=2 != len(p2)=3
    False

    Change how functions are compared for equality:

    >>> pipes_are_equal(Pipe(lambda x: x), Pipe(lambda x: x))
    False
    >>> from inspect import getsource
    >>> source_equality = lambda f, ff: getsource(f) == getsource(ff)
    >>> pipes_are_equal(
    ...     Pipe(lambda x: x), Pipe(lambda x: x), func_equality=source_equality
    ... )
    True

    """
    p1, p2 = map(flatten_pipe, (p1, p2))
    if len(p1) != len(p2):
        if verbose:
            print(
                f"--> Flattened pipes do not have the same number of functions:"
                f" {len(p1)=} != {len(p2)=}"
            )
        return False  # if not same number of keys or keys different, not equality
    else:
        for func1, func2 in zip(p1.funcs.values(), p2.funcs.values()):
            if not func_equality(func1, func2):
                if verbose:
                    print(f"--> func1 and func2 are not equal: {func1=} != {func2=}")
                return False  # these two funcs are not equal
            # else continue
    return True


class FuncFanout(MultiFunc):
    """Applies multiple functions to the same argument(s) and returns a dict of results.

    You know how `map(func, iterable_of_inputs)` applies a same function to an iterable
    of inputs.
    `FuncFanout` (we could call it `pam`) is a sort of dual; used to apply multiple
    functions to a same input.

    >>> def foo(a):
    ...     return a + 2
    ...
    >>> def bar(a):
    ...     return a * 2
    ...
    >>> groot = lambda a: 'I am groot'
    >>> m = FuncFanout(foo, bar, groot)
    >>>
    >>> list(m(3))
    [('foo', 5), ('bar', 6), ('_2', 'I am groot')]
    >>> dict(m(3))
    {'foo': 5, 'bar': 6, '_2': 'I am groot'}

    Don't like that `_2` name?
    Well, If you specify names to the input functions, they'll be used instead of the
    ones found by the `MultObj.auto_namer`.

    >>> m = FuncFanout(foo, bar_results=bar, groot=groot)
    >>> dict(m(10))
    {'foo': 12, 'bar_results': 20, 'groot': 'I am groot'}

    Or if you want your results as a tuple, you could do:

    >>> tuple(dict(m(10)).values())
    (12, 20, 'I am groot')

    The above, gather in a ``dict`` is one way to get your data, but what calling a
    ``FuncFanout`` instance actually gives you is a generator that yields the
    ``(func_key, func_output)`` pairs one at a time

    Sometimes you may want/need more control though, and prefer to iterate through the
    pairs yourself, and in that case use `call_generator` directly.

    >>> gen = m(10)
    >>> next(gen)
    ('foo', 12)
    >>> next(gen)
    ('bar_results', 20)
    >>> next(gen)
    ('groot', 'I am groot')

    So this gives you control on how you want your data.
    Here's a recipe: Say you want to make a function that gives you the data as a dict
    automatically. You can do this, using ``i2.Pipe``:

    >>> f = Pipe(m, dict)
    >>> f(10)
    {'foo': 12, 'bar_results': 20, 'groot': 'I am groot'}

    Or if you want a tuple:

    >>> from operator import itemgetter, methodcaller
    >>> from functools import partial
    >>> f = Pipe(m, partial(map, itemgetter(1)), tuple)
    >>> f(10)
    (12, 20, 'I am groot')

    """

    def call_generator(self, *args, **kwargs):
        for name, func in self.funcs.items():
            yield name, func(*args, **kwargs)

    def __call__(self, *args, **kwargs):
        return self.call_generator(*args, **kwargs)


from i2.signatures import ch_func_to_all_pk, tuple_the_args, Sig


# TODO: Finish this!
# TODO: Test the handling var positional and var keyword
class FlexFuncFanout(MultiFunc):
    """
    Call multiple functions, using a pool of arguments that they will draw from.

    >>> from i2.tests.objects_for_testing import formula1, sum_of_args, mult, add
    >>> mf1 = FlexFuncFanout(formula1=formula1, mult=mult, add=add)
    >>> kwargs_for_func = mf1.kwargs_for_func(w=1, x=2, z=3, a=4, b=5)

    What's this for? Well, the raison d'etre of `FlexFuncFanout` is to be able to do this:

    >>> assert add(a=4, b=5) == add(**kwargs_for_func['add'])

    This wouldn't work on all functions since some functions have position only arguments (e.g. ``formula1``).
    Therefore ``FlexFuncFanout`` holds a "normalized" form of the functions; namely one that handles such things as
    postion only and varargs.

    # TODO: Make this work!
    #   Right now raises: TypeError: formula1() got some positional-only arguments passed as keyword arguments: 'w'
    # >>> assert formula1(1, x=2, z=3) == mf1.normalized_funcs[formula1](**kwargs_for_func[formula1])

    Note: In the following, it looks like ``FlexFuncFanout`` instances return dicts whose keys are strings.
    This is not the case.
    The keys are functions: The same functions that were input.
    The reason for not using functions is that when printed, they include their hash, which invalidates the doctests.

    # >>> def print_dict(d):  # just a util for this doctest
    # ...     from pprint import pprint
    # ...     pprint({k.__name__: d[k] for k in sorted(d, key=lambda x: x.__name__)})

    >>> mf1 = FlexFuncFanout(formula1, mult=mult, addition=add)
    >>> assert mf1.kwargs_for_func(w=1, x=2, z=3, a=4, b=5) == {
    ... 'formula1': {'w': 1, 'x': 2, 'z': 3},
    ... 'mult': {'x': 2},
    ... 'addition': {'a': 4, 'b': 5},
    ... }

    Oh, and you can actually see the signature of kwargs_for_func:

    >>> from inspect import signature
    >>> signature(mf1)
    <Sig (w, x: float, a, y=1, z: int = 1, b: float = 0.0)>

    >>> mf2 = FlexFuncFanout(formula1, mult, addition=add, mysum=sum_of_args)
    >>> assert mf2.kwargs_for_func(
    ...     w=1, x=2, z=3, a=4, b=5, args=(7,8), kwargs={'a': 42}, extra_stuff='ignore'
    ... ) == {
    ... 'formula1': {'w': 1, 'x': 2, 'z': 3},
    ... 'mult': {'x': 2},
    ... 'addition': {'a': 4, 'b': 5},
    ... 'mysum': {'args': (7, 8), 'kwargs': {'a': 42}}}

    """

    # FIXME: TODO: This does indeed change the signature, but not the functionality (position only still raise errors!)
    def normalize_func(self, func):
        return ch_func_to_all_pk(tuple_the_args(func))

    def __init__(self, *unnamed_funcs, **named_funcs):
        # The basic MultiFunc initialization
        super().__init__(*unnamed_funcs, **named_funcs)

        # The extra initialization for FlexFuncFanouts
        self.sigs = {name: Sig(func) for name, func in self.funcs.items()}
        self.normalized_funcs = {
            name: self.normalize_func(func) for name, func in self.funcs.items()
        }
        multi_func_sig = Sig.from_objs(*self.normalized_funcs.values())
        # TODO: Finish attempt to add **all_other_kwargs_ignored to the signature
        # multi_func_sig = (Sig.from_objs(
        #     *self.normalized_funcs.values(),
        #     Parameter(name='all_other_kwargs_ignored', kind=Parameter.VAR_KEYWORD)))
        multi_func_sig.wrap(self)
        # multi_func_sig.wrap(self.kwargs_for_func)

    def kwargs_for_func(self, *args, **kwargs):
        return {
            name: self.sigs[name].source_arguments(**kwargs)
            for name, func in self.funcs.items()
        }

    # TODO: Give it a signature (needs to be done in __init__)
    # TODO: Validation of inputs
    def __call__(self, *args, **kwargs):
        return {
            name: self.sigs[name].source_arguments(**kwargs)
            for name, func in self.funcs.items()
        }


class ParallelFuncs(MultiFunc):
    """Make a multi-channel function from a {name: func, ...} specification.

    >>> multi_func = ParallelFuncs(
    ...     say_hello=lambda x: f"hello {x}", say_goodbye=lambda x: f"goodbye {x}"
    ... )
    >>> multi_func({'say_hello': 'world', 'say_goodbye': 'Lenin'})
    {'say_hello': 'hello world', 'say_goodbye': 'goodbye Lenin'}

    :param spec: A map between a name (str) and a function associated to that name
    :return: A function that takes a dict as an (multi-channel) input and a dict as a
    (multi-channel) output

    Q: Why can I specify the specs both with named_funcs_dict and **named_funcs?
    A: Look at the ``dict(...)`` interface. You see the same thing there.
    Different reason though (here we assert that the keys don't overlap).
    Usually named_funcs is more convenient, but if you need to use keys that are not
    valid python variable names,
    you can always use named_funcs_dict to express that!

    >>> multi_func = ParallelFuncs({
    ...     'x+y': lambda d: f"sum is {d}",
    ...     'x*y': lambda d: f"prod is {d}"}
    ... )
    >>> multi_func({
    ...     'x+y': 5,
    ...     'x*y': 6
    ... })
    {'x+y': 'sum is 5', 'x*y': 'prod is 6'}

    You can also use both. Like with ``dict(...)``.

    Here's a more significant example.

    >>> chunkers = {
    ...     'a': lambda x: x[0] + x[1],
    ...     'b': lambda x: x[0] * x[1]
    ... }
    >>> featurizers = {
    ...     'a': lambda z: str(z),
    ...     'b': lambda z: [z] * 3
    ... }
    >>> multi_chunker = ParallelFuncs(**chunkers)
    >>> multi_chunker({'a': (1, 2), 'b': (3, 4)})
    {'a': 3, 'b': 12}
    >>> multi_featurizer = ParallelFuncs(**featurizers)
    >>> multi_featurizer({'a': 3, 'b': 12})
    {'a': '3', 'b': [12, 12, 12]}
    >>> my_pipe = Pipe(multi_chunker, multi_featurizer)
    >>> my_pipe({'a': (1, 2), 'b': (3, 4)})
    {'a': '3', 'b': [12, 12, 12]}

    #{'a': '(1, 2)', 'b': [(3, 4), (3, 4), (3, 4)]}

    """

    def _key_output_gen(self, d: dict):
        for key, func in self.funcs.items():
            yield key, func(d[key])

    def __call__(self, d: dict):
        return dict(self._key_output_gen(d))


# from collections.abc import ValuesView as BaseValuesView
# class MongoValuesView(ValuesView):
#     def __contains__(self, v):
#         m = self._mapping
#         cursor = m.mgc.find(filter=m._merge_with_filt(v), projection=())
#         return next(cursor, end_of_cursor) is not end_of_cursor
#
#     def __iter__(self):
#         m = self._mapping
#         return m.mgc.find(filter=m.filter, projection=m._getitem_projection)


class ContextFanout(MultiObj):
    """Encapsulates multiple objects into a single context manager that will enter and
    exit all objects that are context managers themselves.

    Context managers show up in situations where you need to have some setup and tear
    down before performing some tasks. It's what you get when you open a file to read
    or write in it, or open a data-base connection, etc.

    Sometimes you need to perform a task that involves more than one context managers,
    or even some objects that may or may not be context managers.
    What `ContextFanout` does for you is allow you to bundle all those (perhaps)
    context managers together, and use them as one single context manager.

    In python 3.10+ you can bundle contexts together by specifying a tuple of context
    managers, as such:

    ```python
    with (open('file.txt'), another_context_manager):
        ...
    ```

    But
    - Python will complain if one of the members of the tuple is not a context manager.
    - A tuple of context managers is not a context manager itself, it's just understood
    by the with (in python 3.10+).

    As an example, let's take two objects. One is a context manager, the other not.

    >>> from contextlib import contextmanager
    >>> @contextmanager
    ... def some_context_manager(x):
    ...     print('open')
    ...     yield f'x + 1 = {x + 1}'
    ...     print('close')
    ...
    >>> def not_a_context_manager(x):
    ...     return x - 1
    ...


    >>> c = ContextFanout(not_a_context_manager, some_context_manager(2))
    >>> list(c)
    ['not_a_context_manager', '_GeneratorContextManager']

    The name (chosen by `MultiObj.auto_namer`) '_GeneratorContextManager' isn't the best.
    Let's give an explicit name:

    >>> c = ContextFanout(not_a_context_manager, context=some_context_manager(2))
    >>> list(c.objects)
    ['not_a_context_manager', 'context']

    See from the prints that "with-ing" c triggers the enter and exit of 'context'

    >>> with c:
    ...     pass
    open
    close

    # Further, know that within the context's scope, a `ContextFanout`
    # instance will have the context managers it contains available, and having the
    # value it is supposed to have "under context".
    #
    # >>> with ContextFanout(not_a_context_manager, some_context_manager(2)) as (f, m):
    # ...     print(f(10))
    #
    #
    # >>> c = ContextFanout(not_a_context_manager, context=some_context_manager(2))
    #
    # >>> c = ContextFanout(
    # ...     some_context_manager=some_context_manager(2),
    # ...     not_a_context_manager=not_a_context_manager
    # ... )
    # >>> # first c doesn't have the some_context_manager attribute
    # >>> assert not hasattr(c, 'some_context_manager')
    # >>> with c:
    # ...     # inside the context, c indeed has the attribute, and it has the expected value
    # ...     assert c.some_context_manager == 'x + 1 = 3'
    # open
    # close
    # >>> # outside the context, c doesn't have the some_context_manager attribute any more again
    # >>> assert not hasattr(c, 'some_context_manager')
    #
    # If you don't specify a name for a given context manager, you'll still have access
    # to it via a hidden attribute ("_i" where i is the index of the object when
    # the `ContextFanout` instance was made.
    #
    # >>> c = ContextFanout(some_context_manager(10), not_a_context_manager)
    # >>> with c:
    # ...     assert c._0 == 'x + 1 = 11'
    # open
    # close

    """

    def __enter__(self):
        for name, obj in self.items():
            if hasattr(obj, "__enter__"):
                obj.__enter__()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        for name, obj in self.items():
            if hasattr(obj, "__exit__"):
                obj.__exit__(exc_type, exc_val, exc_tb)
```

## routing_forest.py

```python
"""
Tools to specify functions through trees and forests.

Whaaa?!?

Well, you see, often -- especially when writing transformers -- you have a series of
if/then conditions nested into eachother, in code, where it gets ugly and un-reusable.

This module explores ways to objectivy this: That is, to give us the means to create
such nested conditions in a way that we can define the parts as reusable operable
components.

Think of the relationship between the for loop (code) and the iterator (object), along
with iterator tools (itertools).
This is what we're trying to explore, but for if/then conditions.

I said explore. Some more work is needed here to make it robust and easily usable.

Let's look at an example involving the three main actors of our play.
Each of these are ``Iterable`` and ``Callable`` (``Generator`` to be precise).

- ``CondNode``: implements the if/then (no else) logic
- ``FinalNode``: Final -- yields (both with call and iter) it's single `.val` attribute.
- ``RoutingForest``: An Iterable of ``CondNode``

You'll note that instances of these classes are all both callables and iterables,
and that when called, they return iterables.
It's this aspect that makes us be able to nest conditions within conditions,
and further, control the flow of the iteration from outside.
A routing node (or forest) called on an object will yield all values that match the
conditions that were specified for it.
For example, if you need all matches, you can wrap it with ``list``, if you need the
first match only, you can wrap it with ``next``, if you have a default value,
you can wrap it in ``next`` with a default value.

>>> import inspect
>>>
>>> def could_be_int(obj):
...     if isinstance(obj, int):
...         b = True
...     else:
...         try:
...             int(obj)
...             b = True
...         except ValueError:
...             b = False
...     if b:
...         print(f'{inspect.currentframe().f_code.co_name}')
...     return b
...
>>> def could_be_float(obj):
...     if isinstance(obj, float):
...         b = True
...     else:
...         try:
...             float(obj)
...             b = True
...         except ValueError:
...             b = False
...     if b:
...         print(f'{inspect.currentframe().f_code.co_name}')
...     return b
...
>>> print(
...     could_be_int(30),
...     could_be_int(30.3),
...     could_be_int('30.2'),
...     could_be_int('nope'),
... )
could_be_int
could_be_int
True True False False
>>> print(
...     could_be_float(30),
...     could_be_float(30.3),
...     could_be_float('30.2'),
...     could_be_float('nope'),
... )
could_be_float
could_be_float
could_be_float
True True True False
>>> assert could_be_int('30.2') is False
>>> assert could_be_float('30.2') is True
could_be_float
>>>
>>> st = RoutingForest(
...     [
...         CondNode(
...             cond=could_be_int,
...             then=RoutingForest(
...                 [
...                     CondNode(
...                         cond=lambda x: int(x) >= 10,
...                         then=FinalNode('More than a digit'),
...                     ),
...                     CondNode(
...                         cond=lambda x: (int(x) % 2) == 1,
...                         then=FinalNode("That's odd!"),
...                     ),
...                 ]
...             ),
...         ),
...         CondNode(cond=could_be_float, then=FinalNode('could be seen as a float')),
...     ]
... )
>>> assert list(st('nothing I can do with that')) == []
>>> assert list(st(8)) == ['could be seen as a float']
could_be_int
could_be_float
>>> assert list(st(9)) == ["That's odd!", 'could be seen as a float']
could_be_int
could_be_float
>>> assert list(st(10)) == ['More than a digit', 'could be seen as a float']
could_be_int
could_be_float
>>> assert list(st(11)) == [
...     'More than a digit',
...     "That's odd!",
...     'could be seen as a float',
... ]
could_be_int
could_be_float
>>>
>>> print(
...     '### RoutingForest ########################################################################################'
... )
### RoutingForest ########################################################################################
>>> rf = RoutingForest(
...     [
...         SwitchCaseNode(
...             switch=lambda x: x % 5,
...             cases={0: FinalNode('zero_mod_5'), 1: FinalNode('one_mod_5')},
...             default=FinalNode('default_mod_5'),
...         ),
...         SwitchCaseNode(
...             switch=lambda x: x % 2,
...             cases={0: FinalNode('even'), 1: FinalNode('odd')},
...             default=FinalNode('that is not an int'),
...         ),
...     ]
... )
>>>
>>> assert list(rf(5)) == ['zero_mod_5', 'odd']
>>> assert list(rf(6)) == ['one_mod_5', 'even']
>>> assert list(rf(7)) == ['default_mod_5', 'odd']
>>> assert list(rf(8)) == ['default_mod_5', 'even']
>>> assert list(rf(10)) == ['zero_mod_5', 'even']
>>>

"""

from itertools import chain
from dataclasses import dataclass
from i2.util import LiteralVal, mk_sentinel

from typing import TypeVar, Tuple, Any
from collections.abc import Iterable, Callable, Mapping, MutableMapping


def return_sentinel(obj: Any, sentinel: Any = None):
    """Return a constanc sentinel value when called. Use partial to set sentinel"""
    return sentinel


def identity(obj: Any):
    return obj


class DelegateToMappingAttrMixin:
    """
    A mixin to delegate ``Mapping`` methods to a mapping attribute called ``mapping``
    """

    mapping = None

    def __getitem__(self, k):
        return self.mapping[k]

    def __iter__(self):
        yield from self.mapping

    def __len__(self):
        return len(self.mapping)

    def __delitem__(self, k):
        del self.mapping[k]

    def __setitem__(self, k, v):
        self.mapping[k] = v


# Note: KeyFuncMapping is very similar to the SwitchCaseMapping in i2.routing_forest,
#  but the latter builds on a more extensive framework for routing.
@dataclass
class KeyFuncMapping(DelegateToMappingAttrMixin, MutableMapping):
    """Implements a switch-case-like mapping with a callable key function.

    The purpose of ``KeyFuncMapping`` is to  allow switch-case logic to be
    given as a plugin specification.

    >>> from i2.routing_forest import KeyFuncMapping
    >>>
    >>> get_extension = lambda x: x.split('.')[-1]
    >>>
    >>> data_type = KeyFuncMapping(
    ...     {'csv': 'table', 'xls': 'table', 'wav': 'audio'}, key=get_extension
    ... )

    Calling a ``KeyFuncMapping`` instance will call the ``key`` function on the input,
    then look up the result in the ``mapping``.

    >>> data_type('my_file.csv')
    'table'
    >>> data_type('another_file.xls')
    'table'
    >>> data_type('sound.wav')
    'audio'

    If the key is not found in the mapping, the ``default_factory`` is **called**
    with the input and the result is returned. The default ``default_factory`` is
    ``return_sentinel``, which by default returns ``None``

    >>> assert data_type('poem.txt') is None

    Note that instances of ``KeyFuncMapping`` are also ``Mapping``s, so all ``Mapping``
    methods can be used.

    >>> list(data_type)
    ['csv', 'xls', 'wav']
    >>> dict(data_type)
    {'csv': 'table', 'xls': 'table', 'wav': 'audio'}

    Including ``update``, which constitutes a convenient way to extend the mapping.

    >>> data_type.update(txt='text')
    >>> data_type('poem.txt')
    'text'

    The ``default_factory`` can be set to any callable, including a
    ``KeyFuncMapping`` itself, which enables us to define an ``else`` for the
    switch-case logic that a ``KeyFuncMapping`` implements.
    Say, for example, if no handled extension is found, we want to check the protocol
    of the input string instead. This is not only a new mapping, but also a new key
    function. We can do it as such:

    >>> get_protocol = lambda x: x.split('://')[0]
    >>> protocol = KeyFuncMapping({'https': 'url'}, get_protocol)
    >>> new_data_type = KeyFuncMapping(
    ...     data_type.mapping, data_type.key, default_factory=protocol
    ... )
    >>> new_data_type('notes.txt')
    'text'
    >>> new_data_type('https://www.python.org/')
    'url'

    Given how useful this pattern is, we made the ``+`` operator implement this.
    Note that here, ``+`` is not associative or commutative (as with numbers).
    It should be understood to function more like the ``+`` for iterables like ``list``
    and ``tuple``.

    >>> nested = data_type + protocol
    >>> nested('https://www.python.org/')
    'url'
    >>> nested('jazz.wav')
    'audio'

    """

    mapping: MutableMapping
    key: Callable = identity  # TODO: Design: should we default this at all?
    default_factory: Callable = return_sentinel

    def __post_init__(self):
        assert callable(self.default_factory), f"{self.default_factory} is not callable"

    # Note: An earlier version used __getitem__ itself instead of __call__ as the
    #  obj routing method, but the misalignment between the keys listed by __iter__ and
    #  the values that could be resolved by __getitem__ lead to unintuitive behaviors
    #  with other mapping methods.
    # Note: The default_factory is a Callable instead of a value because the former is
    # more general and enables recursivity: That is, one can define another
    # KeyFuncMapping to be specified as a default, thus specifying a tree
    # (or rather forest) of conditionals.
    # Note: Could use a mapping.get(..., sentinel) instead of the `obj_key in mapping`
    # check, which would be more efficient, but perhaps less readable/debuggable
    def __call__(self, obj):
        obj_key = self.key(obj)
        if obj_key in self.mapping:  # if the obj_key is in the mapping...
            # ... return the mapping's value for obj_key...
            return self.mapping[obj_key]
        else:
            return self.default_factory(obj)  # ... else call the default function

    # TODO: Is this too hacky? Should we not use __add__ for this? Maybe use
    #  ``extend`` instead? Maybe another method? Maybe not add this convenience at all?
    def __add__(self, other: Callable):
        """Make a copy of the KeyFuncMapping instance with other as default_factory"""
        self_type = type(self)
        assert callable(
            other
        ), f"Can only add a callable (usually a {self_type} itself) to a {self_type}"
        return self_type(self.mapping, self.key, default_factory=other)


# --------------------------------------------------------------------------------------
# A proposal for a routing forest OO backend and convenience mini-language
Obj = TypeVar("Obj")
Output = TypeVar("Output")
Cond = Callable[[Obj], bool]
Then = Callable[[Obj], Output]
Rule = tuple[Cond, Then]
Rules = Iterable[Rule]


# TODO: Think a bit harder about this mini-language
# TODO: Right now use has to explicitly declare final nodes. Can do better.
# TODO: This mini language can itself be expressed as a routing forest. Do it!
def _default_mini_lang(x):
    # TODO: One is really tempted to use CondNode here to define this, right?
    if isinstance(x, (CondNode, RoutingForest, SwitchCaseNode)):
        return x
    elif isinstance(x, LiteralVal):
        return x.get_val()
    elif isinstance(x, tuple):
        if len(x) == 2:
            return CondNode(*map(_default_mini_lang, x))
        elif len(x) == 3:
            return SwitchCaseNode(*map(_default_mini_lang, x))
        else:
            raise ValueError(
                f"If a tuple, element must be a `(cond, then)` pair, "
                f"or a (switch, case, default) triple, or a `Literal`. Was: {x}"
            )
    elif isinstance(x, dict):
        keys = set(x)
        if 2 <= len(keys) <= 3:
            if {"cond", "then"}.issubset(keys):
                return CondNode(**{k: _default_mini_lang(v) for k, v in x.items()})
            elif {"switch", "case"}.issubset(keys) and keys == {
                "switch",
                "case",
                "default",
            }:
                return SwitchCaseNode(
                    **{k: _default_mini_lang(v) for k, v in x.items()}
                )
            else:
                raise ValueError(
                    "keys should be 'switch', 'case' and optionally 'default'. "
                    f"Were: {keys}"
                )
        else:
            raise ValueError(
                f"A non-Literal dict must have keys 'cond' and 'then' "
                f"or 'switch' and 'case' (and optionally 'default'). Was: {x}"
            )
    elif isinstance(x, list):
        return RoutingForest(cond_nodes=list(map(_default_mini_lang, x)))
    else:
        return x


class RoutingNode:
    """A RoutingNode instance needs to be callable on a single object,
    yielding an iterable or a final value"""

    def __call__(self, obj) -> Iterable:
        raise NotImplementedError("You should implement this.")

    @staticmethod
    def from_object(x, mini_lang=_default_mini_lang):
        """Converts an object to a RoutingNode instance.
        Enables mini-languages to be developed for defining routing trees.
        """
        return mini_lang(x)


@dataclass
class FinalNode(RoutingNode):
    """A RoutingNode that is final.
    It yields (both with call and iter) it's single `.val` attribute."""

    val: Any

    def __call__(self, obj=None):
        yield self.val

    def __iter__(self):
        yield self.val

    # def __getstate__(self):
    #     return {'val': self.val}


# TODO: Add tooling to merge validation into `then` functions/values
@dataclass
class CondNode(RoutingNode):
    """A RoutingNode that implements the if/then (no else) logic"""

    cond: Callable[[Any], bool]
    then: Any

    def __call__(self, obj):
        if self.cond(obj):
            # The yield from is important here. It's what allows us to nest further
            # routing nodes.
            # as it allows the then node to
            # contain
            # an iterable (probably a RoutingForest) that is yielded as a single
            yield from self.then(obj)

    def __iter__(self):
        # TODO: Why `yield from` instead of just `yield`?
        yield from self.then


@dataclass
class RoutingForest(RoutingNode):
    """

    >>> rf = RoutingForest([
    ...     CondNode(cond=lambda x: isinstance(x, int),
    ...              then=RoutingForest([
    ...                  CondNode(cond=lambda x: int(x) >= 10, then=FinalNode('More than a digit')),
    ...                  CondNode(cond=lambda x: (int(x) % 2) == 1, then=FinalNode("That's odd!"))])
    ...             ),
    ...     CondNode(cond=lambda x: isinstance(x, (int, float)),
    ...              then=FinalNode('could be seen as a float')),
    ... ])
    >>> assert list(rf('nothing I can do with that')) == []
    >>> assert list(rf(8)) == ['could be seen as a float']
    >>> assert list(rf(9)) == ["That's odd!", 'could be seen as a float']
    >>> assert list(rf(10)) == ['More than a digit', 'could be seen as a float']
    >>> assert list(rf(11)) == ['More than a digit', "That's odd!", 'could be seen as a float']
    """

    cond_nodes: Iterable

    def __post_init__(self):
        self.cond_nodes = list(map(self.from_object, self.cond_nodes))

    def __call__(self, obj):
        yield from chain(*(cond_node(obj) for cond_node in self.cond_nodes))
        # for cond_node in self.cond_nodes:
        #     yield from cond_node(obj)

    def __iter__(self):
        # TODO: Why are we chaining cond_nodes?

        yield from chain(*self.cond_nodes)


Feature = TypeVar("Feature")
Featurizer = Callable[[Obj], Feature]
FeatCondThenMap = Mapping[Feature, Any]


@dataclass
class FeatCondNode(RoutingNode):
    """A RoutingNode that yields multiple routes, one for each of several conditions
    met, where the condition is computed implements computes a feature of the obj and
    according to an iterable of conditions on the feature.

    >>> fcn = FeatCondNode(
    ...     feat=lambda x: x % 5,
    ...     feat_cond_thens=[
    ...         (lambda x: x == 0, lambda x: 'zero_mod_5'),
    ...         (lambda x: x == 1, lambda x: 'one_mod_5'),
    ...         (lambda x: x == 2, lambda x: 'two_mod_5'),
    ...         (lambda x: x == 3, lambda x: 'three_mod_5'),
    ...         (lambda x: x == 4, lambda x: 'four_mod_5'),
    ...     ]
    ... )
    >>> assert list(fcn(0)) == ['zero_mod_5']
    >>> assert list(fcn(1)) == ['one_mod_5']
    >>> assert list(fcn(2)) == ['two_mod_5']
    >>> assert list(fcn(3)) == ['three_mod_5']
    >>> assert list(fcn(4)) == ['four_mod_5']
    >>> assert list(fcn(5)) == ['zero_mod_5']
    >>> assert list(fcn(6)) == ['one_mod_5']

    """

    feat: Featurizer
    feat_cond_thens: Iterable[tuple[Callable[[Feature], bool], Any]]

    @classmethod
    def from_feature_val_map(cls, feat, feat_cond_thens: FeatCondThenMap):
        """
        A FeatCondNode where the conditions are equality checks on the feature value

        # >>> fvn = FeatCondNode.from_feature_val_map(
        # ...     feat=lambda x: x % 3,
        # ...     feat_cond_thens={
        # ...         0: lambda x: 'zero_mod_3',
        # ...         1: lambda x: 'one_mod_3',
        # ...         2: lambda x: 'two_mod_3',
        # ...     }
        # ... )
        # >>> list(fvn(0))
        #
        # >>> assert list(fvn(0)) == ['zero_mod_3']
        # >>> assert list(fvn(1)) == ['one_mod_3']
        # >>> assert list(fvn(2)) == ['two_mod_3']
        #


        """
        feat_cond_map = dict(feat_cond_thens)
        feat_cond_thens = tuple(
            (lambda x: x == feat_val, then) for feat_val, then in feat_cond_map.items()
        )
        self = cls(feat, feat_cond_thens)
        self.feat_cond_map = feat_cond_map
        return self

    def __call__(self, obj):
        feature = self.feat(obj)
        for cond, then in self.feat_cond_thens:
            if cond(feature):
                yield then(obj)

    def __iter__(self):
        # yield from chain.from_iterable(self.feat_cond_map.values())
        yield from self.feat_cond_thens

    # def __call__(self, obj):
    #     feature = self.feat(obj)
    #     val = self.feat_cond_thens_map.get(feature, no_such_key)
    #     if val is not no_such_key:
    #         yield val
    #


# class FeatValNode(FeatCondNode):
#     """A FeatCondNode where the conditions are equality checks on the feature value
#
#     >>> fvn = FeatValNode(
#     ...     feat=lambda x: x % 3,
#     ...     feat_cond_thens={
#     ...         0: lambda x: 'zero_mod_3',
#     ...         1: lambda x: 'one_mod_3',
#     ...         2: lambda x: 'two_mod_3',
#     ...     }
#     ... )
#     >>> list(fvn(0))
#     ['zero_mod_3']
#     >>> list(fvn(0))
#     ['zero_mod_3']
#     #
#     # >>> assert list(fvn(0)) == ['zero_mod_3']
#     # >>> assert list(fvn(1)) == ['one_mod_3']
#     # >>> assert list(fvn(2)) == ['two_mod_3']
#     # >>> assert list(fvn(3)) == ['zero_mod_3']
#
#
#     """
#
#     def __init__(self, feat, feat_cond_thens: FeatCondThensMap):
#         feat_cond_map = dict(feat_cond_thens)
#         feat_cond_thens = (
#             (lambda x: x == feat_val, then) for feat_val, then in feat_cond_map.items()
#         )
#         super().__init__(feat, feat_cond_thens)
#         self.feat_cond_map = feat_cond_map


NoDefault = type("NoDefault", (object,), {})
NO_DFLT = NoDefault()


@dataclass
class SwitchCaseNode(RoutingNode):
    """A RoutingNode that implements the switch/case/else logic.
    It's just a specialization (enhanced with a "default" option) of the FeatCondNode
    class to a situation where the cond function of feat_cond_thens is equality,
    therefore the routing can be
    implemented with a {value_to_compare_to_feature: then_node} map.

    :param switch: A function returning the feature of an object we want to switch on
    :param cases: The mapping from feature to RoutingNode that should be yield for that
    feature. It is often a dict, but only requirement is that it implements the
    ``cases.get(val, default)`` method.
    :param default: Default RoutingNode to yield if no

    >>> rf = RoutingForest([
    ...     SwitchCaseNode(switch=lambda x: x % 5,
    ...                    cases={0: FinalNode('zero_mod_5'), 1: FinalNode('one_mod_5')},
    ...                    default=FinalNode('default_mod_5')),
    ...     SwitchCaseNode(switch=lambda x: x % 2,
    ...                    cases={0: FinalNode('even'), 1: FinalNode('odd')},
    ...                    default=FinalNode('that is not an int')),
    ... ])
    >>>
    >>> assert(list(rf(5)) == ['zero_mod_5', 'odd'])
    >>> assert(list(rf(6)) == ['one_mod_5', 'even'])
    >>> assert(list(rf(7)) == ['default_mod_5', 'odd'])
    >>> assert(list(rf(8)) == ['default_mod_5', 'even'])
    >>> assert(list(rf(10)) == ['zero_mod_5', 'even'])
    """

    switch: Callable
    cases: Mapping
    default: Any = NO_DFLT

    def __call__(self, obj):
        feature = self.switch(obj)
        if self.default is NO_DFLT:
            yield from self.cases.get(feature)(obj)
        else:
            yield from self.cases.get(feature, self.default)(obj)

    def __iter__(self):
        yield from chain(*self.cases.values())
        if self.default:
            yield self.default


def wrap_leafs_with_final_node(x):
    for xx in x:
        if isinstance(xx, RoutingNode):
            yield xx
        else:
            yield FinalNode(xx)


def test_routing_forest():
    def could_be_int(obj):
        if isinstance(obj, int):
            b = True
        else:
            try:
                int(obj)
                b = True
            except ValueError:
                b = False
        # if b:
        #     print(f'{inspect.currentframe().f_code.co_name}')
        return b

    def could_be_float(obj):
        if isinstance(obj, float):
            b = True
        else:
            try:
                float(obj)
                b = True
            except ValueError:
                b = False
        # if b:
        #     print(f'{inspect.currentframe().f_code.co_name}')
        return b

    assert could_be_int(30)
    assert could_be_int(30.3)
    assert not could_be_int("30.2")
    assert not could_be_int("nope")

    assert could_be_float(30)
    assert could_be_float(30.3)
    assert could_be_float("30.2")
    assert not could_be_float("nope")

    assert could_be_int("30.2") is False
    assert could_be_float("30.2") is True

    st = RoutingForest(
        [
            CondNode(
                cond=could_be_int,
                then=RoutingForest(
                    [
                        CondNode(
                            cond=lambda x: int(x) >= 10,
                            then=FinalNode("More than a digit"),
                        ),
                        CondNode(
                            cond=lambda x: (int(x) % 2) == 1,
                            then=FinalNode("That's odd!"),
                        ),
                    ]
                ),
            ),
            CondNode(cond=could_be_float, then=FinalNode("could be seen as a float")),
        ]
    )
    assert list(st("nothing I can do with that")) == []
    assert list(st(8)) == ["could be seen as a float"]
    assert list(st(9)) == ["That's odd!", "could be seen as a float"]
    assert list(st(10)) == ["More than a digit", "could be seen as a float"]
    assert list(st(11)) == [
        "More than a digit",
        "That's odd!",
        "could be seen as a float",
    ]

    rf = RoutingForest(
        [
            SwitchCaseNode(
                switch=lambda x: x % 5,
                cases={0: FinalNode("zero_mod_5"), 1: FinalNode("one_mod_5")},
                default=FinalNode("default_mod_5"),
            ),
            SwitchCaseNode(
                switch=lambda x: x % 2,
                cases={0: FinalNode("even"), 1: FinalNode("odd")},
                default=FinalNode("that is not an int"),
            ),
        ]
    )

    assert list(rf(5)) == ["zero_mod_5", "odd"]
    assert list(rf(6)) == ["one_mod_5", "even"]
    assert list(rf(7)) == ["default_mod_5", "odd"]
    assert list(rf(8)) == ["default_mod_5", "even"]
    assert list(rf(10)) == ["zero_mod_5", "even"]

    # testing default mini-language #####################################################

    st2 = RoutingNode.from_object(
        [  # RoutingForest
            (  # CondNode
                could_be_int,
                [  # RoutingForest
                    (lambda x: int(x) >= 10, FinalNode("More than a digit")),
                    (lambda x: (int(x) % 2) == 1, FinalNode("That's odd!")),
                ],
            ),
            (could_be_float, FinalNode("could be seen as a float")),
        ]
    )

    assert list(st2("nothing I can do with that")) == []
    assert list(st2(8)) == ["could be seen as a float"]
    assert list(st2(9)) == ["That's odd!", "could be seen as a float"]
    assert list(st2(10)) == ["More than a digit", "could be seen as a float"]
    assert list(st2(11)) == [
        "More than a digit",
        "That's odd!",
        "could be seen as a float",
    ]

    rf2 = RoutingForest(
        [
            (
                lambda x: x % 5,
                LiteralVal({0: FinalNode("zero_mod_5"), 1: FinalNode("one_mod_5")}),
                FinalNode("default_mod_5"),
            ),
            (
                lambda x: x % 2,
                LiteralVal({0: FinalNode("even"), 1: FinalNode("odd")}),
                FinalNode("that is not an int"),
            ),
        ]
    )

    assert list(rf2(5)) == ["zero_mod_5", "odd"]
    assert list(rf2(6)) == ["one_mod_5", "even"]
    assert list(rf2(7)) == ["default_mod_5", "odd"]
    assert list(rf2(8)) == ["default_mod_5", "even"]
    assert list(rf2(10)) == ["zero_mod_5", "even"]
```

## scrap/__init__.py

```python
"""Just a place to jot down ideas"""
```

## scrap/scrap.py

```python
"""Scrap"""

from functools import wraps
from typing import Tuple
from collections.abc import Iterable, Callable
from i2.wrapper import Wrap
from i2 import call_forgivingly, Sig

# NOTE: Wrapx is now in i2.wrapper and test in i2.tests.wrapper_test


def default_caller(
    func: Callable, func_args: tuple, func_kwargs: dict, **caller_params
):
    """Function is called normally. caller_kwargs ignored"""
    return func(*func_args, **func_kwargs)


def _extract_params(ingress_output) -> tuple[tuple, dict, dict, dict]:
    """Pads tuple with empty dicts"""
    if isinstance(ingress_output, tuple) and len(ingress_output) == 2:
        # This is the "normal" ingress protocol, so transform to the extended
        return ingress_output[0], ingress_output[1], {}, {}
    elif isinstance(ingress_output, dict):
        # This is the extended ingress protocol
        return (
            ingress_output.get("inner_args", ()),
            ingress_output.get("inner_kwargs", {}),
            ingress_output.get("caller_kwargs", {}),
            ingress_output.get("egress_kwargs", {}),
        )
        # TODO: Consider making a ingress_output object to avoid key-typo errors
        # TODO: Should we assert those were the only keys to mitigate key-typo errors?
    else:
        raise ValueError(
            f"ingress_output should be a 2-tuple or a dict. Was {ingress_output}"
        )


# TODO: Create a few higher-level constructors
# TODO: Add static analysis to ascertain protocols
class Wrapx1(Wrap):
    """An extended ``Wrap`` object that enables control over ``caller`` & ``egress``.

    To get the extended functionality, the ``ingress`` function must return a dict with
    keys ``inner_args, inner_kwargs, caller_kwargs`` and/or  ``egress_kwargs``.
    If not specified, the ``inner_args`` value will default to the empty tuple ``()``,
    and all others to the empty dictionary ``{}``.

    The additional control arguments be keyword-only in ``ingress`` as well as in the
    ``caller`` and/or ``egress`` they control.

    >>> from inspect import signature
    >>> def func(x, y):
    ...     return x + y

    Let's wrap the function endowing the wrapped function with an extra argument, ``z``,
    which can be used to multiply the output.

    >>> def ingress(x, y, *, z):
    ...     return dict(inner_args=(x, y), egress_kwargs=dict(z=z))
    >>> def egress(v, *, z):
    ...     return v * z
    ...
    >>> wrapped_func = Wrapx1(func, ingress=ingress, egress=egress)
    >>>
    >>> func(1, 2)
    3
    >>> str(signature(wrapped_func))
    '(x, y, *, z)'
    >>> assert wrapped_func(1, 2, z=3) == 9 == func(1, 2) * 3

    Here's a bit more realistic application. Say we want to add two arguments,
    ``s`` (a mapping) and ``k`` (a key) that will have the effect of writing outputs
    of the function in ``s[k]``.

    >>> def _saving_ingress(x, y, *, k, s):
    ...     return dict(inner_args=(x, y), egress_kwargs=dict(k=k, s=s))
    >>> def _saving_egress(v, *, k, s):
    ...     s[k] = v
    ...     return v
    >>> save_on_output = Wrapx1(func, ingress=_saving_ingress, egress=_saving_egress)
    >>> str(signature(save_on_output))
    '(x, y, *, k, s)'
    >>>
    >>> store = dict()
    >>> assert save_on_output(1, 2, k='save_here', s=store) == 3 == func(1, 2)
    >>> store
    {'save_here': 3}

    An graph exhibiting how it works:

    .. image:: https://user-images.githubusercontent.com/1906276/159377430-e67f4fb7-22f2-44cd-97f8-6ddbbe53f66d.png


    """

    def __init__(
        self, func, ingress=None, egress=None, *, name=None, caller=default_caller
    ):
        super().__init__(func, ingress=ingress, egress=egress, name=name)
        self.caller = caller

    def __call__(self, *outer_args, **outer_kwargs):
        func_args, func_kwargs, caller_params, egress_params = _extract_params(
            self.ingress(*outer_args, **outer_kwargs)
        )
        # call the function
        func_output = self.caller(self.func, func_args, func_kwargs, **caller_params)
        # process output with egress
        return self.egress(func_output, **egress_params)


def _pad_with_empty_dicts(tup, target_length=4):
    """Pads tuple with empty dicts"""
    return tup + tuple([{}] * max(0, target_length - len(tup)))


# TODO: Add static analysis to ascertain dynamic _pad_with_empty_dicts won't lead to
#  trouble
class Wrapx2(Wrap):
    """
    >>> from inspect import signature
    >>> def func(x, y):
    ...     return x + y
    >>> def ingress(x, y, *, z):
    ...     return (x, y), {}, dict(z=z), {}
    >>> def egress(v, *, z):
    ...     return v * z
    >>> wrapped_func = Wrapx2(func, ingress=ingress, egress=egress)
    >>>
    >>> assert func(1, 2) == 3
    >>> assert str(signature(wrapped_func)) == '(x, y, *, z)'
    >>> assert wrapped_func(1, 2, z=3) == 9 == func(1, 2) * 3
    """

    def __init__(
        self, func, ingress=None, egress=None, *, name=None, caller=default_caller
    ):
        super().__init__(func, ingress=ingress, egress=egress, name=name)
        self.caller = caller

    def __call__(self, *outer_args, **outer_kwargs):
        func_args, func_kwargs, egress_params, caller_params = _pad_with_empty_dicts(
            self.ingress(*outer_args, **outer_kwargs)
        )
        # call the function
        func_output = self.caller(self.func, func_args, func_kwargs, **caller_params)
        # process output with egress
        return self.egress(func_output, **egress_params)


# from i2.deco import double_up_as_factory
#
# # TODO: Other design options? Example making an Egress class that has map_arguments_from_variadics meth
# # TODO: Make "copy" of function before adding map_arguments_from_variadics attr, or let user do it
# #  if it matters? (I vote let user)
# # TODO: Add ability to not pop extracted
# # TODO: Postelize map_arguments_from_variadics further?
# @double_up_as_factory
# def add_extract_kwargs(func=None, *, map_arguments_from_variadics):
#     if isinstance(map_arguments_from_variadics, Iterable):
#         keys_to_extract = tuple(map_arguments_from_variadics)
#         map_arguments_from_variadics = partial(_extract_kwargs, keys_to_extract=keys_to_extract)
#     if callable(map_arguments_from_variadics) and Sig(map_arguments_from_variadics).n_required <= 1:
#         setattr(func, 'map_arguments_from_variadics', map_arguments_from_variadics)
#     else:
#         setattr(func, 'map_arguments_from_variadics', _return_empty_dict)
#
#     return func
#
#
# def _return_empty_dict(kwargs):
#     """Used when we want to have an 'empty' map_arguments_from_variadics on an egress or caller"""
#     return dict()
#
#
# def _extract_kwargs(kwargs, keys_to_extract):
#     return {k: kwargs.pop(k) for k in keys_to_extract}
#
#
# @add_extract_kwargs(map_arguments_from_variadics=_return_empty_dict)
# def default_caller(
#     func: Callable, func_args: tuple, func_kwargs: dict, caller_kwargs
# ):
#     """Function is called normally. caller_kwargs ignored"""
#     return func(*func_args, **func_kwargs)
#
#
# # TODO: Better have an egress that always has a (potentially empty) map_arguments_from_variadics or
# #  check for existence at runtime?
# class Wrapxx(Wrap):
#     def __init__(
#             self, func, ingress=None, egress=None, *, name=None, caller=default_caller
#     ):
#         super().__init__(func, ingress=ingress, egress=egress, name=name)
#         self.caller = caller
#         # current_sig = Sig(self)
#         # # extended_sig = Sig(self) +
#         # self.__signature__ = Sig(
#         #     extended_sig, return_annotation=current_sig.return_annotation
#         # )
#
#     def __call__(self, *user_args, **user_kwargs):
#         # extract the kwargs the egress needs from user input
#         egress_kwargs = self.egress.map_arguments_from_variadics(user_kwargs)
#         # extract the kwargs the caller needs from user input
#         caller_kwargs = self.caller.map_arguments_from_variadics(user_kwargs)
#         # prepare function inputs
#         func_args, func_kwargs = self.ingress(*user_args, **user_kwargs)
#         # call the function
#         func_output = self.caller(self.func, func_args, func_kwargs, caller_kwargs)
#         # process output with egress
#         return self.egress(func_output, **egress_kwargs)
#
```

## scrap/scrap_ch_variadics.py

```python
from i2 import Sig
from i2.signatures import ch_variadics_to_non_variadic_kind


def foo(a, *args, bar, **kwargs):
    return f"{a=}, {args=}, {bar=}, {kwargs=}"


assert str(Sig(foo)) == "(a, *args, bar, **kwargs)"
wfoo = ch_variadics_to_non_variadic_kind(foo)
str(Sig(wfoo))
#'(a, args=(), *, bar, kwargs={})'

# And now to do this:

foo(1, 2, 3, bar=4, hello="world")
# "a=1, args=(2, 3), bar=4, kwargs={'hello': 'world'}"

# We can do it like this instead:

wfoo(1, (2, 3), bar=4, kwargs=dict(hello="world"))
# "a=1, args=(2, 3), bar=4, kwargs={'hello': 'world'}"

# Note, the outputs are the same. It's just the way we call our function that has
# changed.

assert wfoo(1, (2, 3), bar=4, kwargs=dict(hello="world")) == foo(
    1, 2, 3, bar=4, hello="world"
)
assert wfoo(1, (2, 3), bar=4) == foo(1, 2, 3, bar=4)
assert wfoo(1, (), bar=4) == foo(1, bar=4)

# Note that if there is not variadic positional arguments, the variadic keyword
# will still be a keyword-only kind.


@ch_variadics_to_non_variadic_kind
def func(a, bar=None, **kwargs):
    return f"{a=}, {bar=}, {kwargs=}"


str(Sig(func))
#'(a, bar=None, *, kwargs={})'
assert (
    func(1, bar=4, kwargs=dict(hello="world"))
    == "a=1, bar=4, kwargs={'hello': 'world'}"
)

# If the function has neither variadic kinds, it will remain untouched.


def func(a, /, b, *, c=3):
    return a + b + c


ch_variadics_to_non_variadic_kind(func) == func
# True

# If you only want the variadic positional to be handled, but leave leave any
# VARIADIC_KEYWORD kinds (**kwargs) alone, you can do so by setting
# `ch_variadic_keyword_to_keyword=False`.
# If you'll need to use `ch_variadics_to_non_variadic_kind` in such a way
# repeatedly, we suggest you use `functools.partial` to not have to specify this
# configuration repeatedly.

from functools import partial

tuple_the_args = partial(
    ch_variadics_to_non_variadic_kind, ch_variadic_keyword_to_keyword=False
)


@tuple_the_args
def foo(a, *args, bar=None, **kwargs):
    return f"{a=}, {args=}, {bar=}, {kwargs=}"


Sig(foo)
# <Sig (a, args=(), *, bar=None, **kwargs)>
foo(1, (2, 3), bar=4, hello="world")
# "a=1, args=(2, 3), bar=4, kwargs={'hello': 'world'}"
```

## scrap/scrap_kwargs.py

```python
from i2 import Sig
from i2.signatures import _remove_variadics_from_sig, ch_variadics_to_non_variadic_kind
from i2.wrapper import InnerMapIngress
from i2.wrapper import wrap
from inspect import Parameter, Signature


# def bar(w, /, x: float, y=1, *args, z: int = 1, **rest):
#     return ((w + x) * y) ** z


# bar_sig = Sig(bar)


def foo(po, *vp, ko, **vk):
    return f"{po=}, {vp=}, {ko=}, {vk=}"


new_sig = _remove_variadics_from_sig(Sig(foo))
assert str(new_sig) == "(po, vp=(), *, ko, vk={})"

ingress = InnerMapIngress.from_signature(foo, outer_sig=new_sig)

if __name__ == "__main__":
    new_foo = wrap(foo, ingress=ingress)
    # ingress_args = (1, (2, 3))
    # ingress_kwargs = {"ko": 4, "vk": {"hello": "world"}}
    # print(ingress(*ingress_args, **ingress_kwargs))

    assert (
        foo(1, 2, 3, ko=4, hello="world")
        == "po=1, vp=(2, 3), ko=4, vk={'hello': 'world'}"
    )
    new_foo = wrap(foo, ingress=ingress)
    assert str(Sig(new_foo)) == "(po, vp=(), *, ko, vk={})"
    # assert (
    #     new_foo(1, (2, 3), ko=4, vk=dict(hello="world"))
    #     == "po=1, vp=(2, 3), ko=4, vk={'hello': 'world'}"
    # )
    # args, kwargs = foo_sig.mk_args_and_kwargs(dict(w=4, x=3, y=2, z=1, t=12))

    # print(f"args:{args}, kwargs: {kwargs}")
    # print(new_foo(1, (2, 3), ko=4, vk=dict(hello="world")))
```

## scrap/signature_bops.py

```python
"""Signature binary operators

See design issue: https://github.com/i2mint/i2/issues/50
"""

from functools import partial
from collections import defaultdict
from itertools import tee
from typing import TypeVar, Optional, Union
from collections.abc import Callable, Iterable

B = TypeVar("B")
B.__doc__ = (
    "'Base' operand that we are able (example, builtin object) to operate on, as is"
)
A = TypeVar("A")
A.__doc__ = "'Abstract' type we want to operate on through a key-function"
KeyFunction = Callable[[A], B]
# KeyFunction.__doc__ = "Function that transforms an A to a B"

R = TypeVar("R")
R.__doc__ = "The return type of a binary operator"

BinaryOperator = Callable[[B, B], R]
AbstractBinaryOperator = Callable[[A, A], R]


def _keyed_comparator(
    binary_operator: BinaryOperator,
    key: KeyFunction,
    x: A,
    y: A,
) -> R:
    """Apply a binary operator to two operands,
    after transforming them through a key function"""
    return binary_operator(key(x), key(y))


def keyed_comparator(
    binary_operator: BinaryOperator,
    key: KeyFunction,
):
    """Create a key-function enabled binary operator"""
    return partial(_keyed_comparator, binary_operator, key)


# For back-compatibility:
_key_function_enabled_operator = _keyed_comparator
_key_function_factory = keyed_comparator


def _key_mappings(x: Iterable, key: KeyFunction | None = None):
    """Create a mapping from key to value, and a mapping from value to key"""
    if key is None:
        key = lambda x: x

    key_for_item = dict
    item_for_key = defaultdict(list)

    for item in x:
        keyed_item = key(item)
        key_for_item[item] = keyed_item
        item_for_key[keyed_item].append(item)

    return key_for_item, item_for_key

    # def _key_item_
    #     for xi in x:
    #         yield xi


def simple_match(x: Iterable, y: Iterable, key=None):
    key_for_x, x_for_key = _key_mappings(x, key)
    key_for_y, y_for_key = _key_mappings(y, key)
    return x_for_key.keys() & y_for_key.keys()


def _sig_func(sig1, sig2, params_match, score_param_pair, score_aggreg):
    params = params_match(sig1, sig2)
    return score_aggreg(score_param_pair(params))


# Moved to i2.signatures (keep import below for back comp
from i2.signatures import param_binary_func

# from pydantic import validate_arguments, ValidationError

# from graphviz import Digraph
#
#
# def get_edge_list(graph: Digraph) -> list:
#     """Gets a list of edges (as node id pairs) from a digraph."""
#     return [
#         (node, child)
#         for node in graph.body
#         if node.startswith('  ')
#         for child in graph.body[graph.body.index(node) + 1 :]
#         if child.startswith('  ')
#     ]
# import networkx as nx
# nx.nx_agraph.read_dot


# B = TypeVar('B')
# B.__doc__ = (
#     "A 'base' type that we are able (example, builtin object) to operate on, as is"
# )
# A = TypeVar('A')
# A.__doc__ = "The 'abstract' type we want to operate on through a key-function"
# KeyFunction = Callable[[A], B]
# KeyFunction.__doc__ = 'Function that transforms an A to a B'
#
# AorB = TypeVar('AorB', A, B)
# # AorB = Union[A, B]
# AorB.__doc__ = 'An A (abstract) or a B (base)'
# AB = TypeVar('AB', bound=AorB)
# AorB.__doc__ = 'A generic AorB'
# R = TypeVar('R')
# R.__doc__ = 'The return type of a binary operator'
#
# BaseBinaryOperator = Callable[[B, B], R]
# AbstractBinaryOperator = Callable[[A, A], R]
# # BinaryOperator = Union[BaseBinaryOperator, AbstractBinaryOperator]  # equivalent?
# BinaryOperator = Callable[[AB, AB], R]
# KeyEnabledBinaryOperator = Callable[[AB, AB, KeyFunction], R]
#

# # key is None => x is B => y is B
# # key is not None => x is A => y is A
# def key_function_enabled_operator(
#     binary_operator: Union[BaseBinaryOperator, AbstractBinaryOperator],
#     x: AB,
#     y: AB,  # has to be the same as x. If x is A, so should y, if x is B so should y
#     key: Optional[KeyFunction] = None,
# ) -> R:
#     if key is None:
#         return binary_operator(x, y)
#     else:
#         return binary_operator(key(x), key(y))
#
#
# def key_function_factory(binary_operator: BinaryOperator) -> KeyEnabledBinaryOperator:
#     return partial(key_function_enabled_operator, binary_operator)
#
```

## scrap/simple_pymint.py

```python
"""Simple explorations of doc mints"""

import inspect
from collections import defaultdict


class NoDefault:
    def __repr__(self):
        return "no_default"


no_default = NoDefault()

NO_NAME = "_no_name"

valid_types = [str, dict, list, float, int, bool]

types_map = {
    str: "string",
    dict: "object",
    list: "array",
    float: "float",
    int: "int",
    bool: "boolean",
    "{}": "{}",
    None: "{}",
}


def name_of_obj(o):
    if hasattr(o, "__name__"):
        return o.__name__
    elif hasattr(o, "__class__"):
        return name_of_obj(o.__class__)
    else:
        return NO_NAME


def parse_mint_doc(doc: str) -> dict:
    if doc is None:
        doc = ""
    DESCRIPTION = 0
    PARAMS = 1
    RETURN = 2
    split_doc = doc.split("\n")
    summary = ""
    description_lines = []
    inputs = defaultdict(dict)
    return_value = {}
    tags = []
    reading = DESCRIPTION
    cur_item_name = ""
    for line in split_doc:
        line = line.strip()
        if line.startswith(":param"):
            reading = PARAMS
            split_line = line.split(":")
            param_def = split_line[1]
            split_param_def = param_def.split(" ")
            param_name = ""
            param_type = None
            if len(split_param_def) >= 3:
                param_type = split_param_def[1]
                param_name = split_param_def[2]
                try:
                    assert len(param_type) <= 5
                    param_type = eval(param_type)
                    assert param_type in valid_types
                except Exception:
                    param_type = "{}"
            elif len(split_param_def) == 2:
                param_name = split_param_def[1]
            if param_name:
                cur_item_name = param_name
                if param_type is not None:
                    inputs[param_name] = {"type": param_type}
                if param_name in inputs:
                    inputs[param_name]["description"] = (
                        ":".join(split_line[2:]) if len(split_line) > 2 else ""
                    )
        elif line.startswith(":return"):
            reading = RETURN
            split_line = line.split(":")
            return_type = split_line[1]
            try:
                assert len(return_type) <= 5
                return_type = eval(return_type)
                assert return_type in valid_types
            except Exception:
                return_type = "{}"
            return_value = {"type": return_type}
            return_value["description"] = (
                ":".join(split_line[2:]) if len(split_line) > 2 else ""
            )
        elif line.startswith(":tags"):
            split_line = line.split(":")
            tags = split_line[1].replace(" ", "").split(",")
        else:
            if reading == DESCRIPTION:
                if not summary:
                    summary = line
                else:
                    description_lines.append(line)
            elif reading == PARAMS:
                if "description" not in inputs[cur_item_name]:
                    inputs[cur_item_name]["description"] = ""
                inputs[cur_item_name]["description"] += " " + line
            elif reading == RETURN:
                return_value["description"] += " " + line
    return {
        "description": " ".join(description_lines),
        "inputs": dict(inputs),
        "return": return_value,
        "summary": summary,
        "tags": tags,
    }


# TODO: Expand so that user can specify what to include in the mint
def mint_of_callable(f):
    """
    Get meta-data about a callable.
    :param f: A callable (function, method, ...)
    :return: A dict containing information about the interface of f, that is, name, module, doc, and
    input and output information.

    >>> def test_callable(arg1, arg2: int) -> str:
    ...     __doc__ = "A testable callable\\n:param str arg1: A string\\n"
    ...     print('calling!')
    ...     print(arg1)
    ...     print(arg2)
    ...     return 'returned'
    ...
    >>> mint = mint_of_callable(test_callable)
    >>> list(mint.keys())
    ['name', 'module', 'doc', 'description', 'summary', 'tags', 'input', 'output']
    >>> mint['name']
    'test_callable'
    >>> assert mint['module'] == test_callable.__module__, "mint['module'] == test_callable.__module__"
    >>> mint['input']
    {'arg1': {}, 'arg2': {'annot': <class 'int'>}}
    >>> mint['output']
    {'annot': <class 'str'>}
    >>> assert mint.pop('module').endswith('simple_pymint')  # popping off because module is relative so not consistent
    >>> from pprint import pprint
    >>> pprint(mint)
    {'description': '',
     'doc': None,
     'input': {'arg1': {}, 'arg2': {'annot': <class 'int'>}},
     'name': 'test_callable',
     'output': {'annot': <class 'str'>},
     'summary': '',
     'tags': []}
    """
    raw_doc = inspect.getdoc(f)
    parsed_doc = parse_mint_doc(raw_doc)
    doc_inputs = parsed_doc["inputs"]
    doc_return = parsed_doc["return"]
    mint = {
        "name": name_of_obj(f),  # TODO: Better NO_NAME or just not the name field?
        "module": f.__module__,
        "doc": raw_doc,
        "description": parsed_doc["description"] or "",
        "summary": parsed_doc["summary"],
        "tags": parsed_doc["tags"],
    }

    argspec = inspect.getfullargspec(f)
    annotations = argspec.annotations
    input_specs = {}
    args = argspec.args or []
    defaults = argspec.defaults or []
    for arg_name, dflt in zip(
        args, [no_default] * (len(args) - len(defaults)) + list(defaults)
    ):
        input_specs[arg_name] = {}
        if dflt is not no_default:
            input_specs[arg_name]["default"] = dflt
        if arg_name in doc_inputs:
            doc_input_arg = doc_inputs[arg_name]
            input_specs[arg_name]["doc_type"] = doc_input_arg["type"]
            input_specs[arg_name]["description"] = doc_input_arg["description"]
        if arg_name in annotations:
            input_specs[arg_name]["annot"] = annotations[arg_name]
        # if input_specs[arg_name].get('type', None):  # NOTE: Note the pymint concern to convert to string
        #     input_specs[arg_name]['type'] = types_map.get(input_specs[arg_name]['type'], '{}')

    mint["input"] = input_specs

    mint["output"] = {}
    if doc_return:
        mint["output"] = doc_return
    if "return" in annotations:
        mint["output"]["annot"] = annotations["return"]

    return mint


""" TODO:
    - indicate required/optional or nullable input arguments
    - indicate input argument constraints (min, max, enum, etc)
    - allow listing properties of complex objects as input arguments
        (eg argument a1 type is a dict with expected properties p1, p2, p3)
"""

if __name__ == "__main__":
    from pprint import pprint

    def _test_callable(arg1, arg2: int) -> str:
        """
        A testable callable.
        :param str arg1: A string
        :params float arg2: A number
        """
        print("calling!")
        print(arg1)
        print(arg2)
        return "returned"

    minted = mint_of_callable(_test_callable)
    print("mint of _test_callable")
    pprint(minted)

    # and a bit of naughty...

    print("mint of pprint")
    pprint(mint_of_callable(pprint))

    print("mint of mint_of_callable")
    pprint(mint_of_callable(mint_of_callable))
```

## scrap/switch_case_tree.py

```python
"""
Experimental Mapping the maps keys using a switch_case structure.

Development paused while waiting for 3.10's pattern matching that may subsume this.


>>> from i2.scrap.switch_case_tree import *
>>> from collections import Counter
>>>
>>> special_featurizer = {
...     'len': len,
...     'cols': lambda df: df.columns,
...     'sum': lambda df: df.sum().sum(),
... }
>>> special_comparison = {
...     'alleq': lambda x, y: all(x == y),
...     'isin': lambda x, y: x in y,
...     'eq': operator.eq,
... }
>>> featurizer = ChainMap(
...     special_featurizer,
...     special_comparison,
...     {'some_local_func': lambda x: list(map(str, x))}
... )
>>>
>>>
>>> comparison = ChainMap(
...     special_comparison, AttrMap(operator, is_valid_val=is_valid_comparision),
... )
>>>
>>> assert comparison['contains'] == operator.contains
>>>

Let's have a look at the "featurizers" (where we purposely injected 3 functions that
were in fact not featurizers!

>>> sorted(featurizer)  # doctest: +NORMALIZE_WHITESPACE
['alleq', 'cols', 'eq', 'isin', 'len', 'some_local_func', 'sum']
>>> Counter(map(is_valid_featurizer, featurizer.values()))
Counter({True: 4, False: 3})
>>> Counter(map(is_valid_comparision, featurizer.values()))
Counter({False: 4, True: 3})

Let's have a look at the comparison functions...

# >>> Counter(map(is_valid_comparision, comparison.values()))
# Counter({True: 81})

# >>> Counter(map(is_valid_featurizer, comparison.values()))
# Counter({False: 80, True: 1})

What's that single comparison function that's also a featurizer?

>>> next(filter(is_valid_featurizer, comparison.values())).__name__
'length_hint'


>>> from contextlib import suppress
>>>
>>> with suppress(ModuleNotFoundError, ImportError):
...     import pandas as pd
...     from collections import namedtuple
...     Condition = namedtuple('Condition', ['feat', 'comp'])
...     condition = {
...         feat + '_' + comp: Condition(featurizer[feat], comparison[comp])
...         for feat, comp in [('len', 'lt'), ('cols', 'isin'), ('cols', 'contains'),]
...     }
...     assert all(
...         is_valid_feat_and_comp(feat, comp) for feat, comp in condition.values()
...     )
...
...     df = pd.DataFrame({'a': [1, 2, 3], 'b': [10, 20, 30]})
...     filt = mk_filt(df, *condition['len_lt'])
...     result = list(filter(filt, [2, 3, 4, 5]))
...     assert result == [4, 5]

"""

from collections import ChainMap
from collections.abc import Mapping
import operator
import inspect


class AsIsMap:
    def __init__(self, is_valid_key=None):
        if is_valid_key is None:

            def is_valid_key(x):
                return True

        self._is_valid_key = is_valid_key

    def _validate_key(self, k):
        if not self._is_valid_key(k):
            raise KeyError(f"{k} wasn't a valid key")

    def __getitem__(self, k):
        self._validate_key(k)
        return k


class AttrMap(Mapping):
    def __init__(self, obj, is_valid_val=None):
        self._obj = obj
        if is_valid_val is None:

            def is_valid_val(x):
                return True

        self._is_valid_val = is_valid_val

    @classmethod
    def _validate_key(cls, k):
        if not isinstance(k, str):
            raise KeyError("key should be a string")

    def _validate_val(self, v):
        if not self._is_valid_val(v):
            raise ValueError("key was valid and value found, but value wasn't valid")

    def _getitem(self, k):
        return getattr(self._obj, k)

    def _val_of_key_is_valid(self, k):
        return self._is_valid_val(self._getitem(k))

    def __getitem__(self, k):
        self._validate_key(k)
        v = self._getitem(k)
        self._validate_val(v)
        return v

    def __contains__(self, k):
        self._validate_key(k)
        return hasattr(self._obj, k) and self._val_of_key_is_valid(k)

    def __iter__(self):
        return filter(self._val_of_key_is_valid, dir(self._obj))

    def __len__(self):
        c = 0
        for _ in self.__iter__():
            c += 1
        return c


def n_args_and_n_args_with_no_defaults(func):
    try:
        args = inspect.signature(func).parameters.values()
    except ValueError:  # happens because some builtins don't have signatures (!?!?)
        return 0, 0
    return len(args), len(list(filter(lambda x: x.default == x.empty, args)))


def is_valid_featurizer(func):
    if not callable(func):
        return False
    n_args, n_no_dflt_args = n_args_and_n_args_with_no_defaults(func)
    return (n_args >= 1) and (n_no_dflt_args <= 1)


def is_valid_comparision(func):
    if not callable(func):
        return False
    n_args, n_no_dflt_args = n_args_and_n_args_with_no_defaults(func)
    return (n_args >= 2) and (n_no_dflt_args <= 2)


def is_valid_feat_and_comp(feat, comp):
    return is_valid_featurizer(feat) and is_valid_comparision(comp)


def values_are_valid_feat_and_comp(d):
    return all(is_valid_feat_and_comp(feat, comp) for feat, comp in d.values())


def mk_filt(obj, featurizer, comparison):
    feature = featurizer(obj)

    def filt(x):
        return comparison(feature, x)

    return filt


def test_switch_case_tree():
    from collections import Counter

    special_featurizer = {
        "len": len,
        "cols": lambda df: df.columns,
        "sum": lambda df: df.sum().sum(),
    }

    some_local_func = lambda x: list(map(str, x))

    featurizer = ChainMap(
        special_featurizer,
        {
            k: v
            for k, v in locals().items()
            if is_valid_featurizer(v) and getattr(v, "__module__", "").startswith("i2.")
        },
    )

    special_comparison = {
        "alleq": lambda x, y: all(x == y),
        "isin": lambda x, y: x in y,
        "eq": operator.eq,
    }

    comparison = ChainMap(
        special_comparison,
        AttrMap(operator, is_valid_val=is_valid_comparision),
    )

    assert comparison["contains"] == operator.contains

    assert Counter(map(is_valid_featurizer, featurizer.values())) == Counter({True: 3})

    assert Counter(map(is_valid_comparision, featurizer.values())) == Counter(
        {False: 3}
    )
    assert Counter(map(is_valid_featurizer, comparison.values())) == Counter(
        {False: 80, True: 1}
    )
    assert Counter(map(is_valid_comparision, comparison.values())) == Counter(
        {True: 81}
    )

    featurizer_kvs = {k: v.__name__ for k, v in featurizer.items()}
    assert featurizer_kvs == {"len": "len", "cols": "<lambda>", "sum": "<lambda>"}

    from contextlib import suppress

    with suppress(ModuleNotFoundError, ImportError):
        import pandas as pd
        from collections import namedtuple

        Condition = namedtuple("Condition", ["feat", "comp"])
        condition = {
            feat + "_" + comp: Condition(featurizer[feat], comparison[comp])
            for feat, comp in [
                ("len", "lt"),
                ("cols", "isin"),
                ("cols", "contains"),
            ]
        }
        assert all(
            is_valid_feat_and_comp(feat, comp) for feat, comp in condition.values()
        )

        df = pd.DataFrame({"a": [1, 2, 3], "b": [10, 20, 30]})
        filt = mk_filt(df, *condition["len_lt"])
        assert list(filter(filt, [2, 3, 4, 5])) == [4, 5]


##########################################################################################################
# from anytree import Node, AnyNode, RenderTree, ContStyle, NodeMixin
#
# NodeMixin.separator = '.'
#
#
# def get_attr_and_vals(obj):
#     for k in dir(obj):
#         v = getattr(obj, k)
#         yield k, v
#
#
# def attr_and_vals_dict(obj, filt=None):
#     return {k: v for k, v in filter(filt, get_attr_and_vals(obj))}
#
#
# # Filters
# def value_not_callable(kv):
#     return not callable(kv[1])
#
#
# class AttrTreeImporter:
#     _base_types = (int, float, str, bytes, bool, list)
#
#     def __init__(self, val_types=()):
#         self.val_types = tuple(set(val_types).union(self._base_types))
#
#     def dict_of_obj(self, obj):
#         d = dict()
#         for k, v in obj.__dict__.items():
#             if isinstance(v, self.val_types):
#                 d[k] = v
#             else:
#                 d[k] = self.dict_of_obj(v)
#         return d
#
#     def tree_of_dict(self, d, parent=None, root_name='root'):
#         if parent is None:
#             parent = Node(root_name)
#         for k, v in d.items():
#             if isinstance(v, dict):
#                 n = Node(k, parent=parent)
#                 self.tree_of_dict(v, parent=n)
#             else:
#                 n = Node(k, parent=parent, val=v)
#         return parent
#
#     def tree_of_obj(self, obj, root_name=None):
#         if root_name is None:
#             if hasattr(obj, '__name__'):
#                 root_name = obj.__name__
#             elif hasattr(obj, '__class__'):
#                 root_name = obj.__class__.__name__
#             else:
#                 root_name = 'root'
#         return self.tree_of_dict(self.dict_of_obj(obj), root_name=root_name)
```

## signatures.py

```python
"""Signature calculus: Tools to make it easier to work with function's signatures.

How to:

    - get names, kinds, defaults, annotations

    - make signatures flexibly

    - merge two or more signatures

    -
    - give a function a specific signature (with a choice of validations)

    - get an equivalent function with a different order of arguments

    - get an equivalent function with a subset of arguments (like partial)

    - get an equivalent function but with variadic *args and/or **kwargs replaced with
    non-variadic args (tuple) and kwargs (dict)

    - make an f(a) function in to a f(a, b=None) function with b ignored


Get names, kinds, defaults, annotations:

>>> def func(z, a: float=1.0, /, b=2, *, c: int=3):
...     pass
>>> sig = Sig(func)
>>> sig.names
['z', 'a', 'b', 'c']
>>> from inspect import Parameter
>>> assert sig.kinds == {
...     'z': Parameter.POSITIONAL_ONLY,
...     'a': Parameter.POSITIONAL_ONLY,
...     'b': Parameter.POSITIONAL_OR_KEYWORD,
...     'c': Parameter.KEYWORD_ONLY
... }
>>> # Note z is not in there (only defaulted params are included)
>>> sig.defaults
{'a': 1.0, 'b': 2, 'c': 3}
>>> sig.annotations
{'a': <class 'float'>, 'c': <class 'int'>}

Make signatures flexibly:

>>> Sig(func)
<Sig (z, a: float = 1.0, /, b=2, *, c: int = 3)>
>>> Sig(['a', 'b'])
<Sig (a, b)>
>>> Sig('x y z')
<Sig (x, y, z)>

Merge signatures.

>>> def foo(x): pass
>>> def bar(y: int, *, z=2): pass  # note the * (keyword only) will be lost!
>>> Sig(foo) + ['a', 'b'] + Sig(bar)
<Sig (x, a, b, y: int, z=2)>

Give a function a signature.

>>> @Sig('a b c')
... def func(*args, **kwargs):
...     print(args, kwargs)
>>> Sig(func)
<Sig (a, b, c)>


**Notes to the reader**

Both in the code and in the docs, we'll use short hands for parameter (argument) kind.

    - PK = Parameter.POSITIONAL_OR_KEYWORD

    - VP = Parameter.VAR_POSITIONAL

    - VK = Parameter.VAR_KEYWORD

    - PO = Parameter.POSITIONAL_ONLY

    - KO = Parameter.KEYWORD_ONLY

"""

from inspect import Signature, Parameter, signature, unwrap
import re
import sys
from typing import (
    Union,
    Any,
    Dict,
    Tuple,
    TypeVar,
    Literal,
    Optional,
    get_args,
)
from collections.abc import Callable, Iterable, Iterator, Mapping as MappingType
from typing import KT, VT, T
from types import FunctionType
from collections import defaultdict
from operator import eq, attrgetter

from functools import (
    cached_property,
    update_wrapper,
    partial,
    partialmethod,
    WRAPPER_ASSIGNMENTS,
    wraps as _wraps,
    update_wrapper as _update_wrapper,
)


def deprecation_of(func, old_name):
    @wraps(func)
    def wrapper(*args, **kwargs):
        from warnings import warn

        warn(
            f"`{old_name}` is deprecated. Use `{func.__module__}.{func.__qualname__}` instead.",
            DeprecationWarning,
        )
        return func(*args, **kwargs)

    return wrapper


# monkey patching WRAPPER_ASSIGNMENTS to get "proper" wrapping (adding defaults and
# kwdefaults

wrapper_assignments = (*WRAPPER_ASSIGNMENTS, "__defaults__", "__kwdefaults__")

update_wrapper = partial(_update_wrapper, assigned=wrapper_assignments)
wraps = partial(_wraps, assigned=wrapper_assignments)

_empty = Parameter.empty
empty = _empty

ParamsType = Iterable[Parameter]
ParamsAble = Union[ParamsType, Signature, MappingType[str, Parameter], Callable, str]
SignatureAble = Union[Signature, ParamsAble]
HasParams = Union[Iterable[Parameter], MappingType[str, Parameter], Signature, Callable]

# short hands for Parameter kinds
PK = Parameter.POSITIONAL_OR_KEYWORD
VP, VK = Parameter.VAR_POSITIONAL, Parameter.VAR_KEYWORD
PO, KO = Parameter.POSITIONAL_ONLY, Parameter.KEYWORD_ONLY
var_param_kinds = frozenset({VP, VK})
var_param_types = var_param_kinds  # Deprecate: for back-compatibility. Delete in 2021
var_param_kind_dflts_items = tuple({VP: (), VK: {}}.items())

DFLT_DEFAULT_CONFLICT_METHOD = "strict"
SigMergeOptions = Literal[None, "strict", "take_first", "fill_defaults_and_annotations"]

param_attributes = {"name", "kind", "default", "annotation"}


class InvalidSignature(SyntaxError, ValueError):
    """Raise when a signature is not valid"""


class FuncCallNotMatchingSignature(TypeError):
    """Raise when the call signature is not valid"""


class IncompatibleSignatures(ValueError):
    from pprint import pformat

    """Raise when two signatures are not compatible.
    (see https://github.com/i2mint/i2/discussions/76 for more information on signature
    compatibility)"""

    def __init__(self, *args, sig1=None, sig2=None, **kwargs):
        args = list(args or ("",))
        sig_pairs = None
        if sig1 and sig2:
            sig_pairs = SigPair(sig1, sig2)
            # add the signature differences to the error message
            args[0] += (
                f"\n----- Signature differences (not all differences necessarily "
                f"matter in your context): ----- \n{sig_pairs.diff_str()}"
            )
        super().__init__(*args, **kwargs)
        self.sig_pairs = sig_pairs


# TODO: Couldn't make this work. See https://www.python.org/dev/peps/pep-0562/
# deprecated_names = {'assure_callable', 'assure_signature', 'assure_params'}
#
#
# def __getattr__(name):
#     print(name)
#     if name in deprecated_names:
#         from warnings import warn
#         warn(f"{name} is deprecated (see code for new name -- look for aliases)",
#         DeprecationWarning)
#     raise AttributeError(f"module {__name__} has no attribute {name}")


def validate_signature(func: Callable) -> Callable:
    """
    Validates the signature of a function.

    >>> @validate_signature
    ... def has_valid_signature(x=Sig.empty, y=2):
    ...     pass
    >>> # all good, no errors raised
    >>>
    >>> @validate_signature  # doctest: +IGNORE_EXCEPTION_DETAIL
    ... def does_no_have_valid_signature(x=2, y=Sig.empty):
    ...     pass
    Traceback (most recent call last):
    ...
    i2.signatures.InvalidSignature: Invalid signature for function <function does_no_have_valid_signature at 0x106a72a70>: non-default argument follows default a
    rgument

    """
    try:
        Sig(func)  # to get errors if the signature is not valid
    except Exception as e:
        raise InvalidSignature(f"Invalid signature for function {func}: {e}")
    return func  # if all goes well, return the original function


def is_signature_error(e: BaseException) -> bool:
    """Check if an exception is a signature error"""
    return isinstance(InvalidSignature) or (
        isinstance(e, ValueError) and "no signature found" in str(e)
    )


def _param_sort_key(param):
    return (param.kind, param.kind == KO or param.default is not empty)


def sort_params(params):
    """

    :param params: An iterable of `Parameter` instances
    :return: A list of these instances sorted so as to obey the ``kind`` and ``default``
        order rules of python signatures.

    Note 1: It doesn't mean that these params constitute a valid signature together,
    since it doesn't verify rules like unicity of names and variadic kinds.

    Note 2: Though you can use ``sorted`` on an iterable of ``i2.signatures.Param``
    instances, know that even for sorting the three parameters below,
    the ``sort_params`` function is more than twice as fast.

    >>> from inspect import Parameter
    >>> sort_params(
    ...     [Parameter('a', kind=Parameter.POSITIONAL_OR_KEYWORD, default=1),
    ...     Parameter('b', kind=Parameter.POSITIONAL_ONLY),
    ...     Parameter('c', kind=Parameter.POSITIONAL_OR_KEYWORD)]
    ... )
    [<Parameter "b">, <Parameter "c">, <Parameter "a=1">]
    """
    return sorted(params, key=_param_sort_key)


def _return_none(o: object) -> None:
    return None


# (approximately) duplicated from i2.util to keep signatures.py standalone
def name_of_obj(
    o: object,
    *,
    base_name_of_obj: Callable = attrgetter("__name__"),
    caught_exceptions: tuple = (AttributeError,),
    default_factory: Callable = _return_none,
) -> str | None:
    """
    Tries to find the (or "a") name for an object, even if `__name__` doesn't exist.

    >>> name_of_obj(map)
    'map'
    >>> name_of_obj([1, 2, 3])
    'list'
    >>> name_of_obj(print)
    'print'
    >>> name_of_obj(lambda x: x)
    '<lambda>'
    >>> from functools import partial
    >>> name_of_obj(partial(print, sep=","))
    'print'
    >>> from functools import cached_property
    >>> class A:
    ...     @property
    ...     def prop(self):
    ...         return 1.0
    ...     @cached_property
    ...     def cached_prop(self):
    ...         return 2.0
    >>> name_of_obj(A.prop)
    'prop'
    >>> name_of_obj(A.cached_prop)
    'cached_prop'

    Note that ``name_of_obj`` uses the ``__name__`` attribute as its base way to get
    a name. You can customize this behavior though.
    For example, see that:

    >>> from inspect import Signature
    >>> name_of_obj(Signature.replace)
    'replace'

    If you want to get the fully qualified name of an object, you can do:

    >>> alt = partial(name_of_obj, base_name_of_obj=attrgetter('__qualname__'))
    >>> alt(Signature.replace)
    'Signature.replace'

    """
    try:
        return base_name_of_obj(o)
    except caught_exceptions:
        kwargs = dict(
            base_name_of_obj=base_name_of_obj,
            caught_exceptions=caught_exceptions,
            default_factory=default_factory,
        )
        if isinstance(o, (cached_property, partial, partialmethod)) and hasattr(
            o, "func"
        ):
            return name_of_obj(o.func, **kwargs)
        elif isinstance(o, property) and hasattr(o, "fget"):
            return name_of_obj(o.fget, **kwargs)
        elif hasattr(o, "__class__"):
            return name_of_obj(type(o), **kwargs)
        elif hasattr(o, "fset"):
            return name_of_obj(o.fset, **kwargs)
        return default_factory(o)


def ensure_callable(obj: SignatureAble):
    if isinstance(obj, Callable):
        return obj
    else:

        def f(*args, **kwargs):
            """Empty function made just to carry a signature"""

        f.__signature__ = ensure_signature(obj)
        return f


assure_callable = ensure_callable  # alias for backcompatibility


def ensure_signature(obj: SignatureAble) -> Signature:
    if isinstance(obj, Signature):
        return obj
    elif isinstance(obj, Callable):
        return _robust_signature_of_callable(obj)
    elif isinstance(obj, Iterable):
        params = ensure_params(obj)
        try:
            return Signature(parameters=params)
        except TypeError:
            raise TypeError(
                f"Don't know how to make that object into a Signature: {obj}"
            )
    elif isinstance(obj, Parameter):
        return Signature(parameters=(obj,))
    elif obj is None:
        return Signature(parameters=())
    # if you get this far...
    raise TypeError(f"Don't know how to make that object into a Signature: {obj}")


assure_signature = ensure_signature  # alias for backcompatibility


def ensure_param(p):
    if isinstance(p, Parameter):
        return p
    elif isinstance(p, dict):
        return Param(**p)
    elif isinstance(p, str):
        return Param(name=p)
    elif isinstance(p, Iterable):
        name, *r = p
        dflt_and_annotation = dict(zip(["default", "annotation"], r))
        return Param(name, PK, **dflt_and_annotation)
    else:
        raise TypeError(f"Don't know how to make {p} into a Parameter object")


def _params_from_mapping(mapping: MappingType):
    def gen():
        for k, v in mapping.items():
            if isinstance(v, MappingType):
                if "name" in v:
                    assert v["name"] == k, (
                        f"In a mapping specification of a params, "
                        f"either the 'name' of the val shouldn't be specified, "
                        f"or it should be the same as the key ({k}): "
                        f"{dict(mapping)}"
                    )
                    yield v
                else:
                    yield dict(name=k, **v)
            else:
                assert isinstance(v, Parameter) and v.name == k, (
                    f"In a mapping specification of a params, "
                    f"either the val should be a Parameter with the same name as the "
                    f"key ({k}), or it should be a mapping with a 'name' key "
                    f"with the same value as the key: {dict(mapping)}"
                )
                yield v

    return list(gen())


def _add_optional_keywords(sig, kwarg_and_defaults, kwarg_annotations=None):
    """
    Enhances a given signature with additional optional keyword-only arguments.

    Args:
        sig (Signature): The original function signature.
        kwarg_and_defaults (dict): A dictionary of keyword arguments and their default values.
        kwarg_annotations (dict, optional): A dictionary of keyword arguments and their type annotations.

    Returns:
        Signature: The enhanced function signature with additional keyword-only arguments.

    >>> from inspect import signature
    >>> def example_func(x, y): pass
    >>> original_sig = signature(example_func)
    >>> enhanced_sig = _add_optional_keywords(
    ...     original_sig, {'z': 3, 'verbose': False}, {'verbose': bool}
    ... )
    >>> str(enhanced_sig)
    '(x, y, *, z=3, verbose: bool = False)'

    Note:
        - Annotations for the additional keywords are optional.
        - All additional keywords are added as keyword-only arguments.
    """
    if isinstance(sig, Signature):
        sig = Sig(sig).merge_with_sig(
            Sig.from_objs(**kwarg_and_defaults), ch_to_all_pk=False
        )
        sig = sig.ch_kinds(**{k: Sig.KEYWORD_ONLY for k in kwarg_and_defaults})

        kwarg_annotations = kwarg_annotations or {}
        assert all(name in kwarg_and_defaults for name in kwarg_annotations), (
            "Some annotations were given for arguments that were not in kwarg_and_defaults:"
            f"\n{kwarg_and_defaults=}\n{kwarg_annotations=}"
        )
        sig = sig.ch_annotations(**kwarg_annotations)
        return sig
    else:
        func = sig  # assume it's a function
        # apply _add_optional_keywords to that function
        sig = Sig(func)
        sig = _add_optional_keywords(sig, kwarg_and_defaults, kwarg_annotations)
        # and inject the new signature into the function
        return sig(func)


def ensure_params(obj: ParamsAble = None):
    """Get an interable of Parameter instances from an object.

    :param obj:
    :return:

    From a callable:

    >>> def f(w, /, x: float = 1, y=1, *, z: int = 1):
    ...     ...
    >>> ensure_params(f)
    [<Parameter "w">, <Parameter "x: float = 1">, <Parameter "y=1">, <Parameter "z: int = 1">]

    From an iterable of strings, dicts, or tuples

    >>> ensure_params(
    ...     [
    ...         "xyz",
    ...         (
    ...             "b",
    ...             Parameter.empty,
    ...             int,
    ...         ),  # if you want an annotation without a default use Parameter.empty
    ...         (
    ...             "c",
    ...             2,
    ...         ),  # if you just want a default, make it the second element of your tup
    ...         dict(name="d", kind=Parameter.VAR_KEYWORD),
    ...     ]
    ... )  # all kinds are by default PK: Use dict to specify otherwise.
    [<Param "xyz">, <Param "b: int">, <Param "c=2">, <Param "**d">]


    If no input is given, an empty list is returned.

    >>> ensure_params()  # equivalent to ensure_params(None)
    []

    """
    # obj = inspect.unwrap(obj, stop=(lambda f: hasattr(f, "__signature__")))

    if obj is None:
        return []
    elif isinstance(obj, Signature):
        return list(obj.parameters.values())
    try:  # to get params from the builtin signature function
        return list(_robust_signature_of_callable(obj).parameters.values())
    except (TypeError, ValueError):
        if isinstance(obj, Iterable):
            if isinstance(obj, str):
                obj = [obj]
            # TODO: Can do better here! See attempt in _params_from_mapping:
            elif isinstance(obj, Mapping):
                obj = _params_from_mapping(obj)
                # obj = list(obj.values())
            else:
                obj = list(obj)
            if len(obj) == 0:
                return obj
            else:
                # TODO: put this in function that has more kind resolution power
                #  e.g. if a KEYWORD_ONLY arg was encountered, all subsequent
                #  have to be unless otherwise specified.
                return [ensure_param(p) for p in obj]
        else:
            if isinstance(obj, Parameter):
                obj = Signature([obj])
            elif isinstance(obj, Callable):
                obj = _robust_signature_of_callable(obj)
            elif obj is None:
                obj = {}
            if isinstance(obj, Signature):
                return list(obj.parameters.values())
        # if nothing above worked, perhaps you have a wrapped object? Try unwrapping until
        # you find a signature...
        if hasattr(obj, "__wrapped__"):
            obj = unwrap(obj, stop=(lambda f: hasattr(f, "__signature__")))
            return ensure_params(obj)
        else:  # if function didn't return at this point, it didn't find a match, so raise
            # a TypeError
            raise TypeError(
                f"Don't know how to make that object into an iterable of inspect.Parameter "
                f"objects: {obj}"
            )


assure_params = ensure_params  # alias for backcompatibility


class MissingArgValFor:
    """A simple class to wrap an argument name, indicating that it was missing somewhere.

    >>> MissingArgValFor("argname")
    MissingArgValFor("argname")
    """

    def __init__(self, argname: str):
        assert isinstance(argname, str)
        self.argname = argname

    def __repr__(self):
        return f'MissingArgValFor("{self.argname}")'


# TODO: Look into the handling of the Parameter.VAR_KEYWORD kind in params
def extract_arguments(
    params: ParamsAble,
    *,
    what_to_do_with_remainding="return",
    include_all_when_var_keywords_in_params=False,
    assert_no_missing_position_only_args=False,
    **kwargs,
):
    """Extract arguments needed to satisfy the params of a callable, dealing with the
    dirty details.

    Returns an (param_args, param_kwargs, remaining_kwargs) tuple where
    - param_args are the values of kwargs that are PO (POSITION_ONLY) as defined by
    params,
    - param_kwargs are those names that are both in params and not in param_args, and
    - remaining_kwargs are the remaining.

    Intended usage: When you need to call a function `func` that has some
    position-only arguments,
    but you have a kwargs dict of arguments in your hand. You can't just to `func(
    **kwargs)`.
    But you can (now) do
    ```
    args, kwargs, remaining = extract_arguments(kwargs, func)  # extract from kwargs
    what you need for func
    # ... check if remaing is empty (or not, depending on your paranoia), and then
    call the func:
    func(*args, **kwargs)
    ```
    (And if you doing that a lot: Do put it in a decorator!)

    See Also: extract_arguments.without_remainding

    The most frequent case you'll encounter is when there's no POSITION_ONLY args,
    your param_args will be empty
    and you param_kwargs will contain all the arguments that match params,
    in the order of these params.

    >>> from inspect import signature
    >>> def f(a, b, c=None, d=0):
    ...     ...
    ...
    >>> extract_arguments(f, b=2, a=1, c=3, d=4, extra="stuff")
    ((), {'a': 1, 'b': 2, 'c': 3, 'd': 4}, {'extra': 'stuff'})

    But sometimes you do have POSITION_ONLY arguments.
    What extract_arguments will do for you is return the value of these as the first
    element of
    the triple.

    >>> def f(a, b, c=None, /, d=0):
    ...     ...
    ...
    >>> extract_arguments(f, b=2, a=1, c=3, d=4, extra="stuff")
    ((1, 2, 3), {'d': 4}, {'extra': 'stuff'})

    Note above how we get `(1, 2, 3)`, the order defined by the func's signature,
    instead of `(2, 1, 3)`, the order defined by the kwargs.
    So it's the params (e.g. function signature) that determine the order, not kwargs.
    When using to call a function, this is especially crucial if we use POSITION_ONLY
    arguments.

    See also that the third output, the remaining_kwargs, as `{'extra': 'stuff'}` since
    it was not in the params of the function.
    Even if you include a VAR_KEYWORD kind of argument in the function, it won't change
    this behavior.

    >>> def f(a, b, c=None, /, d=0, **kws):
    ...     ...
    ...
    >>> extract_arguments(f, b=2, a=1, c=3, d=4, extra="stuff")
    ((1, 2, 3), {'d': 4}, {'extra': 'stuff'})

    This is because we don't want to assume that all the kwargs can actually be
    included in a call to the function behind the params.
    Instead, the user can chose whether to include the remainder by doing a:
    ```
    param_kwargs.update(remaining_kwargs)
    ```
    et voilà.

    That said, we do understand that it may be a common pattern, so we'll do that
    extra step for you
    if you specify `include_all_when_var_keywords_in_params=True`.

    >>> def f(a, b, c=None, /, d=0, **kws):
    ...     ...
    ...
    >>> extract_arguments(
    ...     f,
    ...     b=2,
    ...     a=1,
    ...     c=3,
    ...     d=4,
    ...     extra="stuff",
    ...     include_all_when_var_keywords_in_params=True,
    ... )
    ((1, 2, 3), {'d': 4, 'extra': 'stuff'}, {})

    If you're expecting no remainder you might want to just get the args and kwargs (
    not this third
    expected-to-be-empty remainder). You have two ways to do that, specifying:
        `what_to_do_with_remainding='ignore'`, which will just return the (args,
        kwargs) pair
        `what_to_do_with_remainding='assert_empty'`, which will do the same, but first
        assert the remainder is empty
    We suggest to use `functools.partial` to configure the `argument_argument` you need.

    >>> from functools import partial
    >>> arg_extractor = partial(
    ...     extract_arguments,
    ...     what_to_do_with_remainding="assert_empty",
    ...     include_all_when_var_keywords_in_params=True,
    ... )
    >>> def f(a, b, c=None, /, d=0, **kws):
    ...     ...
    ...
    >>> arg_extractor(f, b=2, a=1, c=3, d=4, extra="stuff")
    ((1, 2, 3), {'d': 4, 'extra': 'stuff'})

    And what happens if the kwargs doesn't contain all the POSITION_ONLY arguments?

    >>> def f(a, b, c=None, /, d=0):
    ...     ...
    ...
    >>> extract_arguments(f, b=2, d="is a kw arg", e="is not an arg at all")
    ((MissingArgValFor("a"), 2, MissingArgValFor("c")), {'d': 'is a kw arg'}, {'e': 'is not an arg at all'})


    A few more examples...

    Let's call `extract_arguments` with params being not a function,
    but, a Signature instance, a mapping whose values are Parameter instances,
    or an iterable of Parameter instances...

    >>> def func(a, b, /, c=None, *, d=0, **kws):
    ...     ...
    ...
    >>> sig = Signature.from_callable(func)
    >>> param_map = sig.parameters
    >>> param_iterable = param_map.values()
    >>> kwargs = dict(b=2, a=1, c=3, d=4, extra="stuff")
    >>> assert extract_arguments(sig, **kwargs) == extract_arguments(func, **kwargs)
    >>> assert extract_arguments(param_map, **kwargs) == extract_arguments(
    ...     func, **kwargs
    ... )
    >>> assert extract_arguments(param_iterable, **kwargs) == extract_arguments(
    ...     func, **kwargs
    ... )

    Edge case:
    No params specified? No problem. You'll just get empty args and kwargs. Everything
    in the remainder

    >>> extract_arguments(params=(), b=2, a=1, c=3, d=0)
    ((), {}, {'b': 2, 'a': 1, 'c': 3, 'd': 0})

    :param params: Specifies what PO arguments should be extracted.
        Could be a callable, Signature, iterable of Parameters...
    :param what_to_do_with_remainding:
        'return' (default): function will return `param_args`, `param_kwargs`,
        `remaining_kwargs`
        'ignore': function will return `param_args`, `param_kwargs`
        'assert_empty': function will assert that `remaining_kwargs` is empty and then
        return `param_args`, `param_kwargs`
    :param include_all_when_var_keywords_in_params=False,
    :param assert_no_missing_position_only_args=False,
    :param kwargs: The kwargs to extract the args from
    :return: A (param_args, param_kwargs, remaining_kwargs) tuple.
    """

    assert what_to_do_with_remainding in {"return", "ignore", "assert_empty"}
    assert isinstance(include_all_when_var_keywords_in_params, bool)
    assert isinstance(assert_no_missing_position_only_args, bool)

    params = ensure_params(params)
    if not params:
        return (), {}, {k: v for k, v in kwargs.items()}

    params_names = tuple(p.name for p in params)
    names_for_args = [p.name for p in params if p.kind == Parameter.POSITIONAL_ONLY]
    param_kwargs_names = [x for x in params_names if x not in set(names_for_args)]
    remaining_names = [x for x in kwargs if x not in params_names]

    param_args = tuple(kwargs.get(k, MissingArgValFor(k)) for k in names_for_args)
    param_kwargs = {k: kwargs[k] for k in param_kwargs_names if k in kwargs}
    remaining_kwargs = {k: kwargs[k] for k in remaining_names}

    if include_all_when_var_keywords_in_params:
        if (
            next(
                (p.name for p in params if p.kind == Parameter.VAR_KEYWORD),
                None,
            )
            is not None
        ):
            param_kwargs.update(remaining_kwargs)
            remaining_kwargs = {}

    if assert_no_missing_position_only_args:
        missing_argnames = tuple(
            x.argname for x in param_args if isinstance(x, MissingArgValFor)
        )
        assert (
            not missing_argnames
        ), f"There were some missing positional only argnames: {missing_argnames}"

    if what_to_do_with_remainding == "return":
        return param_args, param_kwargs, remaining_kwargs
    elif what_to_do_with_remainding == "ignore":
        return param_args, param_kwargs
    elif what_to_do_with_remainding == "assert_empty":
        assert (
            len(remaining_kwargs) == 0
        ), f"remaining_kwargs not empty: remaining_kwargs={remaining_kwargs}"
        return param_args, param_kwargs


extract_arguments_ignoring_remainder = partial(
    extract_arguments, what_to_do_with_remainding="ignore"
)
extract_arguments_asserting_no_remainder = partial(
    extract_arguments, what_to_do_with_remainding="assert_empty"
)

from collections.abc import Mapping
from typing import Optional
from collections.abc import Iterable


def function_caller(func, args, kwargs):
    return func(*args, **kwargs)


class Param(Parameter):
    """A thin wrap of Parameters: Adds shorter aliases to argument kinds and
    a POSITIONAL_OR_KEYWORD default to the argument kind to make it faster to make
    Parameter objects

    >>> list(map(Param, 'some quick arg params'.split()))
    [<Param "some">, <Param "quick">, <Param "arg">, <Param "params">]
    >>> from inspect import Signature
    >>> P = Param
    >>> Signature([P('x', P.PO), P('y', default=42, annotation=int), P('kw', P.KO)])
    <Signature (x, /, y: int = 42, *, kw)>
    """

    # aliases
    PK = Parameter.POSITIONAL_OR_KEYWORD
    PO = Parameter.POSITIONAL_ONLY
    KO = Parameter.KEYWORD_ONLY
    VP = Parameter.VAR_POSITIONAL
    VK = Parameter.VAR_KEYWORD

    def __init__(self, name, kind=PK, *, default=empty, annotation=empty):
        super().__init__(name, kind, default=default, annotation=annotation)

    def __lt__(self, other) -> bool:
        """Whether the self parameter can be before the other parameter in a signature.

        >>> Param('b') < Param('a', default=1)
        True
        >>> Param('b') > Param('a', default=1)
        False
        >>> Param('b', kind=Param.POSITIONAL_OR_KEYWORD) < Param('a', kind=Param.KEYWORD_ONLY)
        True
        >>> Param('b', kind=Param.POSITIONAL_OR_KEYWORD) > Param('a', kind=Param.KEYWORD_ONLY)
        False

        Note 1: The dual ``>`` operator is also infered.

        Note 2: This means that you can used ``sorted`` on an iterable of Param
        instances, but know that even for sorting the three parameters below,
        the ``sort_params`` function in the ``i2.signatures`` module is more than twice
        as fast.

        >>> sorted(
        ...     [Param('a', default=1),
        ...     Param('b', kind=Param.POSITIONAL_ONLY),
        ...     Param('c')]
        ... )
        [<Param "b">, <Param "c">, <Param "a=1">]
        """
        return (self.kind, self.default is not empty) < (
            other.kind,
            other.default is not empty,
        )


P = Param  # useful shorthand alias


def param_has_default_or_is_var_kind(p: Parameter):
    return p.default is not p.empty or p.kind in var_param_kinds


def parameter_to_dict(p: Parameter) -> dict:
    return dict(name=p.name, kind=p.kind, default=p.default, annotation=p.annotation)


WRAPPER_UPDATES = ("__dict__",)

# A default signature of (*no_sig_args, **no_sig_kwargs)
DFLT_SIGNATURE = signature(lambda *no_sig_args, **no_sig_kwargs: ...)


def _names_of_kind(sig):
    """Compute a tuple containing tuples of names for each kind

    >>> f = lambda a00, /, a11, a12, *a23, a34, a35, a36, **a47: None
    >>> _names_of_kind(Sig(f))
    (('a00',), ('a11', 'a12'), ('a23',), ('a34', 'a35', 'a36'), ('a47',))
    """
    d = defaultdict(list)
    for param in sig.params:
        d[param.kind].append(param.name)
    return tuple(tuple(d[kind]) for kind in range(5))


def maybe_first(items):
    return next(iter(items), None)


def name_of_var_kw_argument(sig):
    var_kw_list = [param.name for param in sig.params if param.kind == VK]
    result = maybe_first(var_kw_list)
    return result


def _map_action_on_cond(kvs, cond, expand):
    for k, v in kvs:
        if cond(
            k
        ):  # make a conditional on (k,v), use type KV, Iterable[KV], expand:KV -> Iterable[KV]
            yield from expand(v[k])  # expand should result in (k,v)
        else:
            yield k, v


def expand_nested_key(d, k):
    for key in d:
        if key == k and isinstance(d[k], dict) and k in d[k]:
            pass

    if k in d and len(d) >= 2:
        return d.items()

    if k in d and isinstance(d[k], dict) and k in d[k]:
        if len(d[k]) == 1:
            return expand_nested_key(d[k], k)
        else:
            return d[k].items()
    else:
        return d.items()


def flatten_if_var_kw(kvs, var_kw_name):
    cond = lambda k: k == var_kw_name
    expand = lambda k: k.items()
    # expand = lambda k: k.values()
    return _map_action_on_cond(kvs, cond, expand)


# TODO: See other signature operating functions below in this module:
#   Do we need them now that we have Sig?
#   Do we want to keep them and have Sig use them?
class Sig(Signature, Mapping):
    """A subclass of inspect.Signature that has a lot of extra api sugar,
    such as
        - making a signature for a variety of input types (callable,
            iterable of callables, parameter lists, strings, etc.)
        - has a dict-like interface
        - signature merging (with operator interfaces)
        - quick access to signature data
        - positional/keyword argument mapping.

    # Positional/Keyword argument mapping

    In python, arguments can be positional (args) or keyword (kwargs).
    ... sometimes both, sometimes a single one is imposed.
    ... and you have variadic versions of both.
    ... and you can have defaults or not.
    ... and all these different kinds have a particular order they must be in.
    It's is mess really. The flexibility is nice -- but still; a mess.

    You only really feel the mess if you try to do some meta-programming with your
    functions.
    Then, methods like `normalize_kind` can help you out, since you can enforce, and
    then assume, some stable interface to your functions.

    Two of the base methods for dealing with positional (args) and keyword (kwargs)
    inputs are:
        - `map_arguments`: Map some args/kwargs input to a keyword-only
            expression of the inputs. This is useful if you need to do some processing
            based on the argument names.
        - `mk_args_and_kwargs`: Translate a fully keyword expression of some
            inputs into an (args, kwargs) pair that can be used to call the function.
            (Remember, your function can have constraints, so you may need to do this.

    The usual pattern of use of these methods is to use `map_arguments`
    to map all the inputs to their corresponding name, do what needs to be done with
    that (example, validation, transformation, decoration...) and then map back to an
    (args, kwargs) pair than can actually be used to call the function.

    Examples of methods and functions using these:
    `call_forgivingly`, `tuple_the_args`, `map_arguments_from_variadics`, `extract_args_and_kwargs`,
    `source_arguments`, and `source_args_and_kwargs`.

    # Making a signature

    You can construct a `Sig` object from a callable,

    >>> def f(w, /, x: float = 1, y=1, *, z: int = 1):
    ...     ...
    >>> Sig(f)
    <Sig (w, /, x: float = 1, y=1, *, z: int = 1)>

    but also from any "ParamsAble" object. Such as...
    an iterable of Parameter instances, strings, tuples, or dicts:

    >>> Sig(
    ...     [
    ...         "a",
    ...         ("b", Parameter.empty, int),
    ...         ("c", 2),
    ...         ("d", 1.0, float),
    ...         dict(name="special", kind=Parameter.KEYWORD_ONLY, default=0),
    ...     ]
    ... )
    <Sig (a, b: int, c=2, d: float = 1.0, *, special=0)>
    >>>
    >>> Sig(
    ...     [
    ...         "a",
    ...         "b",
    ...         dict(name="args", kind=Parameter.VAR_POSITIONAL),
    ...         dict(name="kwargs", kind=Parameter.VAR_KEYWORD),
    ...     ]
    ... )
    <Sig (a, b, *args, **kwargs)>

    The parameters of a signature are like a matrix whose rows are the parameters,
    and the 4 columns are their properties: name, kind, default, and annotation
    (the two laste ones being optional).
    You get a row view when doing `Sig(...).parameters.values()`,
    but what if you want a column-view?
    Here's how:

    >>> def f(w, /, x: float = 1, y=2, *, z: int = 3):
    ...     ...
    >>>
    >>> s = Sig(f)
    >>> s.kinds  # doctest: +NORMALIZE_WHITESPACE
    {'w': <_ParameterKind.POSITIONAL_ONLY: 0>,
    'x': <_ParameterKind.POSITIONAL_OR_KEYWORD: 1>,
    'y': <_ParameterKind.POSITIONAL_OR_KEYWORD: 1>,
    'z': <_ParameterKind.KEYWORD_ONLY: 3>}

    >>> s.annotations
    {'x': <class 'float'>, 'z': <class 'int'>}
    >>> assert (
    ...     s.annotations == f.__annotations__
    ... )  # same as what you get in `__annotations__`
    >>>
    >>> s.defaults
    {'x': 1, 'y': 2, 'z': 3}
    >>> # Note that it's not the same as you get in __defaults__ though:
    >>> assert (
    ...     s.defaults != f.__defaults__ == (1, 2)
    ... )  # not 3, since __kwdefaults__ has that!

    We can sum (i.e. merge) and subtract (i.e. remove arguments) Sig instances.
    Also, Sig instance is callable. It has the effect of inserting it's signature in
    the input
    (in `__signature__`, but also inserting the resulting `__defaults__` and
    `__kwdefaults__`).
    One of the intents is to be able to do things like:

    >>> import inspect
    >>> def f(w, /, x: float = 1, y=1, *, z: int = 1):
    ...     ...
    >>> def g(i, w, /, j=2):
    ...     ...
    ...
    >>>
    >>> @Sig.from_objs(f, g, ["a", ("b", 3.14), ("c", 42, int)])
    ... def some_func(*args, **kwargs):
    ...     ...
    >>> inspect.signature(some_func)
    <Sig (w, i, /, a, x: float = 1, y=1, j=2, b=3.14, c: int = 42, *, z: int = 1)>
    >>>
    >>> sig = Sig(f) + g + ["a", ("b", 3.14), ("c", 42, int)] - "b" - ["a", "z"]
    >>> @sig
    ... def some_func(*args, **kwargs):
    ...     ...
    >>> inspect.signature(some_func)
    <Sig (w, i, x: float = 1, y=1, j=2, c: int = 42)>

    """

    # Adding parameter kinds as class attributes for usage convenience
    POSITIONAL_ONLY = Parameter.POSITIONAL_ONLY
    POSITIONAL_OR_KEYWORD = Parameter.POSITIONAL_OR_KEYWORD
    VAR_POSITIONAL = Parameter.VAR_POSITIONAL
    KEYWORD_ONLY = Parameter.KEYWORD_ONLY
    VAR_KEYWORD = Parameter.VAR_KEYWORD

    def __init__(
        self,
        obj: ParamsAble = None,
        *,
        name=None,
        return_annotation=empty,
        __validate_parameters__=True,
    ):
        """Initialize a Sig instance.
        See Also: `ensure_params` to see what kind of objects you can make `Sig`s with.

        :param obj: A ParamsAble object, which could be:
            - a callable,
            - and iterable of Parameter instances
            - an iterable of strings (representing annotation-less, default-less)
            argument names,
            - tuples: (argname, default) or (argname, default, annotation),
            - dicts: ``{'name': REQUIRED,...}`` with optional `kind`, `default` and
            `annotation` fields
            - None (which will produce an argument-less Signature)

        >>> Sig(["a", "b", "c"])
        <Sig (a, b, c)>
        >>> Sig(
        ...     ["a", ("b", None), ("c", 42, int)]
        ... )  # specifying defaults and annotations
        <Sig (a, b=None, c: int = 42)>
        >>> import inspect
        >>> Sig(
        ...     ["a", ("b", inspect._empty, int)]
        ... )  # specifying an annotation without a default
        <Sig (a, b: int)>
        >>> Sig(["a", "b", "c"], return_annotation=str)  # specifying return annotation
        <Sig (a, b, c) -> str>
        >>> Sig('(a: int = 0, b: str = None, c: float = 3.14) -> str')
        <Sig (a: int = 0, b: str = None, c: float = 3.14) -> str>

        But you can always specify parameters the "long" way

        >>> Sig(
        ...     [inspect.Parameter(name="kws", kind=inspect.Parameter.VAR_KEYWORD)],
        ...     return_annotation=str,
        ... )
        <Sig (**kws) -> str>

        And note that:

        >>> Sig()
        <Sig ()>
        >>> Sig(None)
        <Sig ()>
        """
        if isinstance(obj, str):
            if re.match(r"^\(.*\)", obj):
                # This is a string representation of a signature
                # Dynamically create a function with the given signature then generate
                # the Sig object from this function.
                exec_env = dict()
                f_def = f"def f{obj}: pass"
                exec(f_def, exec_env)
                obj = exec_env["f"]
            else:
                obj = obj.split()

        if isinstance(obj, property):
            obj = obj.fget
        elif isinstance(obj, cached_property):
            obj = obj.func

        if (
            not isinstance(obj, Signature)
            and callable(obj)
            and return_annotation is empty
        ):
            return_annotation = _robust_signature_of_callable(obj).return_annotation
        # TODO: Catch errors and enhance error message with more what-to-do-about it
        #  message. For example,
        #  ValueError: wrong parameter order: positional or keyword parameter before
        #  positional-only parameter
        #  --> Here we could tell the user what pair of variables violated the rule
        super().__init__(
            ensure_params(obj),
            return_annotation=return_annotation,
            __validate_parameters__=__validate_parameters__,
        )
        self.names_of_kind = _names_of_kind(self)

        if len(self.names_of_kind[Parameter.VAR_POSITIONAL]) > 1:
            vps = self.names_of_kind[Parameter.VAR_POSITIONAL]
            raise InvalidSignature(f"You can't have several variadic keywords: {vps}")
        if len(self.names_of_kind[Parameter.VAR_KEYWORD]) > 1:
            vks = self.names_of_kind[Parameter.VAR_KEYWORD]
            raise InvalidSignature(f"You can't have several variadic keywords: {vks}")

        self.name = name or name_of_obj(obj)

    # TODO: Add params for more validation (e.g. arg number/name matching?)
    # TODO: Switch to ignore_incompatible_signatures=False when existing code is
    #   changed accordingly.
    def wrap(
        self,
        func: Callable,
        ignore_incompatible_signatures: bool = True,
        *,
        copy_function: bool | Callable = False,
    ):
        """Gives the input function the signature.

        This is similar to the `functools.wraps` function, but parametrized by a
        signature
        (not a callable). Also, where as both write to the input func's `__signature__`
        attribute, here we also write to
        - `__defaults__` and `__kwdefaults__`, extracting these from `__signature__`
            (functools.wraps doesn't do that at the time of writing this
            (see https://github.com/python/cpython/pull/21379)).
        - `__annotations__` (also extracted from `__signature__`)
        - does not write to `__module__`, `__name__`, `__qualname__`, `__doc__`
            (because again, we're basinig the injecton on a signature, not a function,
            so we have no name, doc, etc...)

        WARNING: The fact that you've modified the signature of your function doesn't
        mean that the decorated function will work as expected (or even work at all).
        See below for examples.

        >>> def f(w, /, x: float = 1, y=2, z: int = 3):
        ...     return w + x * y ** z
        >>> f(0, 1)  # 0 + 1 * 2 ** 3
        8
        >>> f.__defaults__
        (1, 2, 3)
        >>> assert 8 == f(0) == f(0, 1) == f(0, 1, 2) == f(0, 1, 2, 3)

        Now let's create a very similar function to f, but where:
        - w is not position-only
        - x annot is int instead of float, and doesn't have a default
        - z's default changes to 10

        >>> def g(w, x: int, y=2, z: int = 10):
        ...     return w + x * y ** z
        >>> s = Sig(g)
        >>> f = s.wrap(f)
        >>> import inspect
        >>> inspect.signature(f)  # see that
        <Sig (w, x: int, y=2, z: int = 10)>
        >>> # But (unlike with functools.wraps) here we get __defaults__ and
        __kwdefault__
        >>> f.__defaults__  # see that x has no more default & z's default is now 10
        (2, 10)
        >>> f(
        ...     0, 1
        ... )  # see that now we get a different output because using different defaults
        1024

        Remember that you are modifying the signature, not the function itself.
        Signature changes in defaults will indeed change the function's behavior.
        But changes in name or kind will only be reflected in the signature, and
        misalignment with the wrapped function will lead to unexpected results.

        >>> def f(w, /, x: float = 1, y=2, *, z: int = 3):
        ...     return w + x * y ** z
        >>> f(0)  # 0 + 1 * 2 ** 3
        8
        >>> f(0, 1, 2, 3)  # error expected!
        Traceback (most recent call last):
          ...
        TypeError: f() takes from 1 to 3 positional arguments but 4 were given

        But if you try to remove the argument kind constraint by just changing the
        signature, you'll fail.

        >>> def g(w, x: float = 1, y=2, z: int = 3):
        ...     return w + x * y ** z
        >>> f = Sig(g).wrap(f)
        >>> f(0)
        Traceback (most recent call last):
          ...
        TypeError: f() missing 1 required keyword-only argument: 'z'
        >>> f(0, 1, 2, 3)
        Traceback (most recent call last):
          ...
        TypeError: f() takes from 0 to 3 positional arguments but 4 were given

        TODO: Give more explanations why this is.
        """

        # TODO: Should we make copy_function=False the default,
        #  so as to not override decorated function itself by default?
        if copy_function:
            if isinstance(copy_function, bool):
                from i2.util import copy_func as copy_function
            else:
                assert callable(
                    copy_function
                ), f"copy_function must be a callable. This is not: {copy_function}"
            func = copy_function(func)

        # Analyze self and func signature to validate sanity
        _validate_sanity_of_signature_change(func, self, ignore_incompatible_signatures)

        # Change (mutate!) func, writing a new __signature__, __annotations__,
        # __defaults__ and __kwdefaults__
        func.__signature__ = Sig(
            self.parameters.values(), return_annotation=self.return_annotation
        )
        func.__annotations__ = self.annotations
        func.__defaults__, func.__kwdefaults__ = self._dunder_defaults_and_kwdefaults()

        # special case of functools.partial: need to tell .keywords about kwdefaults
        if isinstance(func, partial):
            # TODO: .args can't be modified -- write test to see if problem.
            #   If it is, consider returning a new partial with updated args & keywords.
            # wrapped_func.args = wrapped_func.args + wrapped_func.__defaults__
            func.keywords.update(func.__kwdefaults__)

        return func

    def __call__(self, func: Callable):
        """Gives the input function the signature.
        Just calls Sig.wrap so see docs of Sig.wrap (which contains examples and
        doctests).
        """
        return self.wrap(func)

    @classmethod
    def sig_or_default(cls, obj, default_signature=DFLT_SIGNATURE):
        """Returns a Sig instance, or a default signature if there was a ValueError
        trying to construct it.

        For example, `time.time` doesn't have a signature

        >>> import time
        >>> has_signature(time.time)
        False

        But we can tell `Sig` to give it the default one:

        >>> str(Sig.sig_or_default(time.time))
        '(*no_sig_args, **no_sig_kwargs)'

        That's the default signature, which should work for most purposes.
        You can also specify what the default should be though.

        >>> fake_signature = Sig(lambda *time_takes_no_arguments: ...)
        >>> str(Sig.sig_or_default(time.time, fake_signature))
        '(*time_takes_no_arguments)'

        Careful though. If you assign a signature to a function that is not aligned
        with that actually functioning of the function, bad things will happen.
        In this case, the actual signature of time is the empty signature:

        >>> str(Sig.sig_or_default(time.time, Sig(lambda: ...)))
        '()'

        """
        try:
            # (try to) return cls(obj) if obj is callable:
            if callable(obj):
                sig = cls(obj)
                # Check if we got our default signature (which means no real signature exists)
                if str(sig) == str(DFLT_SIGNATURE):
                    return Sig(default_signature)
                return sig
            else:
                raise TypeError(f"Object is not callable: {obj}")
        except ValueError:
            # if a ValueError is raised, return the default_signature
            return Sig(default_signature)

    @classmethod
    def sig_or_none(cls, obj):
        """Returns a Sig instance, or None if there was a ValueError trying to
        construct it.
        One use case is to be able to tell if an object has a signature or not.

        >>> robust_has_signature = lambda obj: bool(Sig.sig_or_none(obj))
        >>> robust_has_signature(robust_has_signature)  # an easy case
        True
        >>> robust_has_signature(
        ...     Sig
        ... )  # another easy one: This time, a type/class (which is callable, yes)
        True

        But here's where it get's interesting. `print`, a builtin, doesn't have a
        signature through inspect.signature.

        >>> has_signature(print)  # doctest: +SKIP
        False

        But we do get one with robust_has_signature

        >>> robust_has_signature(print)
        True

        """
        return cls.sig_or_default(obj, default_signature=None)

    def __bool__(self):
        return True

    def _positional_and_keyword_defaults(self):
        """Get ``{name: default, ...}`` dicts of positional and keyword defaults.

        >>> def foo(w, /, x: float, y=1, *, z: int = 1):
        ...     ...
        >>> pos_defaults, kw_defaults = Sig(foo)._positional_and_keyword_defaults()
        >>> pos_defaults
        {'y': 1}
        >>> kw_defaults
        {'z': 1}
        """
        ko_names = self.names_of_kind[KO]
        dflts = self.defaults
        return (
            {name: dflts[name] for name in dflts if name not in ko_names},
            {name: dflts[name] for name in dflts if name in ko_names},
        )

    def _dunder_defaults_and_kwdefaults(self):
        """Get the __defaults__, __kwdefaults__ (i.e. what would be the dunders baring
        these names in a python callable)

        >>> def foo(w, /, x: float, y=1, *, z: int = 1):
        ...     ...
        >>> __defaults__, __kwdefaults__ = Sig(foo)._dunder_defaults_and_kwdefaults()
        >>> __defaults__
        (1,)
        >>> __kwdefaults__
        {'z': 1}
        """

        pos_defaults, kw_defaults = self._positional_and_keyword_defaults()
        return (
            tuple(
                pos_defaults.values()
            ),  # as known as __defaults__ in python callables
            kw_defaults,  # as known as __kwdefaults__ in python callables
        )

    def to_signature_kwargs(self):
        """The dict of keyword arguments to make this signature instance.

        >>> def f(w, /, x: float = 2, y=1, *, z: int = 0) -> float:
        ...     ...
        >>> Sig(f).to_signature_kwargs()  # doctest: +NORMALIZE_WHITESPACE
        {'parameters':
            [<Parameter "w">,
            <Parameter "x: float = 2">,
            <Parameter "y=1">,
            <Parameter "z: int = 0">],
        'return_annotation': <class 'float'>}

        Note that this does NOT return:
        ```
                {'parameters': self.parameters,
                'return_annotation': self.return_annotation}
        ```
        which would not actually work as keyword arguments of ``Signature``.
        Yeah, I know. Don't ask me, ask the authors of `Signature`!

        Instead, `parammeters` will be ``list(self.parameters.values())``, which does
        work.

        """
        return {
            "parameters": list(self.parameters.values()),
            "return_annotation": self.return_annotation,
        }

    def to_simple_signature(self):
        """A builtin ``inspect.Signature`` instance equivalent (i.e. without the extra
        properties and methods)

        >>> def f(w, /, x: float = 2, y=1, *, z: int = 0):
        ...     ...
        >>> Sig(f).to_simple_signature()
        <Signature (w, /, x: float = 2, y=1, *, z: int = 0)>

        """
        return Signature(**self.to_signature_kwargs())

    def pair_with(self, other_sig) -> "SigPair":
        """Get an object that pairs with another signature for comparison, merging, etc.

        See `SigPair` for more details.
        """
        return SigPair(self, other_sig)

    def is_call_compatible_with(self, other_sig, *, param_comparator: Callable = None):
        """Return True if the signature is compatible with ``other_sig``. Meaning that
        all valid ways to call the signature are valid for ``other_sig``.
        """
        return is_call_compatible_with(
            self, other_sig, param_comparator=param_comparator
        )

    # TODO: Make these dunders open/close
    # def __le__(self, other_sig):
    #     """The "less than or equal" operator (<=).
    #     Return True if the signature is compatible with ``other_sig``. Meaning that
    #     all valid ways to call the signature are valid for ``other_sig``.
    #     """
    #     return self.is_call_compatible_with(other_sig)

    # def __ge__(self, other_sig):
    #     """The "greater than or equal" operator (>=).
    #     Return True if ``other_sig`` is compatible with the signature. Meaning that
    #     all valid ways to call ``other_sig`` are valid for the signature.
    #     """
    #     return other_sig <= self

    @classmethod
    def from_objs(
        cls,
        *objs,
        default_conflict_method: str = DFLT_DEFAULT_CONFLICT_METHOD,
        return_annotation=empty,
        **name_and_dflts,
    ):
        objs = list(objs)
        for name, default in name_and_dflts.items():
            objs.append([{"name": name, "kind": PK, "default": default}])
        if len(objs) > 0:
            first_obj, *objs = objs
            sig = cls(ensure_params(first_obj))
            for obj in objs:
                sig = sig.merge_with_sig(
                    obj, default_conflict_method=default_conflict_method
                )
                # sig = sig + obj
            return Sig(sig, return_annotation=return_annotation)
        else:  # if no objs are given
            return cls(return_annotation=return_annotation)  # return an empty signature

    @classmethod
    def from_params(cls, params):
        if isinstance(params, Parameter):
            params = (params,)
        return cls(params)

    @property
    def params(self):
        """Just list(self.parameters.values()), because that's often what we want.
        Why a Sig.params property when we already have a Sig.parameters property?

        Well, as much as is boggles my mind, it so happens that the Signature.parameters
        is a name->Parameter mapping, but the Signature argument `parameters`,
        though baring the same name,
        is expected to be a list of Parameter instances.

        So Sig.params is there to restore semantic consistence sanity.
        """
        return list(self.parameters.values())

    @property
    def names(self):
        return list(self.keys())

    @property
    def kinds(self):
        return {p.name: p.kind for p in self.values()}

    @property
    def defaults(self):
        """A ``{name: default,...}`` dict of defaults (regardless of kind)"""
        return {p.name: p.default for p in self.values() if p.default is not p.empty}

    @property
    def _defaults_(self):
        """What the ``__defaults__`` value would be for a func of the same signature"""
        return tuple(
            p.default
            for p in self.values()
            if (p.default is not p.empty and p.kind != KO)
        )

    @property
    def _kwdefaults_(self):
        """What the ``__kwdefaults__`` value would be for a func of the same signature"""
        return {
            p.name: p.default
            for p in self.values()
            if p.default is not p.empty and p.kind == KO
        }

    @property
    def annotations(self):
        """{arg_name: annotation, ...} dict of annotations of the signature.
        What `func.__annotations__` would give you.
        """
        return {
            p.name: p.annotation for p in self.values() if p.annotation is not p.empty
        }

    def detail_names_by_kind(self):
        return (
            self.names_of_kind[PO],
            self.names_of_kind[PK],
            next(iter(self.names_of_kind[VP]), None),
            self.names_of_kind[KO],
            next(iter(self.names_of_kind[VK]), None),
        )

    # TODO: Can be cleaned and generalized (include/exclude, function filter etc.)
    def get_names(self, spec, *, conserve_sig_order=True, allow_excess=False):
        """Return a tuple of names corresponding to the given spec.

        :param spec: An integer, string, or iterable of intergers and strings
        :param conserve_sig_order: Whether to order according to the signature
        :param allow_excess: Whether to allow items in spec that are not in signature

        >>> sig = Sig('a b c d e')
        >>> sig.get_names(0)
        ('a',)
        >>> sig.get_names([0, 2])
        ('a', 'c')
        >>> sig.get_names('b')
        ('b',)
        >>> sig.get_names([0, 'c', -1])
        ('a', 'c', 'e')

        See that by default the order of the signature is conserved:

        >>> sig.get_names('b e d')
        ('b', 'd', 'e')

        But you can change that default to conserve the order of the ``spec`` instead:

        >>> sig.get_names('b e d', conserve_sig_order=False)
        ('b', 'e', 'd')

        By default, you can't mention names that are not in signature.
        To allow this (making ``spec`` have "extract these" interpretation),
        set ``allow_excess=True``:

        >>> sig.get_names(['a', 'c', 'e', 'g', 'h'], allow_excess=True)
        ('a', 'c', 'e')

        """
        if isinstance(spec, str):
            spec = spec.split()
        elif isinstance(spec, int):
            spec = [spec]
        if isinstance(spec, Iterable):

            def find_names():
                names = self.names
                for item in spec:
                    if isinstance(item, int):
                        if item < len(names):
                            yield names[item]
                        elif not allow_excess:
                            raise IndexError(
                                f"There are only {len(names)} names in the signatures,"
                                f"but you asked for the index: {item}"
                            )
                    else:
                        if item in names:
                            yield item
                        elif not allow_excess:
                            raise ValueError(
                                f"No such param name in signatures: {item}"
                            )

            matched_names = tuple(find_names())
            if conserve_sig_order:
                _matched_names = tuple(x for x in self.names if x in matched_names)
                matched_names = _matched_names + tuple(
                    x for x in matched_names if x not in _matched_names
                )
            return matched_names
        else:
            raise TypeError(f"Unknown spec type: {spec}")

    def __iter__(self):
        return iter(self.parameters)

    def __len__(self):
        return len(self.parameters)

    # TODO: Return type inconsistent. When k is a string, returns Parameter,
    #  when an iterable of strings (or 'space separated argument names'),
    #  returns a signature. Could also return a single argument signatures.
    #  Behavior might be confusing. Pros/Cons? See if any current users of getitem,
    #  and switch to single arg signature return (that's consistent, and convenience
    #  of sig[argname] is weak (given sig.params[argname] does it)!)
    def __getitem__(self, k):
        if isinstance(k, int) or isinstance(k, slice):
            # TODO: Could extend slice handing to be able to use names as start and stop
            k = self.names[k]
        if isinstance(k, str):
            names = k.split()  # to handle 'multiple args in a string'
            if len(names) == 1:
                return self.parameters[k]
        else:
            assert isinstance(k, Iterable), f"key should be iterable, was: {k}"
            names = k
        params = [self[name] for name in names]
        return Sig.from_params(params)

    # TODO: Deprecate. Should use names_of_kind directly
    def names_for_kind(self, kind):
        """Get the arg names tuple for a given kind.
        Note, if you need to do this several times, or for several kinds, use
        ``names_of_kind`` property (a tuple) instead: It groups all names of kinds once,
        and caches the result.
        """
        from warnings import warn

        warn("Deprecated", DeprecationWarning)
        return self.names_of_kind[kind]

    # TODO: Consider using names_of_kind in other methods/properties

    @property
    def has_var_kinds(self):
        """
        >>> Sig(lambda x, *, y: None).has_var_kinds
        False
        >>> Sig(lambda x, *y: None).has_var_kinds
        True
        >>> Sig(lambda x, **y: None).has_var_kinds
        True
        """
        return bool(self.names_of_kind[VP]) or bool(self.names_of_kind[VK])
        # Old version:
        # return any(p.kind in var_param_kinds for p in self.values())

    @property
    def index_of_var_positional(self):
        """The index of the VAR_POSITIONAL param kind if any, and None if not.
        See also, Sig.index_of_var_keyword

        >>> assert Sig(lambda x, *y, z: 0).index_of_var_positional == 1
        >>> assert Sig(lambda x, /, y, **z: 0).index_of_var_positional == None
        """
        return next((i for i, p in enumerate(self.params) if p.kind == VP), None)

    @property
    def var_positional_name(self):
        idx = self.index_of_var_positional
        if idx is not None:
            return self.names[idx]
        # else returns None

    @property
    def has_var_positional(self):
        """
        Use index_of_var_positional or var_keyword_name directly when needing that
        information as well. This will avoid having to check the kinds list twice.
        """
        return any(p.kind == VP for p in self.values())

    @property
    def index_of_var_keyword(self):
        """The index of a VAR_KEYWORD param kind if any, and None if not.
        See also, Sig.index_of_var_positional

        >>> assert Sig(lambda **kwargs: 0).index_of_var_keyword == 0
        >>> assert Sig(lambda a, **kwargs: 0).index_of_var_keyword == 1
        >>> assert Sig(lambda a, *args, **kwargs: 0).index_of_var_keyword == 2

        And if there's none...

        >>> assert Sig(lambda a, *args, b=1: 0).index_of_var_keyword is None

        """
        last_arg_idx = len(self) - 1
        if last_arg_idx != -1:
            if self.params[last_arg_idx].kind == VK:
                return last_arg_idx
        # else returns None

    @property
    def var_keyword_name(self):
        idx = self.index_of_var_keyword
        if idx is not None:
            return self.names[idx]
        # else returns None

    @property
    def has_var_keyword(self):
        """
        Use index_of_var_keyword or var_keyword_name directly when needing that
        information as well. This will avoid having to check the kinds list twice.
        """
        return any(p.kind == VK for p in self.values())

    @property
    def required_names(self):
        """A tuple of required names, preserving the original signature order.

        A required name is that must be given in a function call, that is, the name of a
        paramater that doesn't have a default, and is not a variadic.

        That lost one is a frequent gotcha, so oo not fall in that gotcha that easily,
        we provide a property that contains what we need.

        >>> f = lambda a00, /, a11, a12, *a23, a34, a35=1, a36='two', **a47: None
        >>> Sig(f).required_names
        ('a00', 'a11', 'a12', 'a34')
        """
        # Note: This is quicker than using self.names_of_kind:
        return tuple(
            p.name
            for p in self.params
            if p.default is empty and p.kind not in var_param_kinds
        )

    @property
    def n_required(self):
        """The number of required arguments.
        A required argument is one that doesn't have a default, nor is VAR_POSITIONAL
        (*args) or VAR_KEYWORD (**kwargs).
        Note: Sometimes a minimum number of arguments in VAR_POSITIONAL and
        VAR_KEYWORD are in fact required,
        but we can't see this from the signature, so we can't tell you about that! You
        do the math.

        >>> f = lambda a00, /, a11, a12, *a23, a34, a35=1, a36='two', **a47: None
        >>> Sig(f).n_required
        4
        """
        return len(self.required_names)

    @property
    def positional_names(self):
        return self.names_of_kind[PO] + self.names_of_kind[PK]

    @property
    def keyword_names(self):
        return self.names_of_kind[PK] + self.names_of_kind[KO]

    def _transform_params(self, changes_for_name: dict):
        for name in self:
            if name in changes_for_name:
                p = changes_for_name[name]
                if isinstance(p, Parameter):
                    p = parameter_to_dict(p)
                yield self[name].replace(**p)
            else:
                # if name is not in params, just use existing param
                yield self[name]

    def modified(self, /, _allow_reordering=False, **changes_for_name):
        """Returns a modified (new) signature object.

        Note: This function doesn't modify the signature, but creates a modified copy
        of the signature.

        IMPORTANT WARNING: This is an advanced feature. Avoid wrapping a function with
        a modified signature, as this may not have the intended effect.

        >>> def foo(pka, *vpa, koa, **vka): ...
        >>> sig = Sig(foo)
        >>> sig
        <Sig (pka, *vpa, koa, **vka)>
        >>> assert sig.kinds['pka'] == PK

        Let's make a signature that is the same as sig, except that
            - `poa` is given a PO (POSITIONAL_ONLY) kind insteadk of PK
            - `koa` is given a default of None
            - the signature is given a return_annotation of str

        >>> new_sig = sig.modified(
        ...     pka={'kind': PO},
        ...     koa={'default': None},
        ...     return_annotation=str
        ... )
        >>> new_sig
        <Sig (pka, /, *vpa, koa=None, **vka) -> str>
        >>> assert new_sig.kinds['pka'] == PO  # now pos is of the PO kind!

        Here's an example of changing signature parameters in bulk.
        Here we change all kinds to be the friendly PK kind.

        >>> sig.modified(**{name: {'kind': PK} for name in sig.names})
        <Sig (pka, vpa, koa, vka)>

        Repetition of the above: This gives you a signature with all PK kinds.
        If you wrap a function with it, it will look like it has all PK kinds.
        But that doesn't mean you can actually use thenm as such.
        You'll need to modify (decorate further) your function further to reflect
        its new signature.

        On the other hand, if you decorate a function with a sig that adds or modifies
        defaults, these defaults will actually be used (unlike with `functools.wraps`).

        """
        new_return_annotation = changes_for_name.pop(
            "return_annotation", self.return_annotation
        )

        if _allow_reordering:
            params = sort_params(self._transform_params(changes_for_name))
        else:
            params = list(self._transform_params(changes_for_name))

        return Sig(params, name=self.name, return_annotation=new_return_annotation)

    def sort_params(self):
        """Returns a signature with the parameters sorted by kind and default presence."""
        sorted_params = sort_params(self.params)
        return type(self)(
            sorted_params, name=self.name, return_annotation=self.return_annotation
        )

    def ch_param_attrs(
        self, /, param_attr, *arg_new_vals, _allow_reordering=False, **kwargs_new_vals
    ):
        """Change a specific attribute of the params, returning a modified signature.
        This is a convenience method for the modified method when we're targetting
        a fixed param attribute: 'name', 'kind', 'default', or 'annotation'

        Instead of having to do this

        >>> def foo(a, *b, **c): ...
        >>> Sig(foo).modified(a={'name': 'A'}, b={'name': 'B'}, c={'name': 'C'})
        <Sig (A, *B, **C)>

        We can simply do this

        >>> Sig(foo).ch_param_attrs('name', a='A', b='B', c='C')
        <Sig (A, *B, **C)>

        One quite useful thing you can do with this is to set defaults, or set defaults
        where there are none. If you wrap your function with such a modified signature,
        you get a "curried" version of your function (called "partial" in python).
        (Note that the `functools.wraps` won't deal with defaults "correctly", but
        wrapping with `Sig` objects takes care of that oversight!)

        >>> def foo(a, b, c):
        ...     return a + b * c
        >>> special_foo = Sig(foo).ch_param_attrs('default', b=2, c=3)(foo)
        >>> Sig(special_foo)
        <Sig (a, b=2, c=3)>
        >>> special_foo(5)  # should be 5 + 2 * 3 == 11
        11


        # TODO: Would like to make this work (reordering)
        # Now, if you want to set a default for a but not b and c for example, you'll
        # get complaints:
        #
        # ```
        # ValueError: non-default argument follows default argument
        # ```
        #
        # will tell you.
        #
        # It's true. But if you're fine with rearranging the argument order,
        # `ch_param_attrs` can take care of that for you.
        # You'll have to tell it explicitly that you wish for this though, because
        # it's conservative.
        #
        # >>> # Note that for time being, Sig.wraps doesn't make a copy of the function
        # >>> #  so we need to redefine foo here@
        # >>> def foo(a, b, c):
        # ...     return a + b * c
        # >>> wrapper = Sig(foo).ch_param_attrs(
        # ... 'default', a=10, _allow_reordering=True
        # ... )
        # >>> another_foo = wrapper(foo)
        # >>> Sig(another_foo)
        # <Sig (b, c, a=10)>
        # >>> another_foo(2, 3)  # should be 10 + (2 * 3) =
        # 16

        """

        if not param_attr in param_attributes:
            raise ValueError(
                f"param_attr needs to be one of: {param_attributes}.",
                f" Was: {param_attr}",
            )
        all_pk_self = self.modified(
            _allow_reordering=True, **{name: {"kind": PK} for name in self.names}
        )
        new_attr_vals = all_pk_self.bind_partial(
            *arg_new_vals, **kwargs_new_vals
        ).arguments
        changes_for_name = {
            name: {param_attr: val} for name, val in new_attr_vals.items()
        }
        return self.modified(_allow_reordering=_allow_reordering, **changes_for_name)

    # Note: Oh, functools, why do you make currying so limited!
    # ch_names = partialmethod(ch_param_attrs, param_attr="name")
    # ch_kinds = partialmethod(ch_param_attrs, param_attr="kind", _allow_reordering=True)
    # ch_defaults = partialmethod(
    #     ch_param_attrs, param_attr="default", _allow_reordering=True
    # )
    # ch_annotations = partialmethod(ch_param_attrs, param_attr="annotation")

    def ch_names(self, /, **changes_for_name):
        argnames_not_in_sig = changes_for_name.keys() - self.keys()
        if argnames_not_in_sig:
            raise ValueError(
                f"argument names not in signature: {', '.join(argnames_not_in_sig)}"
            )
        return self.ch_param_attrs("name", **changes_for_name)

    def ch_kinds(self, /, _allow_reordering=True, **changes_for_name):
        return self.ch_param_attrs(
            "kind", _allow_reordering=_allow_reordering, **changes_for_name
        )

    def ch_kinds_to_position_or_keyword(self):
        return all_pk_signature(self)

    def ch_defaults(self, /, _allow_reordering=True, **changes_for_name):
        return self.ch_param_attrs(
            "default", _allow_reordering=_allow_reordering, **changes_for_name
        )

    def ch_annotations(self, /, **changes_for_name):
        return self.ch_param_attrs("annotation", **changes_for_name)

    def add_optional_keywords(
        self=None, /, kwarg_and_defaults=None, kwarg_annotations=None
    ):
        """Add optional keyword arguments to a signature.

        >>> @Sig.add_optional_keywords({"c": 2, "d": 3}, {"c": int})
        ... def foo(a, *, b=1, **kwargs):
        ...     return f"{a=}, {b=}, {kwargs=}"
        ...

        You can still call the function as before, and like before, any "extra" keyword
        arguments will be passed to kwargs:

        >>> foo(0, d=10)
        "a=0, b=1, kwargs={'d': 10}"

        The difference is that now the signature of `foo` now has `c` and `d`:

        >>> str(Sig(foo))
        '(a, *, c: int = 2, d=3, b=1, **kwargs)'

        """

        # Resolve arguments ( to be able to use this method as a decorator)
        if isinstance(self, dict):
            if kwarg_and_defaults is not None:
                kwarg_annotations = kwarg_and_defaults
                kwarg_and_defaults = None
            if kwarg_and_defaults is None:
                kwarg_and_defaults = self
            self = None

        # If self is None, a factory is returned
        if self is None:
            return partial(
                _add_optional_keywords,
                kwarg_and_defaults=kwarg_and_defaults,
                kwarg_annotations=kwarg_annotations,
            )
        else:  # if not, apply _add_optional_keywords to self
            return _add_optional_keywords(
                self, kwarg_and_defaults, kwarg_annotations=kwarg_annotations
            )

    # TODO: Make default_conflict_method be able to be a callable and get rid of string
    #  mapping complexity in merge_with_sig code
    def merge_with_sig(
        self,
        sig: ParamsAble,
        ch_to_all_pk: bool = False,
        *,
        default_conflict_method: SigMergeOptions = DFLT_DEFAULT_CONFLICT_METHOD,
    ):
        """Return a signature obtained by merging self signature with another signature.
        Insofar as it can, given the kind precedence rules, the arguments of self will
        appear first.

        :param sig: The signature to merge with.
        :param ch_to_all_pk: Whether to change all kinds of both signatures to PK (
        POSITIONAL_OR_KEYWORD)
        :return:

        >>> def func(a=None, *, b=1, c=2):
        ...     ...
        ...
        >>>
        >>> s = Sig(func)
        >>> s
        <Sig (a=None, *, b=1, c=2)>

        Observe where the new arguments ``d`` and ``e`` are placed,
        according to whether they have defaults and what their kind is:

        >>> s.merge_with_sig(["d", "e"])
        <Sig (d, e, a=None, *, b=1, c=2)>
        >>> s.merge_with_sig(["d", ("e", 4)])
        <Sig (d, a=None, e=4, *, b=1, c=2)>
        >>> s.merge_with_sig(["d", dict(name="e", kind=KO, default=4)])
        <Sig (d, a=None, *, b=1, c=2, e=4)>
        >>> s.merge_with_sig(
        ...     [dict(name="d", kind=KO), dict(name="e", kind=KO, default=4)]
        ... )
        <Sig (a=None, *, d, b=1, c=2, e=4)>

        If the kind of the params is not important, but order is, you can specify
        ``ch_to_all_pk=True``:

        >>> s.merge_with_sig(["d", "e"], ch_to_all_pk=True)
        <Sig (d, e, a=None, b=1, c=2)>
        >>> s.merge_with_sig([("d", 3), ("e", 4)], ch_to_all_pk=True)
        <Sig (a=None, b=1, c=2, d=3, e=4)>

        """
        if ch_to_all_pk:
            _self = Sig(all_pk_signature(self))
            _sig = Sig(all_pk_signature(ensure_signature(sig)))
        else:
            _self = self
            _sig = Sig(sig)

        # Validation of the signatures

        _msg = f"\nHappened during an attempt to merge {self} and {sig}"
        errors = {}

        # Check if both signatures have VAR_POSITIONAL parameters
        if _self.has_var_keyword and _sig.has_var_keyword:
            errors["var_positional_conflict"] = (
                f"Can't merge two signatures if they both have a VAR_POSITIONAL parameter: {_msg}"
            )

        # Check if both signatures have VAR_KEYWORD parameters
        if _self.has_var_keyword and _sig.has_var_keyword:
            errors["var_keyword_conflict"] = (
                f"Can't merge two signatures if they both have a VAR_KEYWORD parameter: {_msg}"
            )

        # Check if parameters with the same name have the same kind
        if not all(
            _self[name].kind == _sig[name].kind for name in _self.keys() & _sig.keys()
        ):
            errors["kind_mismatch"] = (
                "During a signature merge, if two names are the same, they must have the "
                f"**same kind**:\n\t{_msg}\n"
                "Tip: If you're trying to merge functions in some way, consider decorating "
                "them with a signature mapping that avoids the argument name clashing"
            )

        # Check if default_conflict_method is a valid SigMergeOption
        if default_conflict_method not in get_args(SigMergeOptions):
            errors["invalid_conflict_method"] = (
                "default_conflict_method should be one of: "
                f"{get_args(SigMergeOptions)}"
            )

        if errors:
            # TODO: Raise all errors at once?
            # TODO: Raise custom errors with more info?

            # raise the first error
            error_msg = next(iter(errors.values()))
            raise IncompatibleSignatures(error_msg, sig1=_self, sig2=_sig)

        if default_conflict_method == "take_first":
            _sig = _sig - set(_self.keys() & _sig.keys())
        elif default_conflict_method == "fill_defaults_and_annotations":
            _self = _fill_defaults_and_annotations(_self, _sig)
            _sig = _fill_defaults_and_annotations(_sig, _self)

        if not all(
            _self[name].default == _sig[name].default
            for name in _self.keys() & _sig.keys()
        ):
            # if default_conflict_method == 'take_first':
            #     _sig = _sig - set(_self.keys() & _sig.keys())
            # else:

            error_msg = (
                "During a signature merge, if two names are the same they must have the"
                f"**same default**:\n\t{_msg}\n"
                "Tip: If you're trying to merge functions in some way, consider "
                "decorating "
                "them with a signature mapping that avoids the argument name clashing."
                "You can also set ch_to_all_pk=True to ignore the kind of the "
                'parameters or change the default_conflict_method to "take_first",'
                "or another method that suits your needs."
            )

            raise IncompatibleSignatures(error_msg, sig1=_self, sig2=_sig)

        # assert all(
        #     _self[name].default == _sig[name].default
        #     for name in _self.keys() & _sig.keys()
        # ), (
        #     'During a signature merge, if two names are the same, they must have the '
        #     f'**same default**:\n\t{_msg}\n'
        #     "Tip: If you're trying to merge functions in some way, consider
        #     decorating "
        #     "them a signature mapping that "
        #     'avoids the argument name clashing'
        # )

        params = list(
            self._chain_params_of_signatures(
                _self.without_defaults,
                _sig.without_defaults,
                _self.with_defaults,
                _sig.with_defaults,
            )
        )
        params.sort(key=lambda p: p.kind)
        return self.__class__(params)

    def __add__(self, sig: ParamsAble):
        """Merge two signatures (casting all non-VAR kinds to POSITIONAL_OR_KEYWORD
        before hand)

        Important Notes:
        - The resulting Sig will loose it's return_annotation if it had one.
            This is to avoid making too many assumptions about how the sig sum will be
            used.
            If a return_annotation is needed (say, for composition, the last
            return_annotation
            summed), one can subclass Sig and overwrite __add__
        - POSITION_ONLY and KEYWORD_ONLY kinds will be replaced by
        POSITIONAL_OR_KEYWORD kind.
        This is to simplify the interface and code.
        If the user really wants to maintain those kinds, they can replace them back
        after the fact.

        >>> def f(w, /, x: float = 1, y=1, *, z: int = 1):
        ...     ...
        >>> def h(i, j, w):
        ...     ...  # has a 'w' argument, like f and g
        ...
        >>> def different(a, b: str, c=None):
        ...     ...  # No argument names in common with other functions

        >>> Sig(f) + Sig(different)
        <Sig (w, a, b: str, x: float = 1, y=1, z: int = 1, c=None)>
        >>> Sig(different) + Sig(f)
        <Sig (a, b: str, w, c=None, x: float = 1, y=1, z: int = 1)>

        The order of the first signature will take precedence over the second,
        but default-less arguments have to come before arguments with defaults.
         first, and Note the difference of the orders.

        >>> Sig(f) + Sig(h)
        <Sig (w, i, j, x: float = 1, y=1, z: int = 1)>
        >>> Sig(h) + Sig(f)
        <Sig (i, j, w, x: float = 1, y=1, z: int = 1)>

        The sum of two Sig's takes a safe-or-blow-up-now approach.
        If any of the arguments have different defaults or annotations, summing will
        raise an AssertionError.
        It's up to the user to decorate their input functions to express the default
        they actually desire.

        >>> def ff(w, /, x: float, y=1, *, z: int = 1):
        ...     ...  # just like f, but without the default for x
        >>> Sig(f) + Sig(ff)  # doctest: +IGNORE_EXCEPTION_DETAIL +ELLIPSIS
        Traceback (most recent call last):
        ...
        IncompatibleSignatures: During a signature merge, if two names are the same, they must
        have the **same default**
        ...

        >>> def hh(i, j, w=1):
        ...     ...  # like h, but w has a default
        ...
        >>> Sig(h) + Sig(hh)  # doctest: +IGNORE_EXCEPTION_DETAIL +ELLIPSIS
        Traceback (most recent call last):
        ...
        IncompatibleSignatures: During a signature merge, if two names are the same, they must
        have the **same default**
        ...

        >>> Sig(f) + [
        ...     "w",
        ...     ("y", 1),
        ...     ("d", 1.0, float),
        ...     dict(name="special", kind=Parameter.KEYWORD_ONLY, default=0),
        ... ]
        <Sig (w, x: float = 1, y=1, z: int = 1, d: float = 1.0, special=0)>

        """
        return self.merge_with_sig(sig, ch_to_all_pk=True)

    def __radd__(self, sig: ParamsAble):
        """Adding on the right.
        The raison d'être for this is so that you can start your summing with any
        signature speccifying
         object that Sig will be able to resolve into a signature. Like this:

        >>> ["first_arg", ("second_arg", 42)] + Sig(lambda x, y: x * y)
        <Sig (first_arg, x, y, second_arg=42)>

        Note that the ``second_arg`` doesn't actually end up being the second argument
        because
        it has a default and x and y don't. But if you did this:

        >>> ["first_arg", ("second_arg", 42)] + Sig(lambda x=0, y=1: x * y)
        <Sig (first_arg, second_arg=42, x=0, y=1)>

        you'd get what you expect.

        Of course, we could have just obliged you to say ``Sig(['first_arg',
        ('second_arg', 42)])``
        explicitly and spare ourselves yet another method.
        The reason we made ``__radd__`` is so we can make it handle 0 + Sig(...),
        so that you can
        merge an iterable of signatures like this:

        >>> def f(a, b, c):
        ...     ...
        ...
        >>> def g(c, b, e):
        ...     ...
        ...
        >>> sigs = map(Sig, [f, g])
        >>> sum(sigs)
        <Sig (a, b, c, e)>

        Let's say, for whatever reason (don't ask me), you wanted to make a function
        that contains all the
        arguments of all the functions of ``os.path`` (that don't contain any var arg
        kinds).

        >>> import os.path
        >>> funcs = list(
        ...     filter(
        ...         callable,
        ...         (
        ...             getattr(os.path, a)
        ...             for a in dir(os.path)
        ...             if not a.startswith("_")
        ...         ),
        ...     )
        ... )
        >>> sigs = filter(lambda sig: not sig.has_var_kinds, map(Sig, funcs))
        >>> # Note: Skipping because not stable between python versions
        >>> sum(sigs)  # doctest: +SKIP
        <Sig (path, p, paths, m, filename, s, f1, f2, fp1, fp2, s1, s2, start=None)>
        """
        if sig == 0:  # so that we can do ``sum(iterable_of_sigs)``
            sig = Sig([])
        else:
            sig = Sig(sig)
        return sig.merge_with_sig(self)

    def remove_names(self, names):
        names = {p.name for p in ensure_params(names)}
        new_params = {
            name: p for name, p in self.parameters.items() if name not in names
        }
        return self.__class__(new_params, return_annotation=self.return_annotation)

    def add_params(self, params: Iterable):
        """Creates a new instance of Sig after merging the parameters of this signature
        with a list of new parameters. The new list of parameters is automatically
        sorted based on signature constraints given by kinds and default values.
        See Python native signature documentation for more details.

        >>> s = Sig('(a, /, b, *, c)')
        >>> s.add_params([
        ...     Param('kwargs', VK),
        ...     dict(name='d', kind=KO),
        ...     Param('args', VP),
        ...     'e',
        ...     Param('f', PO),
        ... ])
        <Sig (a, f, /, b, e, *args, c, d, **kwargs)>
        """

        def comparator(param):
            return (param.kind, param.kind == KO or param.default is not empty)

        new_params = self.params + [ensure_param(p) for p in params]
        new_params = sorted(new_params, key=comparator)
        return type(self)(new_params)

    def __sub__(self, sig):
        return self.remove_names(sig)

    @staticmethod
    def _chain_params_of_signatures(*sigs):
        """Yields Parameter instances taken from sigs without repeating the same name
        twice.

        >>> str(list(
        ...     Sig._chain_params_of_signatures(
        ...         Sig(lambda x, *args, y=1: ...),
        ...         Sig(lambda x, y, z, **kwargs: ...),
        ...     )
        ...   )
        ... )
        '[<Parameter "x">, <Parameter "*args">, <Parameter "y=1">, <Parameter "z">, <Parameter "**kwargs">]'

        """
        already_merged_names = set()
        for s in sigs:
            for p in s.parameters.values():
                if p.name not in already_merged_names:
                    yield p
                already_merged_names.add(p.name)

    @property
    def without_defaults(self):
        """Sub-signature containing only "required" (i.e. without defaults) parameters.

        >>> list(Sig(lambda *args, a, b, x=1, y=1, **kwargs: ...).without_defaults)
        ['a', 'b']
        """
        return self.__class__(
            p for p in self.values() if not param_has_default_or_is_var_kind(p)
        )

    @property
    def with_defaults(self):
        """Sub-signature containing only "not required" (i.e. with defaults) parameters.

        >>> list(Sig(lambda *args, a, b, x=1, y=1, **kwargs: ...).with_defaults)
        ['args', 'x', 'y', 'kwargs']
        """
        return self.__class__(
            p for p in self.values() if param_has_default_or_is_var_kind(p)
        )

    def normalize_kind(
        self,
        kind=PK,
        except_kinds=var_param_kinds,
        add_defaults_if_necessary=False,
        argname_to_default=None,
        allow_reordering=False,
    ):
        except_kinds = except_kinds or set()
        if add_defaults_if_necessary:
            if argname_to_default is None:

                def argname_to_default(argname):
                    return None

        def changed_params():
            there_was_a_default = False
            for p in self.parameters.values():
                if p.kind not in except_kinds:
                    if add_defaults_if_necessary:
                        if there_was_a_default and p.default is _empty:
                            p = p.replace(kind=kind, default=argname_to_default(p.name))
                        there_was_a_default = p.default is not _empty
                    else:
                        p = p.replace(kind=kind)
                yield p

        params = list(changed_params())
        try:
            return type(self)(params, return_annotation=self.return_annotation)
        except ValueError as e:
            if allow_reordering:
                return self.sort_params()
            else:
                raise

    def map_arguments(
        self,
        args: tuple = None,
        kwargs: dict = None,
        *,
        apply_defaults=False,
        allow_partial=False,
        allow_excess=False,
        ignore_kind=False,
    ) -> dict:
        """Map arguments (args and kwargs) to the parameters of function's signature.

        When you need to manage how the arguments of a function are specified,
        you need to take care of
        multiple cases depending on whether they were specified as positional arguments
        (`args`) or keyword arguments (`kwargs`).

        The `map_arguments` (and it's sorta-inverse inverse,
        `mk_args_and_kwargs`)
        are there to help you manage this.

        If you could rely on the the fact that only `kwargs` were given it would
        reduce the complexity of your code.
        This is why we have the `all_pk_signature` function in `signatures.py`.

        We also need to have a means to make a `kwargs` only from the actual `(*args,
        **kwargs)` used at runtime.
        We have `Signature.bind` (and `bind_partial`) for that.

        But these methods will fail if there is extra stuff in the `kwargs`.
        Yet sometimes we'd like to have a `dict` that services several functions that
        will extract their needs from it.

        That's where  `Sig.map_arguments_from_variadics(*args, **kwargs)` is needed.
        :param args: The args the function will be called with.
        :param kwargs: The kwargs the function will be called with.
        :param apply_defaults: (bool) Whether to apply signature defaults to the
        non-specified argument names
        :param allow_partial: (bool) True iff you want to allow partial signature
        fulfillment.
        :param allow_excess: (bool) Set to True iff you want to allow extra kwargs
        items to be ignored.
        :param ignore_kind: (bool) Set to True iff you want to ignore the position and
        keyword only kinds,
            in order to be able to accept args and kwargs in such a way that there can
            be cross-over
            (args that are supposed to be keyword only, and kwargs that are supposed
            to be positional only)
        :return: An {param_name: arg_val, ...} dict

        See also the sorta-inverse of this function: mk_args_and_kwargs

        >>> def foo(w, /, x: float, y="YY", *, z: str = "ZZ"):
        ...     ...
        >>> sig = Sig(foo)
        >>> assert (
        ...     sig.map_arguments((11, 22, "you"), dict(z="zoo"))
        ...     == sig.map_arguments((11, 22), dict(y="you", z="zoo"))
        ...     == {"w": 11, "x": 22, "y": "you", "z": "zoo"}
        ... )

        By default, `apply_defaults=False`, which will lead to only get those
        arguments you input.

        >>> sig.map_arguments(args=(11,), kwargs={"x": 22})
        {'w': 11, 'x': 22}

        But if you specify `apply_defaults=True` non-specified non-require arguments
        will be returned with their defaults:

        >>> sig.map_arguments(
        ...     args=(11,), kwargs={"x": 22}, apply_defaults=True
        ... )
        {'w': 11, 'x': 22, 'y': 'YY', 'z': 'ZZ'}

        By default, `ignore_excess=False`, so specifying kwargs that are not in the
        signature will lead to an exception.

        >>> sig.map_arguments(
        ...     args=(11,), kwargs={"x": 22, "not_in_sig": -1}
        ... )
        Traceback (most recent call last):
            ...
        TypeError: got an unexpected keyword argument 'not_in_sig'

        Specifying `allow_excess=True` will ignore such excess fields of kwargs.
        This is useful when you want to source several functions from a same dict.

        >>> sig.map_arguments(
        ...     args=(11,), kwargs={"x": 22, "not_in_sig": -1}, allow_excess=True
        ... )
        {'w': 11, 'x': 22}

        On the other side of `ignore_excess` you have `allow_partial` that will allow
        you, if
        set to `True`, to underspecify the params of a function (in view of being
        completed later).

        >>> sig.map_arguments(args=(), kwargs={"x": 22})
        Traceback (most recent call last):
        ...
        TypeError: missing a required argument: 'w'

        But if you specify `allow_partial=True`...

        >>> sig.map_arguments(
        ...     args=(), kwargs={"x": 22}, allow_partial=True
        ... )
        {'x': 22}

        That's a lot of control (eight combinations total), but not everything is
        controllable here:
        Position only and keyword only kinds need to be respected:

        >>> sig.map_arguments(args=(1, 2, 3, 4), kwargs={})
        Traceback (most recent call last):
        ...
        TypeError: too many positional arguments
        >>> sig.map_arguments(args=(), kwargs=dict(w=1, x=2, y=3, z=4))
        Traceback (most recent call last):
        ...
        TypeError:...'w'...

        But if you want to ignore the kind of parameter, just say so:

        >>> sig.map_arguments(
        ...     args=(1, 2, 3, 4), kwargs={}, ignore_kind=True
        ... )
        {'w': 1, 'x': 2, 'y': 3, 'z': 4}
        >>> sig.map_arguments(
        ...     args=(), kwargs=dict(w=1, x=2, y=3, z=4), ignore_kind=True
        ... )
        {'w': 1, 'x': 2, 'y': 3, 'z': 4}
        """

        def get_var_dflts():
            if self.has_var_positional:
                yield self.var_positional_name, ()
            if self.has_var_keyword:
                yield self.var_keyword_name, {}

        _args = args or ()
        _kwargs = kwargs or {}

        if ignore_kind:
            var_dflts = dict(get_var_dflts())
            sig = self.normalize_kind(kind=KO, except_kinds=None)
            sig = sig.ch_defaults(**var_dflts)
            for arg, p in zip(_args, sig.params):
                if p.name in _kwargs:
                    raise TypeError(f"multiple values for argument '{p.name}'")
                _kwargs[p.name] = arg
            _args = ()
        else:
            sig = self

        if not sig.has_var_positional and allow_excess:
            max_allowed_num_of_posisional_args = sum(
                k <= PK for k in sig.kinds.values()
            )
            _args = _args[:max_allowed_num_of_posisional_args]
        if not sig.has_var_keyword and allow_excess:
            _kwargs = {k: v for k, v in _kwargs.items() if k in sig}

        binder = sig.bind_partial if allow_partial else sig.bind
        b = binder(*_args, **_kwargs)
        if apply_defaults:
            b.apply_defaults()

        return b.arguments

    kwargs_from_args_and_kwargs = deprecation_of(
        map_arguments, "kwargs_from_args_and_kwargs"
    )

    def mk_args_and_kwargs(
        self,
        arguments: dict,
        *,
        apply_defaults=False,
        allow_partial=False,
        allow_excess=False,
        ignore_kind=False,
        args_limit: int | None = 0,
    ) -> tuple[tuple, dict]:
        """Extract args and kwargs such that func(*args, **kwargs) can be called,
        where func has instance's signature.

        :param arguments: The {param_name: arg_val,...} dict to process
        :param args_limit: How "far" in the params should args (positional arguments)
            be searched for.
            - args_limit==0: Take the minimum number possible of args (positional
                arguments). Only those that are position only or before a var-positional.
            - args_limit is None: Take the maximum number of args (positional arguments).
                The only kwargs (keyword arguments) you should have are keyword-only
                and var-keyword arguments.
            - args_limit positive integer: Take the args_limit first argument names
                (of signature) as args, and the rest as kwargs.

        >>> def foo(w, /, x: float, y=1, *, z: int = 1):
        ...     return ((w + x) * y) ** z
        >>> foo_sig = Sig(foo)
        >>> args, kwargs = foo_sig.mk_args_and_kwargs(
        ...     dict(w=4, x=3, y=2, z=1)
        ... )
        >>> assert (args, kwargs) == ((4,), {"x": 3, "y": 2, "z": 1})
        >>> assert foo(*args, **kwargs) == foo(4, 3, 2, z=1) == 14

        What about variadics?

        >>> def bar(a, /, b, *args, c=2, **kwargs):
        ...     pass
        >>> Sig(bar).mk_args_and_kwargs(
        ...     dict(a=1, b=2, args=(3,4), c=5, kwargs=dict(d=6, e=7))
        ... )
        ((1, 2, 3, 4), {'c': 5, 'd': 6, 'e': 7})

        You can also give the arguments in a different order:

        >>> Sig(bar).mk_args_and_kwargs(
        ...     dict(args=(3,4), kwargs=dict(d=6, e=7), b=2, c=5, a=1)
        ... )
        ((1, 2, 3, 4), {'c': 5, 'd': 6, 'e': 7})

        The `args_limit` begs explanation.
        Consider the signature of `def foo(w, /, x: float, y=1, *, z: int = 1): ...`
        for instance. We could call the function with the following (args, kwargs) pairs:
        - ((1,), {'x': 2, 'y': 3, 'z': 4})
        - ((1, 2), {'y': 3, 'z': 4})
        - ((1, 2, 3), {'z': 4})
        The two other combinations (empty args or empty kwargs) are not valid
        because of the / and * constraints.

        But when asked for an (args, kwargs) pair, which of the three valid options
        should be returned? This is what the `args_limit` argument controls.

        If `args_limit == 0`, the least args (positional arguments) will be returned.
        It's the default.

        >>> arguments = dict(w=4, x=3, y=2, z=1)
        >>> foo_sig.mk_args_and_kwargs(arguments, args_limit=0)
        ((4,), {'x': 3, 'y': 2, 'z': 1})

        If `args_limit is None`, the least kwargs (keyword arguments) will be returned.

        >>> foo_sig.mk_args_and_kwargs(arguments, args_limit=None)
        ((4, 3, 2), {'z': 1})

        If `args_limit` is a positive integer, the first `[args_limit]` arguments
        will be returned (not checking at all if this is valid!).

        >>> foo_sig.mk_args_and_kwargs(arguments, args_limit=1)
        ((4,), {'x': 3, 'y': 2, 'z': 1})
        >>> foo_sig.mk_args_and_kwargs(arguments, args_limit=2)
        ((4, 3), {'y': 2, 'z': 1})
        >>> foo_sig.mk_args_and_kwargs(arguments, args_limit=3)
        ((4, 3, 2), {'z': 1})

        Note that if you specify `args_limit` to be greater than the maximum of
        positional arguments, it behaves as if `args_limit` was `None`:

        >>> foo_sig.mk_args_and_kwargs(arguments, args_limit=4)
        ((4, 3, 2), {'z': 1})

        Note that 'args_limit''s behavior is consistent with list behvior in the sense
        that:

        >>> args = (0, 1, 2, 3)
        >>> args[:0]
        ()
        >>> args[:None]
        (0, 1, 2, 3)
        >>> args[2]
        2

        If variable positional arguments are present, `args_limit` is ignored and
        all positional arguments are returned as args.

        >>> Sig(bar).mk_args_and_kwargs(
        ...     dict(a=1, b=2, args=(3,4), c=5, kwargs=dict(d=6, e=7)),
        ...     args_limit=1
        ... )
        ((1, 2, 3, 4), {'c': 5, 'd': 6, 'e': 7})

        By default, only the arguments that were given in the `arguments` input will be
        returned in the (args, kwargs) output.
        If you also want to get those that have defaults (according to signature),
        you need to specify it with the `apply_defaults=True` argument.

        >>> foo_sig.mk_args_and_kwargs(dict(w=4, x=3))
        ((4,), {'x': 3})
        >>> foo_sig.mk_args_and_kwargs(dict(w=4, x=3), apply_defaults=True)
        ((4,), {'x': 3, 'y': 1, 'z': 1})

        By default, all required arguments must be given.
        Not doing so will lead to a `TypeError`.
        If you want to process your arguments anyway, specify `allow_partial=True`.

        >>> foo_sig.mk_args_and_kwargs(dict(w=4))
        Traceback (most recent call last):
          ...
        TypeError: missing a required argument: 'x'
        >>> foo_sig.mk_args_and_kwargs(dict(w=4), allow_partial=True)
        ((4,), {})

        Specifying argument names that are not recognized by the signature will
        lead to a `TypeError`.
        If you want to avoid this (and just take from the input `kwargs` what ever you
        can), specify this with `allow_excess=True`.

        >>> foo_sig.mk_args_and_kwargs(dict(w=4, x=3, extra='stuff'))
        Traceback (most recent call last):
            ...
        TypeError: Got unexpected keyword arguments: extra
        >>> foo_sig.mk_args_and_kwargs(dict(w=4, x=3, extra='stuff'),
        ...     allow_excess=True)
        ((4,), {'x': 3})

        See `map_arguments` (namely for the description of the arguments).
        """
        arguments = arguments or {}
        extra_arguments = set(arguments) - set(self.names)
        if extra_arguments and not allow_excess:
            raise TypeError(
                f"Got unexpected keyword arguments: {', '.join(extra_arguments)}"
            )
        _arguments = {p: arguments[p] for p in self.names if p in arguments}
        vp_args = _arguments.get(self.var_positional_name, ())
        vk_args = _arguments.get(self.var_keyword_name, {})
        if vp_args:
            # If there are var positional arguments, we ignore the args_limit
            args_limit = None

        pos, pks, kos = (
            self.names_of_kind[PO],
            self.names_of_kind[PK],
            self.names_of_kind[KO],
        )
        names_for_args = pos
        names_for_kwargs = kos
        if args_limit is None:
            # All the PKs go to args, so we have:
            # names_for_args == POs + PKs
            # names_for_kwargs == KOs
            names_for_args += pks
        else:
            # Take the [args_limit] first arguments (of signature) as args. The minimum
            # number of args is the number of POs. The maximum number of args is the
            # number of POs + PKs. The rest are kwargs.
            nb_of_positional_pks = min(max(args_limit - len(pos), 0), len(pks))
            names_for_args += pks[:nb_of_positional_pks]
            names_for_kwargs = pks[nb_of_positional_pks:] + names_for_kwargs

        args = tuple(_arguments[name] for name in names_for_args if name in _arguments)
        kwargs = {
            name: _arguments[name] for name in names_for_kwargs if name in _arguments
        }

        # Note that, at this stage, the variadics arguments are not yet in the args and
        # kwargs variables.
        # We first call map_arguments with the args and kwargs with no variadics to
        # validate that all the explicit arguments are valid and there is no missing
        # required argument.

        # In fact, imagine the following:

        # >>> def foo(a, *args):
        # ...     ...
        # >>> foo_sig = Sig(foo)
        # >>> foo_sig.mk_args_and_kwargs(arguments=dict(args=(1,)))

        # This should fail because `a` is missing in the arguments.
        # But if we included the variadics in the args, the value '1' would have been
        # mapped to `a` by `map_arguments` and the error would not have been caught.
        # Same logic for kwargs.

        __arguments = self.map_arguments(
            args,
            kwargs,
            apply_defaults=apply_defaults,
            allow_partial=allow_partial,
            # allow_excess=allow_excess,
            # ignore_kind=ignore_kind,
        )

        # Let's retrieve the args and kwargs from the output of `map_arguments`, because
        # some extra stuff might have been added (defaults). And let's also add the
        # variadics.
        pos_arguments = {
            name: arg for name, arg in __arguments.items() if name in names_for_args
        }
        kw_arguments = {
            name: arg for name, arg in __arguments.items() if name in names_for_kwargs
        }

        if ignore_kind:
            # If ignore_kind is True, return all arguments as kwargs
            args = ()
            d_vp_args = (
                {self.var_positional_name: vp_args} if self.has_var_positional else {}
            )
            d_vk_args = {self.var_keyword_name: vk_args} if self.has_var_keyword else {}
            kwargs = {**pos_arguments, **d_vp_args, **kw_arguments, **d_vk_args}
        else:
            args = tuple(pos_arguments.values()) + vp_args
            kwargs = dict(kw_arguments, **vk_args)

        return args, kwargs

    args_and_kwargs_from_kwargs = deprecation_of(
        mk_args_and_kwargs, "args_and_kwargs_from_kwargs"
    )

    def map_arguments_from_variadics(
        self,
        *args,
        _apply_defaults=False,
        _allow_partial=False,
        _allow_excess=False,
        _ignore_kind=False,
        **kwargs,
    ):
        """Convenience method that calls map_arguments from variadics

        >>> def foo(w, /, x: float, y="YY", *, z: str = "ZZ"):
        ...     ...
        >>> sig = Sig(foo)
        >>> assert (
        ...     sig.map_arguments_from_variadics(1, 2, 3, z=4)
        ...     == sig.map_arguments_from_variadics(1, 2, y=3, z=4)
        ...     == {"w": 1, "x": 2, "y": 3, "z": 4}
        ... )

        What about var positional and var keywords?

        >>> def bar(*args, **kwargs):
        ...     ...
        ...
        >>> Sig(bar).map_arguments_from_variadics(1, 2, y=3, z=4)
        {'args': (1, 2), 'kwargs': {'y': 3, 'z': 4}}

        Note that though `w` is a position only argument, you can specify `w=11` as
        a keyword argument too, using `_ignore_kind=True`:

        >>> Sig(foo).map_arguments_from_variadics(w=11, x=22, _ignore_kind=True)
        {'w': 11, 'x': 22}

        You can use `_allow_partial` that will allow you, if
        set to `True`, to underspecify the params of a function
        (in view of being completed later).

        >>> Sig(foo).map_arguments_from_variadics(x=3, y=2)
        Traceback (most recent call last):
          ...
        TypeError: missing a required argument: 'w'

        But if you specify `_allow_partial=True`...

        >>> Sig(foo).map_arguments_from_variadics(x=3, y=2, _allow_partial=True)
        {'x': 3, 'y': 2}

        By default, `_apply_defaults=False`, which will lead to only get those arguments
        you input.

        >>> Sig(foo).map_arguments_from_variadics(4, x=3, y=2)
        {'w': 4, 'x': 3, 'y': 2}

        But if you specify `_apply_defaults=True` non-specified non-require arguments
        will be returned with their defaults:

        >>> Sig(foo).map_arguments_from_variadics(4, x=3, y=2, _apply_defaults=True)
        {'w': 4, 'x': 3, 'y': 2, 'z': 'ZZ'}
        """
        return self.map_arguments(
            args,
            kwargs,
            apply_defaults=_apply_defaults,
            allow_partial=_allow_partial,
            allow_excess=_allow_excess,
            ignore_kind=_ignore_kind,
        )

    extract_kwargs = deprecation_of(map_arguments_from_variadics, "extract_kwargs")

    def extract_args_and_kwargs(
        self,
        *args,
        _ignore_kind=True,
        _allow_partial=False,
        _allow_excess=True,
        _apply_defaults=False,
        _args_limit=0,
        **kwargs,
    ):
        """Source the (args, kwargs) for the signature instance, ignoring excess
        arguments.

        >>> def foo(w, /, x: float, y=2, *, z: int = 1):
        ...     return w + x * y ** z
        >>> args, kwargs = Sig(foo).extract_args_and_kwargs(4, x=3, y=2)
        >>> (args, kwargs) == ((4,), {"x": 3, "y": 2})
        True

        The difference with map_arguments_from_variadics is that here the output is
        ready to be called by the function whose signature we have, since the
        position-only arguments will be returned as args.

        >>> foo(*args, **kwargs)
        10

        Note that though `w` is a position only argument, you can specify `w=4` as a
        keyword argument too (by default):

        >>> args, kwargs = Sig(foo).extract_args_and_kwargs(w=4, x=3, y=2)
        >>> (args, kwargs) == ((4,), {"x": 3, "y": 2})
        True

        If you don't want to allow that, you can say `_ignore_kind=False`

        >>> Sig(foo).extract_args_and_kwargs(w=4, x=3, y=2, _ignore_kind=False)
        Traceback (most recent call last):
          ...
        TypeError:...'w'...

        You can use `_allow_partial` that will allow you, if
        set to `True`, to underspecify the params of a function (in view of being
        completed later).

        >>> Sig(foo).extract_args_and_kwargs(x=3, y=2)
        Traceback (most recent call last):
          ...
        TypeError:...'w'...

        But if you specify `_allow_partial=True`...

        >>> args, kwargs = Sig(foo).extract_args_and_kwargs(
        ...     x=3, y=2, _allow_partial=True
        ... )
        >>> (args, kwargs) == ((), {"x": 3, "y": 2})
        True

        By default, `_apply_defaults=False`, which will lead to only get those
        arguments you input.

        >>> args, kwargs = Sig(foo).extract_args_and_kwargs(4, x=3, y=2)
        >>> (args, kwargs) == ((4,), {"x": 3, "y": 2})
        True

        But if you specify `_apply_defaults=True` non-specified non-require arguments
        will be returned with their defaults:

        >>> args, kwargs = Sig(foo).extract_args_and_kwargs(
        ...     4, x=3, y=2, _apply_defaults=True
        ... )
        >>> (args, kwargs) == ((4,), {"x": 3, "y": 2, "z": 1})
        True
        """
        arguments = self.map_arguments(
            args,
            kwargs,
            apply_defaults=_apply_defaults,
            allow_partial=_allow_partial,
            allow_excess=_allow_excess,
            ignore_kind=_ignore_kind,
        )
        return self.mk_args_and_kwargs(
            arguments,
            allow_partial=_allow_partial,
            args_limit=_args_limit,
        )

    def source_arguments(
        self,
        *args,
        _apply_defaults=False,
        _allow_partial=False,
        _ignore_kind=True,
        **kwargs,
    ):
        """Source the arguments for the signature instance, ignoring excess arguments.

        >>> def foo(w, /, x: float, y="YY", *, z: str = "ZZ"):
        ...     ...
        >>> Sig(foo).source_arguments(11, x=22, extra="keywords", are="ignored")
        {'w': 11, 'x': 22}

        Note that though `w` is a position only argument, you can specify `w=11` as a
        keyword argument too (by default):

        >>> Sig(foo).source_arguments(w=11, x=22, extra="keywords", are="ignored")
        {'w': 11, 'x': 22}

        If you don't want to allow that, you can say `_ignore_kind=False`

        >>> Sig(foo).source_arguments(
        ...     w=11, x=22, extra="keywords", are="ignored", _ignore_kind=False
        ... )
        Traceback (most recent call last):
          ...
        TypeError: ...'w'...

        You can use `_allow_partial` that will allow you, if
        set to `True`, to underspecify the params of a function (in view of being
        completed later).

        >>> Sig(foo).source_arguments(x=3, y=2, extra="keywords", are="ignored")
        Traceback (most recent call last):
          ...
        TypeError: ...'w'...

        But if you specify `_allow_partial=True`...

        >>> Sig(foo).source_arguments(
        ...     x=3, y=2, extra="keywords", are="ignored", _allow_partial=True
        ... )
        {'x': 3, 'y': 2}

        By default, `_apply_defaults=False`, which will lead to only get those
        arguments you input.

        >>> Sig(foo).source_arguments(4, x=3, y=2, extra="keywords", are="ignored")
        {'w': 4, 'x': 3, 'y': 2}

        But if you specify `_apply_defaults=True` non-specified non-require arguments
        will be returned with their defaults:

        >>> Sig(foo).source_arguments(
        ...     4, x=3, y=2, extra="keywords", are="ignored", _apply_defaults=True
        ... )
        {'w': 4, 'x': 3, 'y': 2, 'z': 'ZZ'}


        """
        return self.map_arguments(
            args,
            kwargs,
            apply_defaults=_apply_defaults,
            allow_partial=_allow_partial,
            allow_excess=True,
            ignore_kind=_ignore_kind,
        )

    source_kwargs = deprecation_of(source_arguments, "source_kwargs")

    def source_args_and_kwargs(
        self,
        *args,
        _ignore_kind=True,
        _allow_partial=False,
        _apply_defaults=False,
        _args_limit=0,
        **kwargs,
    ):
        """Source the (args, kwargs) for the signature instance, ignoring excess
        arguments.

        >>> def foo(w, /, x: float, y=2, *, z: int = 1):
        ...     return w + x * y ** z
        >>> args, kwargs = Sig(foo).source_args_and_kwargs(
        ...     4, x=3, y=2, extra="keywords", are="ignored"
        ... )
        >>> args, kwargs
        ((4,), {'x': 3, 'y': 2})

        The difference with source_arguments is that here the output is ready to be
        called by the
        function whose signature we have, since the position-only arguments will be
        returned as
        args.

        >>> foo(*args, **kwargs)
        10

        Note that though `w` is a position only argument, you can specify `w=4` as a
        keyword argument too (by default):

        >>> args, kwargs = Sig(foo).source_args_and_kwargs(
        ...     w=4, x=3, y=2, extra="keywords", are="ignored"
        ... )
        >>> assert (args, kwargs) == ((4,), {"x": 3, "y": 2})

        If you don't want to allow that, you can say `_ignore_kind=False`

        >>> Sig(foo).source_args_and_kwargs(
        ...     w=4, x=3, y=2, extra="keywords", are="ignored", _ignore_kind=False
        ... )
        Traceback (most recent call last):
          ...
        TypeError: ...'w'...

        You can use `_allow_partial` that will allow you, if
        set to `True`, to underspecify the params of a function (in view of being
        completed later).

        >>> Sig(foo).source_args_and_kwargs(x=3, y=2, extra="keywords", are="ignored")
        Traceback (most recent call last):
          ...
        TypeError:...'w'...

        But if you specify `_allow_partial=True`...

        >>> args, kwargs = Sig(foo).source_args_and_kwargs(
        ...     x=3, y=2, extra="keywords", are="ignored", _allow_partial=True
        ... )
        >>> (args, kwargs) == ((), {"x": 3, "y": 2})
        True

        By default, `_apply_defaults=False`, which will lead to only get those
        arguments you input.

        >>> args, kwargs = Sig(foo).source_args_and_kwargs(
        ...     4, x=3, y=2, extra="keywords", are="ignored"
        ... )
        >>> (args, kwargs) == ((4,), {"x": 3, "y": 2})
        True

        But if you specify `_apply_defaults=True` non-specified non-require arguments
        will be returned with their defaults:

        >>> args, kwargs = Sig(foo).source_args_and_kwargs(
        ...     4, x=3, y=2, extra="keywords", are="ignored", _apply_defaults=True
        ... )
        >>> (args, kwargs) == ((4,), {"x": 3, "y": 2, "z": 1})
        True
        """
        arguments = self.source_arguments(
            *args,
            _apply_defaults=_apply_defaults,
            _allow_partial=_allow_partial,
            _ignore_kind=_ignore_kind,
            **kwargs,
        )
        return self.mk_args_and_kwargs(
            arguments,
            allow_partial=_allow_partial,
            args_limit=_args_limit,
        )

    @property
    def inject_into_keyword_variadic(self):
        """
        Decorator that uses signature to source the keyword variadic of target function.

        See replace_kwargs_using function for more details, including examples.

        >>> def apple(a, x: int, y=2, *, z=3, **extra_apple_options):
        ...     return a + x + y + z
        >>> @Sig(apple).inject_into_keyword_variadic
        ... def sauce(a, b, c, **sauce_kwargs):
        ...     return b * c + apple(a, **sauce_kwargs)

        The function will works:

        >>> sauce(1, 2, 3, x=4, z=5)  # func still works? Should be: 1 + 4 + 2 + 5 + 2 * 3
        18

        But the signature now doesn't have the `**sauce_kwargs`, but more informative
        signature elements sourced from `apple`:

        >>> Sig(sauce)
        <Sig (a, b, c, *, x: int, y=2, z=3, **extra_apple_options)>

        """
        return replace_kwargs_using(self)


def _fill_defaults_and_annotations(sig1: Sig, sig2: Sig):
    """Return the same signature as ``sig1``, but where empty param properties
    (default or annotation) were filled by the property found in ``sig2`` if it has a
    param of the same name

    >>> _fill_defaults_and_annotations(
    ...    Sig('(a, /, b: str, *, c=3)'), Sig('(a: float, b: int = 2, c=300)')
    ... )
    <Sig (a: float, /, b: str = 2, *, c=3)>

    """

    def filled_properties_of_sig1():
        alt_defaults = sig2.defaults
        alt_annotations = sig2.annotations
        for p in sig1.params:
            yield Parameter(
                p.name,
                p.kind,
                default=(
                    p.default
                    if p.default is not empty
                    else alt_defaults.get(p.name, empty)
                ),
                annotation=(
                    p.annotation
                    if p.annotation is not empty
                    else alt_annotations.get(p.name, empty)
                ),
            )

    return Sig(filled_properties_of_sig1())


def _validate_sanity_of_signature_change(
    func: Callable, new_sig: Sig, ignore_incompatible_signatures: bool = True
):
    func_pos, func_kw = Sig(func)._positional_and_keyword_defaults()
    self_pos, self_kw = new_sig._positional_and_keyword_defaults()
    # print(func_pos, func_kw )
    # print(self_pos, self_kw)

    pos_default_switching_to_kw = set(func_pos) & set(self_kw)
    kw_default_switching_to_pos = set(func_kw) & set(self_pos)

    # print(pos_default_switching_to_kw, kw_default_switching_to_pos)

    if not ignore_incompatible_signatures and (
        pos_default_switching_to_kw or kw_default_switching_to_pos
    ):
        raise IncompatibleSignatures(
            f"Changing both the kind and the default of a param will result to "
            f"unexpected behaviors if the function is not properly wrapped to do so."
            f"If you really want to do this, inject signature using the "
            f"`ignore_incompatible_signatures=True`"
            f"argument in `Sig.wrap(...)`. "
            f"Alternatively, you can use `i2.wrapper` tools to have more control "
            f"over function defaults and signatures."
            f"The function you were wrapping had signature: "
            f"{name_of_obj(func) or ''}{Sig(func)} and "
            f"the signature you wanted to inject was {new_sig.name or ''}{new_sig}",
            sig1=Sig(func),
            sig2=new_sig,
        )


########################################################################################
# Utils


def _signature_differences_str_for_error_msg(sig1, sig2):

    from pprint import pformat

    sig_diff = sig1.pair_with(sig2)

    sig1_name = f"{sig1.name}" if sig1.name else "sig1"
    sig2_name = f"{sig2.name}" if sig2.name else "sig2"

    return (
        "FYI: Here are the raw signature differences for {sig1_name} and {sig2_name} "
        f"(not all need to necessarily be resolved):\n{pformat(sig_diff)}"
    )


########################################################################################
# Recipes


def mk_sig_from_args(*args_without_default, **args_with_defaults):
    """Make a Signature instance by specifying args_without_default and
    args_with_defaults.

    >>> mk_sig_from_args("a", "b", c=1, d="bar")
    <Signature (a, b, c=1, d='bar')>
    """
    assert all(
        isinstance(x, str) for x in args_without_default
    ), "all default-less arguments must be strings"
    return Sig.from_objs(
        *args_without_default, **args_with_defaults
    ).to_simple_signature()


def _remove_variadics_from_sig(sig, ch_variadic_keyword_to_keyword=True):
    """Remove variadics from signature
    >>> def foo(a, *args, bar, **kwargs):
    ...     return f"{a=}, {args=}, {bar=}, {kwargs=}"
    >>> sig = Sig(foo)
    >>> assert str(sig) == '(a, *args, bar, **kwargs)'
    >>> new_sig = _remove_variadics_from_sig(sig)
    >>> str(new_sig)=='(a, args=(), *, bar, kwargs={})'
    True

    Note that if there is not variadic positional arguments, the variadic keyword
    will still be a keyword-only kind.

    >>> def func(a, bar=None, **kwargs):
    ...     return f"{a=}, {bar=}, {kwargs=}"
    >>> nsig = _remove_variadics_from_sig(Sig(func))
    >>> assert str(nsig)=='(a, bar=None, *, kwargs={})'

    If the function has neither variadic kinds, it will remain untouched.

    >>> def func(a, /, b, *, c=3):
    ...     return a + b + c
    >>> sig = _remove_variadics_from_sig(Sig(func))

    >>> assert sig == Sig(func)


    If you only want the variadic positional to be handled, but leave leave any
    VARIADIC_KEYWORD kinds (**kwargs) alone, you can do so by setting
    `ch_variadic_keyword_to_keyword=False`.

    >>> def foo(a, *args, bar=None, **kwargs):
    ...     return f"{a=}, {args=}, {bar=}, {kwargs=}"
    >>> assert str(Sig(_remove_variadics_from_sig(Sig(foo))))=='(a, args=(), *, bar=None, kwargs={})'
    """

    idx_of_vp = sig.index_of_var_positional
    var_keyword_argname = sig.var_keyword_name
    result_sig = sig
    if idx_of_vp is not None or var_keyword_argname is not None:
        params = sig.params
        if var_keyword_argname:  # if there's a VAR_KEYWORD argument
            if ch_variadic_keyword_to_keyword:
                i = sig.index_of_var_keyword
                # TODO: Reflect on pros/cons of having mutable {} default here:
                params[i] = params[i].replace(kind=Parameter.KEYWORD_ONLY, default={})

        try:  # TODO: Avoid this try catch. Look in advance for default ordering?
            if idx_of_vp is not None:
                params[idx_of_vp] = params[idx_of_vp].replace(kind=PK, default=())
            result_sig = Signature(params, return_annotation=sig.return_annotation)
        except ValueError:
            if idx_of_vp is not None:
                params[idx_of_vp] = params[idx_of_vp].replace(kind=PK)
            result_sig = Signature(params, return_annotation=sig.return_annotation)

    return result_sig


# TODO: Might want to make func be a positional-only argument, because if kwargs
#  contains a func key, we have a problem. But call_forgivingly is used broadly,
#  so must first test all dependents before making this change.
def call_forgivingly(func, *args, **kwargs):
    """
    Call function on given args and kwargs, but only taking what the function needs
    (not choking if they're extras variables)

    Tip: If you into trouble because your kwargs has a 'func' key,
    (which would then clash with the ``func`` param of call_forgivingly), then
    use `_call_forgivingly` instead, specifying args and kwargs as tuple and
    dict.

    >>> def foo(a, b: int = 0, c=None) -> int:
    ...     return "foo", (a, b, c)
    >>> call_forgivingly(
    ...     foo,  # the function you want to call
    ...     "input for a",  # meant for a -- the first (and only) argument foo requires
    ...     c=42,  # skiping b and giving c a non-default value
    ...     intruder="argument",  # but wait, this argument name doesn't exist! Oh no!
    ... )  # well, as it happens, nothing bad -- the intruder argument is just ignored
    ('foo', ('input for a', 0, 42))

    An example of what happens when variadic kinds are involved:

    >>> def bar(x, *args1, y=1, **kwargs1):
    ...     return x, args1, y, kwargs1
    >>> call_forgivingly(bar, 1, 2, 3, y=4, z=5)
    (1, (2, 3), 4, {'z': 5})

    # >>> def bar(x, y=1, **kwargs1):
    # ...     return x, y, kwargs1
    # >>> call_forgivingly(bar, 1, 2, 3, y=4, z=5)
    # (1, 4, {'z': 5})

    # >>> call_forgivingly(bar, 1, 2, 3, y=4, z=5)

    # >>> def bar(x, *args1, y=1):
    # ...     return x, args1, y
    # >>> call_forgivingly(bar, 1, 2, 3, y=4, z=5)
    # (1, (2, 3), {'z': 5})

    """
    return _call_forgivingly(func, args, kwargs)


# TODO: See if there's a more elegant way to do this
def _call_forgivingly(func, args, kwargs):
    """
    Helper for _call_forgivingly.
    """

    sig = Sig(func)
    arguments = sig.map_arguments(args, kwargs, allow_excess=True)
    _args, _kwargs = sig.mk_args_and_kwargs(arguments, args_limit=len(args))
    return func(*_args, **_kwargs)

    # sig = Sig(func)
    # variadic_kinds = {
    #     name: kind for name, kind in sig.kinds.items() if kind in [VP, VK]
    # }
    # if VP in variadic_kinds.values() and VK in variadic_kinds.values():
    #     _args = args
    #     _kwargs = kwargs
    # else:
    #     new_sig = sig - variadic_kinds.keys()
    #     _args, _kwargs = new_sig.source_args_and_kwargs(*args, _ignore_kind=False, **kwargs)
    #     for k, v in _kwargs.items():
    #         if k not in kwargs:
    #             _args = _args + (v,)
    #     _kwargs = {k: v for k, v in _kwargs.items() if k in kwargs}
    #     if VP in variadic_kinds.values():
    #         _args = args
    #     elif VK in variadic_kinds.values():
    #         _kwargs = dict(_kwargs, **kwargs)
    # return func(*_args, **_kwargs)


def call_somewhat_forgivingly(
    func, args, kwargs, enforce_sig: SignatureAble | None = None
):
    """Call function on given args and kwargs, but with controllable argument leniency.
    By default, the function will only pick from args and kwargs what matches it's
    signature, ignoring anything else in args and kwargs.

    But the real use of `call_somewhat_forgivingly` kicks in when you specify a
    `enforce_sig`: A signature (or any object that can be resolved into a signature
    through `Sig(enforce_sig)`) that will be used to bind the inputs, thus validating
    them against the `enforce_sig` signature (including extra arguments, defaults,
    etc.).

    `call_somewhat_forgivingly` helps you do this kind of thing systematically.

    >>> f = lambda a: a * 11
    >>> assert call_somewhat_forgivingly(f, (2,), {}) == f(2)

    In the above, we have no `enforce_sig`. The real use of call_somewhat_forgivingly
    is when we ask it to enforce a signature. Let's do this by specifying a function
    (no need for it to do anything: Only the signature is used.

    >>> g = lambda a, b=None: ...

    Calling `f` on it's normal set of inputs (one input in this case) gives you the
    same thing as `f`:

    >>> assert call_somewhat_forgivingly(f, (2,), {}, enforce_sig=g) == f(2)
    >>> assert call_somewhat_forgivingly(f, (), {'a': 2}, enforce_sig=g) == f(2)

    If you call with an extra positional argument, it will just be ignored.

    >>> assert call_somewhat_forgivingly(f, (2, 'ignored'), {}, enforce_sig=g) == f(2)

    If you call with a `b` keyword-argument (which matches `g`'s signature,
    it will also be ignored.

    >>> assert call_somewhat_forgivingly(
    ... f, (2,), {'b': 'ignored'}, enforce_sig=g
    ... ) == f(2)
    >>> assert call_somewhat_forgivingly(
    ...     f, (), {'a': 2, 'b': 'ignored'}, enforce_sig=g
    ... ) == f(2)

    But if you call with three positional arguments (one more than g allows),
    or call with a keyword argument that is not in `g`'s signature, it will
    raise a `TypeError`:

    >>> call_somewhat_forgivingly(f,
    ...     (2, 'ignored', 'does_not_fit_g_signature_anymore'), {}, enforce_sig=g
    ... )
    Traceback (most recent call last):
        ...
    TypeError: too many positional arguments
    >>> call_somewhat_forgivingly(f,
    ...     (2,), {'this_argname': 'is not in g'}, enforce_sig=g
    ... )
    Traceback (most recent call last):
        ...
    TypeError: got an unexpected keyword argument 'this_argname'

    """
    enforce_sig = Sig(enforce_sig or func)
    # Validate that args and kwargs are compatible with enforce_sig
    enforce_sig.bind(*args, **kwargs)
    return _call_forgivingly(func, args, kwargs)


def convert_to_PK(kinds):
    return {name: PK for name in kinds}


def kind_forgiving_func(func, kinds_modifier=convert_to_PK):
    """Wraps the func, changing the argument kinds according to kinds_modifier.
    The default behaviour is to change all kinds to POSITIONAL_OR_KEYWORD kinds.
    The original purpose of this function is to remove argument-kind restriction
    annoyances when doing functional manipulations such as:

    >>> from functools import partial
    >>> isinstance_of_str = partial(isinstance, class_or_tuple=str)
    >>> isinstance_of_str('I am a string')
    Traceback (most recent call last):
      ...
    TypeError: isinstance() takes no keyword arguments

    Here, instead, we can just get a kinder version of the function and do what we
    want to do:

    >>> _isinstance = kind_forgiving_func(isinstance)
    >>> isinstance_of_str = partial(_isinstance, class_or_tuple=str)
    >>> isinstance_of_str('I am a string')
    True
    >>> isinstance_of_str(42)
    False

    See also: ``i2.signatures.all_pk_signature``

    """
    sig = Sig(func)
    kinds_modif = kinds_modifier(sig.kinds)
    _sig = sig.ch_kinds(**kinds_modif)

    @_sig
    @wraps(func)
    def _func(*args, **kwargs):
        _args, _kwargs = sig.extract_args_and_kwargs(
            *args, _allow_excess=False, **kwargs
        )
        return func(*_args, **_kwargs)

    # _func.__signature__ = sig
    return _func


# TODO: Should we protect from misuse with signature compatibility check?
def use_interface(interface_sig):
    """Use interface_sig as (enforced/validated) signature of the decorated function.
    That is, the decorated function will use the original function has the backend,
    the function actually doing the work, but with a frontend specified
    (in looks and in argument validation) `interface_sig`

    consider the situation where are functionality is parametrized by a
    function `g` taking two inputs, `a`, and `b`.
    Now you want to carry out this functionality using a function `f` that does what
    `g` should do, but doesn't use `a`, and doesn't even have it in it's arguments.

    The solution to this is to _adapt_ `f` to the `g` interface:
    ```
    def my_g(a, b):
        return f(a)
    ```
    and use `my_g`.

    >>> f = lambda a: a * 11
    >>> interface = lambda a, b=None: ...
    >>>
    >>> new_f = use_interface(interface)(f)

    See how only the first argument, or `a` keyword argument, is taken into account
    in `new_f`:

    >>> assert new_f(2) == f(2)
    >>> assert new_f(2, 3) == f(2)
    >>> assert new_f(2, b=3) == f(2)
    >>> assert new_f(b=3, a=2) == f(2)

    But if we add more positional arguments than `interface` allows,
    or any keyword arguments that `interface` doesn't recognize...

    >>> new_f(1,2,3)
    Traceback (most recent call last):
      ...
    TypeError: too many positional arguments
    >>> new_f(1, c=2)
    Traceback (most recent call last):
      ...
    TypeError: got an unexpected keyword argument 'c'
    """
    interface_sig = Sig(interface_sig)

    def interface_wrapped_decorator(func):
        @interface_sig
        def _func(*args, **kwargs):
            return call_somewhat_forgivingly(
                func, args, kwargs, enforce_sig=interface_sig
            )

        return _func

    return interface_wrapped_decorator


import inspect


def has_signature(obj, robust=False):
    """Check if an object has a signature -- i.e. is callable and inspect.signature(
    obj) returns something.

    This can be used to more easily get signatures in bulk without having to write
    try/catches:

    >>> from functools import partial
    >>> len(
    ...     list(
    ...         filter(
    ...             None,
    ...             map(
    ...                 partial(has_signature, robust=False),
    ...                 (Sig, print, map, filter, Sig.wrap),
    ...             ),
    ...         )
    ...     )
    ... )  # doctest: +SKIP
    2

    If robust is set to True, `has_signature` will use `Sig` to get the signature,
    so will return True in most cases.

    """
    if robust:
        return bool(Sig.sig_or_none(obj))
    else:
        try:
            return bool((callable(obj) or None) and signature(obj))
        except ValueError:
            return False


# TODO: Need to define and use this function more carefully.
#   Is the goal to remove positional? Remove variadics? Normalize the signature?
def all_pk_signature(callable_or_signature: Callable | Signature):
    """Changes all (non-variadic) arguments to be of the PK (POSITION_OR_KEYWORD) kind.

    Wrapping a function with the resulting signature doesn't make that function callable
    with PK kinds in itself.
    It just gives it a signature without position and keyword ONLY kinds.
    It should be used to wrap such a function that actually carries out the
    implementation though!

    >>> def foo(w, /, x: float, y=1, *, z: int = 1, **kwargs):
    ...     ...
    >>> def bar(*args, **kwargs):
    ...     ...
    ...
    >>> from inspect import signature
    >>> new_foo = all_pk_signature(foo)
    >>> Sig(new_foo)
    <Sig (w, x: float, y=1, z: int = 1, **kwargs)>
    >>> all_pk_signature(signature(foo))
    <Sig (w, x: float, y=1, z: int = 1, **kwargs)>

    But note that the variadic arguments *args and **kwargs remain variadic:

    >>> all_pk_signature(signature(bar))
    <Signature (*args, **kwargs)>

    It works with `Sig` too (since Sig is a Signature), and maintains it's other
    attributes (like name).

    >>> sig = all_pk_signature(Sig(bar))
    >>> sig
    <Sig (*args, **kwargs)>
    >>> sig.name
    'bar'

    See also: ``i2.signatures.kind_forgiving_func``

    """

    if isinstance(callable_or_signature, Signature):
        sig = callable_or_signature

        def changed_params():
            for p in sig.parameters.values():
                if p.kind not in var_param_kinds:
                    yield p.replace(kind=PK)
                else:
                    yield p

        new_sig = type(sig)(
            list(changed_params()), return_annotation=sig.return_annotation
        )
        for attrname, attrval in getattr(sig, "__dict__", {}).items():
            setattr(new_sig, attrname, attrval)
        return new_sig
    elif isinstance(callable_or_signature, Callable):
        func = callable_or_signature
        sig = all_pk_signature(Sig(func))
        return sig(func)


# Changed ch_signature_to_all_pk to all_pk_signature because ch_signature_to_all_pk
# was misleading: It doesn't change anything at all, it returns a constructed signature.
# It doesn't change all kinds to PK -- just the non-variadic ones.
ch_signature_to_all_pk = all_pk_signature  # alias for back-compatibility


def normalized_func(func):
    sig = Sig(func)

    def argument_values_tuple(args, kwargs):
        b = sig.bind(*args, **kwargs)
        arg_vals = dict(b.arguments)

        poa, pka, vpa, koa, vka = [], [], (), {}, {}

        for name, val in arg_vals.items():
            kind = sig.kinds[name]
            if kind == PO:
                poa.append(val)
            elif kind == PK:
                pka.append(val)
            elif kind == VP:
                vpa = val  # there can only be one VP!
            elif kind == KO:
                koa.update({name: val})
            elif kind == VK:
                vka = val  # there can only be one VK!
        return poa, pka, vpa, koa, vka

    def _args_and_kwargs(args, kwargs):
        poa, pka, vpa, koa, vka = argument_values_tuple(args, kwargs)

        _args = (*poa, *pka, *vpa)
        _kwargs = {**koa, **vka}

        return _args, _kwargs

    # @sig.modified(**{name: {'kind': PK} for name in sig.names})
    def _func(*args, **kwargs):
        # poa, pka, vpa, koa, vka = argument_values_tuple(args, kwargs)
        # print(poa, pka, vpa, koa, vka)
        _args, _kwargs = _args_and_kwargs(args, kwargs)
        return func(*_args, **_kwargs)

    return _func


def ch_variadics_to_non_variadic_kind(func, *, ch_variadic_keyword_to_keyword=True):
    """A decorator that will change a VAR_POSITIONAL (*args) argument to a tuple (args)
    argument of the same name.

    Essentially, given a `func(a, *b, c, **d)` function want to get a
    `new_func(a, b=(), c=None, d={})` that has the same functionality
    (in fact, calls the original `func` function behind the scenes), but without
    where the variadic arguments *b and **d are replaced with a `b` expecting an
    iterable (e.g. tuple/list) and `d` expecting a `dict` to contain the
    desired inputs.

    Besides this, the decorator tries to be as conservative as possible, making only
    the minimum changes needed to meet the goal of getting to a variadic-less
    interface. When it doubt, and error will be raised.

    >>> def foo(a, *args, bar, **kwargs):
    ...     return f"{a=}, {args=}, {bar=}, {kwargs=}"
    >>> assert str(Sig(foo)) == '(a, *args, bar, **kwargs)'
    >>> wfoo = ch_variadics_to_non_variadic_kind(foo)
    >>> str(Sig(wfoo))
    '(a, args=(), *, bar, kwargs={})'

    And now to do this:

    >>> foo(1, 2, 3, bar=4, hello="world")
    "a=1, args=(2, 3), bar=4, kwargs={'hello': 'world'}"

    We can do it like this instead:

    >>> wfoo(1, (2, 3), bar=4, kwargs=dict(hello="world"))
    "a=1, args=(2, 3), bar=4, kwargs={'hello': 'world'}"

    Note, the outputs are the same. It's just the way we call our function that has
    changed.

    >>> assert wfoo(1, (2, 3), bar=4, kwargs=dict(hello="world")
    ... ) == foo(1, 2, 3, bar=4, hello="world")
    >>> assert wfoo(1, (2, 3), bar=4) == foo(1, 2, 3, bar=4)
    >>> assert wfoo(1, (), bar=4) == foo(1, bar=4)

    Note that if there is not variadic positional arguments, the variadic keyword
    will still be a keyword-only kind.

    >>> @ch_variadics_to_non_variadic_kind
    ... def func(a, bar=None, **kwargs):
    ...     return f"{a=}, {bar=}, {kwargs=}"
    >>> str(Sig(func))
    '(a, bar=None, *, kwargs={})'
    >>> assert func(1, bar=4, kwargs=dict(hello="world")
    ...     ) == "a=1, bar=4, kwargs={'hello': 'world'}"

    If the function has neither variadic kinds, it will remain untouched.

    >>> def func(a, /, b, *, c=3):
    ...     return a + b + c
    >>> ch_variadics_to_non_variadic_kind(func) == func
    True

    If you only want the variadic positional to be handled, but leave leave any
    VARIADIC_KEYWORD kinds (**kwargs) alone, you can do so by setting
    `ch_variadic_keyword_to_keyword=False`.
    If you'll need to use `ch_variadics_to_non_variadic_kind` in such a way
    repeatedly, we suggest you use `functools.partial` to not have to specify this
    configuration repeatedly.

    >>> from functools import partial
    >>> tuple_the_args = partial(ch_variadics_to_non_variadic_kind,
    ...     ch_variadic_keyword_to_keyword=False
    ... )
    >>> @tuple_the_args
    ... def foo(a, *args, bar=None, **kwargs):
    ...     return f"{a=}, {args=}, {bar=}, {kwargs=}"
    >>> Sig(foo)
    <Sig (a, args=(), *, bar=None, **kwargs)>
    >>> foo(1, (2, 3), bar=4, hello="world")
    "a=1, args=(2, 3), bar=4, kwargs={'hello': 'world'}"




    """
    if func is None:
        return partial(
            ch_variadics_to_non_variadic_kind,
            ch_variadic_keyword_to_keyword=ch_variadic_keyword_to_keyword,
        )
    sig = Sig(func)
    idx_of_vp = sig.index_of_var_positional
    var_keyword_argname = sig.var_keyword_name

    if idx_of_vp is not None or var_keyword_argname is not None:
        # If the function has any variadic (position or keyword)...

        @wraps(func)
        def variadic_less_func(*args, **kwargs):
            # extract from kwargs those inputs that need to be expressed positionally
            if ch_variadic_keyword_to_keyword:
                arguments = kwargs
            else:
                arguments = {k: v for k, v in kwargs.items() if k in sig}
                if sig.has_var_keyword:
                    arguments[sig.var_keyword_name] = {
                        k: v for k, v in kwargs.items() if k not in sig
                    }
            _args, _kwargs = sig.mk_args_and_kwargs(arguments, allow_partial=True)
            # print('COUCOU', kwargs, arguments)
            # add these to the existing args
            args = args + _args

            if idx_of_vp is not None:
                # separate the args that are positional, variadic, and after variadic
                a, _vp_args_, args_after_vp = (
                    args[:idx_of_vp],
                    args[idx_of_vp],
                    args[idx_of_vp + 1 :],
                )
                if args_after_vp:
                    raise FuncCallNotMatchingSignature(
                        "There should be only keyword arguments after the Variadic "
                        "args. "
                        f"Function was called with (positional={args}, keywords="
                        f"{_kwargs})"
                    )
            else:
                a, _vp_args_ = args, ()

            # extract from the remaining _kwargs, the dict corresponding to the
            # variadic keywords, if any, since these need to be **-ed later
            _var_keyword_kwargs = _kwargs.pop(var_keyword_argname, {})

            if ch_variadic_keyword_to_keyword:
                # an extra level of extraction is needed in this case
                # _var_keyword_kwargs = _var_keyword_kwargs.pop(var_keyword_argname, {})
                return func(*a, *_vp_args_, **_kwargs, **_var_keyword_kwargs)
            else:
                # call the original function with the unravelled args
                return func(*a, *_vp_args_, **_kwargs, **_var_keyword_kwargs)

        params = sig.params

        if var_keyword_argname:  # if there's a VAR_KEYWORD argument
            if ch_variadic_keyword_to_keyword:
                i = sig.index_of_var_keyword
                # TODO: Reflect on pros/cons of having mutable {} default here:
                params[i] = params[i].replace(kind=Parameter.KEYWORD_ONLY, default={})

        try:  # TODO: Avoid this try catch. Look in advance for default ordering?
            if idx_of_vp is not None:
                params[idx_of_vp] = params[idx_of_vp].replace(kind=PK, default=())
            variadic_less_func.__signature__ = Sig(
                # Note: Changed signature(func) to Sig(func) but don't know if the first
                #  was on purpose.
                params,
                return_annotation=Sig(func).return_annotation,
            )
        except ValueError:
            if idx_of_vp is not None:
                params[idx_of_vp] = params[idx_of_vp].replace(kind=PK)
            variadic_less_func.__signature__ = Sig(
                params, return_annotation=Sig(func).return_annotation
            )

        return variadic_less_func
    else:
        return func


tuple_the_args = partial(
    ch_variadics_to_non_variadic_kind, ch_variadic_keyword_to_keyword=False
)
tuple_the_args.__name__ = "tuple_the_args"
tuple_the_args.__doc__ = """
A decorator that will change a VAR_POSITIONAL (*args) argument to a tuple (args)
argument of the same name.
"""


def ch_func_to_all_pk(func):
    """Returns a decorated function where all arguments are of the PK kind.
    (PK: Positional_or_keyword)

    :param func: A callable
    :return:

    >>> def f(a, /, b, *, c=None, **kwargs):
    ...     return a + b * c
    ...
    >>> print(Sig(f))
    (a, /, b, *, c=None, **kwargs)
    >>> ff = ch_func_to_all_pk(f)
    >>> print(Sig(ff))
    (a, b, c=None, **kwargs)
    >>> ff(1, 2, 3)
    7
    >>>
    >>> def g(x, y=1, *args, **kwargs):
    ...     ...
    ...
    >>> print(Sig(g))
    (x, y=1, *args, **kwargs)
    >>> gg = ch_func_to_all_pk(g)
    >>> print(Sig(gg))
    (x, y=1, args=(), **kwargs)

    # >>> def h(x, *y, z):
    # ...     print(f"{x=}, {y=}, {z=}")
    # >>> h(1, 2, 3, z=4)
    # x=1, y=(2, 3), z=4
    # >>> hh = ch_func_to_all_pk(h)
    # >>> hh(1, (2, 3), z=4)
    # x=1, y=(2, 3), z=4
    """

    # _func = tuple_the_args(func)
    # sig = Sig(_func)
    #
    # @wraps(func)
    # def __func(*args, **kwargs):
    #     # b = Sig(_func).bind_partial(*args, **kwargs)
    #     # return _func(*b.args, **b.kwargs)
    #     args, kwargs = Sig(_func).extract_args_and_kwargs(
    #         *args, **kwargs, _ignore_kind=False
    #     )
    #     return _func(*args, **kwargs)
    #
    _func = tuple_the_args(func)
    sig = Sig(_func)

    @wraps(func)
    def __func(*args, **kwargs):
        args, kwargs = Sig(_func).extract_args_and_kwargs(
            *args,
            **kwargs,
            # _ignore_kind=False,
            # _allow_partial=True
        )
        return _func(*args, **kwargs)

    __func.__signature__ = all_pk_signature(sig)
    return __func


def copy_func(f):
    """Copy a function (not sure it works with all types of callables)"""
    g = FunctionType(
        f.__code__,
        f.__globals__,
        name=f.__name__,
        argdefs=f.__defaults__,
        closure=f.__closure__,
    )
    g = update_wrapper(g, f)
    g.__kwdefaults__ = f.__kwdefaults__
    if hasattr(f, "__signature__"):
        g.__signature__ = f.__signature__
    return g


# TODO: Similar to other function in this module -- merge.
def params_of(obj: HasParams):
    if isinstance(obj, Signature):
        obj = list(obj.parameters.values())
    elif isinstance(obj, Mapping):
        obj = list(obj.values())
    elif callable(obj):
        obj = list(signature(obj).parameters.values())
    assert all(
        isinstance(p, Parameter) for p in obj
    ), "obj needs to be a Iterable[Parameter] at this point"
    return obj  # as is


########################################################################################################################
# TODO: Encorporate in Sig
def insert_annotations(s: Signature, /, *, return_annotation=empty, **annotations):
    """Insert annotations in a signature.
    (Note: not really insert but returns a copy of input signature)

    >>> from inspect import signature
    >>> s = signature(lambda a, b, c=1, d="bar": 0)
    >>> s
    <Signature (a, b, c=1, d='bar')>
    >>> ss = insert_annotations(s, b=int, d=str)
    >>> ss
    <Signature (a, b: int, c=1, d: str = 'bar')>
    >>> insert_annotations(s, b=int, d=str, e=list)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    AssertionError: These argument names weren't found in the signature: {'e'}
    """
    assert set(annotations) <= set(s.parameters), (
        f"These argument names weren't found in the signature: "
        f"{set(annotations) - set(s.parameters)}"
    )
    params = dict(s.parameters)
    for name, annotation in annotations.items():
        p = params[name]
        params[name] = Parameter(
            name=name, kind=p.kind, default=p.default, annotation=annotation
        )
    return Signature(params.values(), return_annotation=return_annotation)


def common_and_diff_argnames(func1: callable, func2: callable) -> dict:
    """Get list of argument names that are common to two functions, as well as the two
    lists of names that are different

    Args:
        func1: First function
        func2: Second function

    Returns: A dict with fields 'common', 'func1_not_func2', and 'func2_not_func1'

    >>> def f(t, h, i, n, k):
    ...     ...
    ...
    >>> def g(t, w, i, c, e):
    ...     ...
    ...
    >>> common_and_diff_argnames(f, g)
    {'common': ['t', 'i'], 'func1_not_func2': ['h', 'n', 'k'], 'func2_not_func1': ['w', 'c', 'e']}
    >>> common_and_diff_argnames(g, f)
    {'common': ['t', 'i'], 'func1_not_func2': ['w', 'c', 'e'], 'func2_not_func1': ['h', 'n', 'k']}
    """
    p1 = signature(func1).parameters
    p2 = signature(func2).parameters
    return {
        "common": [x for x in p1 if x in p2],
        "func1_not_func2": [x for x in p1 if x not in p2],
        "func2_not_func1": [x for x in p2 if x not in p1],
    }


dflt_name_for_kind = {
    Parameter.VAR_POSITIONAL: "args",
    Parameter.VAR_KEYWORD: "kwargs",
}

arg_order_for_param_tuple = ("name", "default", "annotation", "kind")


def set_signature_of_func(
    func, parameters, *, return_annotation=empty, __validate_parameters__=True
):
    """Set the signature of a function, with sugar.

    Args:
        func: Function whose signature you want to set
        signature: A list of parameter specifications. This could be an
        inspect.Parameter object or anything that
            the mk_param function can resolve into an inspect.Parameter object.
        return_annotation: Passed on to inspect.Signature.
        __validate_parameters__: Passed on to inspect.Signature.

    Returns:
        None (but sets the signature of the input function)

    >>> import inspect
    >>> def foo(*args, **kwargs):
    ...     pass
    ...
    >>> inspect.signature(foo)
    <Signature (*args, **kwargs)>
    >>> set_signature_of_func(foo, ["a", "b", "c"])
    >>> inspect.signature(foo)
    <Signature (a, b, c)>
    >>> set_signature_of_func(
    ...     foo, ["a", ("b", None), ("c", 42, int)]
    ... )  # specifying defaults and annotations
    >>> inspect.signature(foo)
    <Signature (a, b=None, c: int = 42)>
    >>> set_signature_of_func(
    ...     foo, ["a", "b", "c"], return_annotation=str
    ... )  # specifying return annotation
    >>> inspect.signature(foo)
    <Signature (a, b, c) -> str>
    >>> # But you can always specify parameters the "long" way
    >>> set_signature_of_func(
    ...     foo,
    ...     [inspect.Parameter(name="kws", kind=inspect.Parameter.VAR_KEYWORD)],
    ...     return_annotation=str,
    ... )
    >>> inspect.signature(foo)
    <Signature (**kws) -> str>

    """
    sig = Sig(
        parameters,
        return_annotation=return_annotation,
        __validate_parameters__=__validate_parameters__,
    )
    func.__signature__ = sig.to_simple_signature()
    # Not returning func so it's clear(er) that the function is transformed in place


# Pattern: (rewiring) wrapper of make_dataclass
# TODO: Is there a clean way for module to be populated by __name__ of caller module?
def sig_to_dataclass(
    sig: SignatureAble, *, cls_name=None, bases=(), module=None, **kwargs
):
    """
    Make a ``class`` (through ``make_dataclass``) from the given signature.

    :param sig: A ``SignatureAble``, that is, anything that ensure_signature can
        resolve into an ``inspect.Signature`` object, including a signature object
        itself, but also most callables, a list or params, etc.
    :param cls_name: The same as ``cls_name`` of ``dataclasses.make_dataclass``
    :param bases: The same as ``bases`` of ``dataclasses.make_dataclass``
    :param module: Set to module (usually ``__name__`` to specify ther module of
        caller) so that the class and instances can be pickle-able.
    :param kwargs: Passed on to ``dataclasses.make_dataclass``
    :return: A dataclass

    >>> def foo(a, /, b : int=2, *, c=3):
    ...     pass
    ...
    >>> K = sig_to_dataclass(foo, cls_name='K')
    >>> str(Sig(K))
    '(a, b: int = 2, c=3) -> None'
    >>> k = K(1,2,3)
    >>> (k.a, k.b, k.c)
    (1, 2, 3)

    Would also work with any of these (and more):

    >>> K = sig_to_dataclass(Sig(foo), cls_name='K')
    >>> K = sig_to_dataclass(Sig(foo).params, cls_name='K')

    Note: ``cls_name`` is not required (we'll try to figure out a good default for you),
    but it's advised to only use this convenience in extreme mode.
    Choosing your own name might make for a safer future if you're reusing your class.

    """
    from dataclasses import make_dataclass

    sig = ensure_signature(sig)
    cls_name = cls_name or getattr(sig, "name", "_made_by_sig_to_dataclass")
    params = ensure_params(sig)
    fields = [(p.name, p.annotation, p.default) for p in params]
    cls = make_dataclass(cls_name, fields, bases=bases, **kwargs)
    if module:
        cls.__module__ = module
    return cls


def replace_kwargs_using(sig: SignatureAble):
    """
    Decorator that replaces the variadic keyword argument of the target function using
    the `sig`, the signature of a source function.
    It essentially injects the difference between `sig` and the target function's
    signature into the target function's signature. That is, it replaces the
    variadic keyword argument (a.k.a. "kwargs") with those parameters that are in `sig`
    but not in the target function's signature.

    This is meant to be used when a `targ_func` (the function you'll apply the
    decorator to) has a variadict keyword argument that is just used to forward "extra"
    arguments to another function, and you want to make sure that the signature of the
    `targ_func` is consistent with the `sig` signature.
    (Also, you don't want to copy the signatures around manually.)

    In the following, `sauce` (the target function) has a variadic keyword argument,
    `sauce_kwargs`, that is used to forward extra arguments to `apple` (the source
    function).

    >>> def apple(a, x: int, y=2, *, z=3, **extra_apple_options):
    ...     return a + x + y + z
    >>> @replace_kwargs_using(apple)
    ... def sauce(a, b, c, **sauce_kwargs):
    ...     return b * c + apple(a, **sauce_kwargs)

    The function will works:

    >>> sauce(1, 2, 3, x=4, z=5)  # func still works? Should be: 1 + 4 + 2 + 5 + 2 * 3
    18

    But the signature now doesn't have the `**sauce_kwargs`, but more informative
    signature elements sourced from `apple`:

    >>> Sig(sauce)
    <Sig (a, b, c, *, x: int, y=2, z=3, **extra_apple_options)>

    One thing to note is that the order of the arguments in the signature of `apple`
    may change to accomodate for the python parameter order rules
    (see https://docs.python.org/3/reference/compound_stmts.html#function-definitions).
    The new order will try to conserve the order of the original arguments of `sauce`
    in-so-far as it doesn't violate the python parameter order rules, though.
    See examples below:

    >>> @Sig.replace_kwargs_using(apple)
    ... def sauce(a, b=2, c=3, **sauce_kwargs):
    ...     return b * c + apple(a, **sauce_kwargs)
    >>> Sig(sauce)
    <Sig (a, b=2, c=3, *, x: int, y=2, z=3, **extra_apple_options)>

    >>> @Sig.replace_kwargs_using(apple)
    ... def sauce(a=1, b=2, c=3, **sauce_kwargs):
    ...     return b * c + apple(a, **sauce_kwargs)
    >>> Sig(sauce)
    <Sig (a=1, b=2, c=3, *, x: int, y=2, z=3, **extra_apple_options)>

    """

    def decorator(targ_func):
        targ_func_sig = Sig(targ_func)  # function whose signature we're changing
        if targ_func_sig.has_var_keyword:
            # remove it from the signature of targ_sig (we're replacing it!)
            targ_func_sig = Sig(targ_func)[:-1]
        else:
            # if there is none, we shouldn't be using replace_kwargs_using!
            raise ValueError(
                f"Target function {targ_func} must have a variadict keyword argument"
            )

        src_sig = Sig(sig)  # signature we're using to replace kwargs of targ_func

        # Remove all params of src_sig that are in targ_func_sig
        # This is because if they're used, they will be bound to the non-variadic
        # target arguments, so there's no conflict: the target kind, default,
        # and annotation should be used not the source ones.
        src_sig -= targ_func_sig

        # make all parameters of src_sig keyword-only
        # (they're replacing variadic keywords after all!)
        # All? No -- a variadic keyword in the src_sig should remain so
        names_of_all_params_in_src_sig_that_are_not_variadic_keyword = [
            p.name for p in src_sig.params if p.kind != Parameter.VAR_KEYWORD
        ]
        n = len(names_of_all_params_in_src_sig_that_are_not_variadic_keyword)

        src_sig = src_sig.ch_kinds(
            **dict(
                zip(
                    names_of_all_params_in_src_sig_that_are_not_variadic_keyword,
                    [Parameter.KEYWORD_ONLY] * n,
                )
            )
        )

        new_sig = targ_func_sig.merge_with_sig(src_sig)
        return new_sig(targ_func)

    return decorator


Sig.replace_kwargs_using = replace_kwargs_using

#########################################################################################
# Manual construction of missing signatures
# ############################################################################


# TODO: Might want to monkey-patch inspect._signature_from_callable to use
#  sigs_for_sigless_builtin_name
def _robust_signature_of_callable(callable_obj: Callable) -> Signature:
    r"""Get the signature of a Callable, returning a custom made one for those
    builtins that don't have one

    >>> _robust_signature_of_callable(
    ...     _robust_signature_of_callable
    ... )  # has a normal signature
    <Signature (callable_obj: ...Callable) -> inspect.Signature>
    >>> s = _robust_signature_of_callable(print)  # has one that this module provides
    >>> assert isinstance(s, Signature)
    >>> # Will be: <Signature (*value, sep=' ', end='\n', file=<_io.TextIOWrapper
    name='<stdout>' mode='w' encoding='utf-8'>, flush=False)>
    >>> _robust_signature_of_callable(
    ...     slice
    ... )  # doesn't have one, so will return a blanket one
    <Signature (*no_sig_args, **no_sig_kwargs)>

    """
    # First check if we have a custom signature for this type/object
    # This is important for operator instances that might have generic signatures in Python 3.12+
    obj_name = getattr(callable_obj, "__name__", None)
    if obj_name in sigs_for_sigless_builtin_name:
        return sigs_for_sigless_builtin_name[obj_name] or DFLT_SIGNATURE

    type_name = getattr(type(callable_obj), "__name__", None)
    if type_name in sigs_for_type_name:
        return sigs_for_type_name[type_name] or DFLT_SIGNATURE

    # Try to get the signature normally
    try:
        return signature(callable_obj)
    except ValueError:
        # if all attempts fail, return the default signature
        return DFLT_SIGNATURE


def resolve_function(obj: T) -> T | Callable:
    """Get the underlying function of a property or cached_property

    Note that if all conditions fail, the object itself is returned.

    The problem this function solves is that sometimes there's a function behind an
    object, but it's not always easy to get to it. For example, in a class, you might
    want to get the source of the code decorated with ``@property``, a
    ``@cached_property``, or a ``partial`` function.

    Consider the following example:

    >>> from functools import cached_property, partial
    >>> class C:
    ...     @property
    ...     def prop(self):
    ...         pass
    ...     @cached_property
    ...     def cached_prop(self):
    ...         pass
    ...     partial_func = partial(partial)

    Note that ``prop`` is not callable, and you can't get its source.

    >>> import inspect
    >>> callable(C.prop)
    False
    >>> inspect.getsource(C.prop)  # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    TypeError: <property object at 0x...> is not a module, class, method, function, traceback, frame, or code object

    But if you grab the underlying function, you can get the source:

    >>> func = resolve_function(C.prop)
    >>> callable(func)
    True
    >>> isinstance(inspect.getsource(func), str)
    True

    Same goes with ``cached_property`` and ``partial``:

    >>> isinstance(inspect.getsource(resolve_function(C.cached_prop)), str)
    True
    >>> isinstance(inspect.getsource(resolve_function(C.partial_func)), str)
    True

    """
    if isinstance(obj, cached_property):
        return obj.func
    elif isinstance(obj, property):
        return obj.fget
    elif isinstance(obj, (partial, partialmethod)):
        return obj.func
    elif not callable(obj) and callable(wrapped := getattr(obj, "__wrapped__", None)):
        # If obj is not callable, but has a __wrapped__ attribute that is, return that
        return wrapped
    else:  # if not just return obj
        return obj


def dict_of_attribute_signatures(cls: type) -> dict[str, Signature]:
    """
    A function that extracts the signatures of all callable attributes of a class.

    :param cls: The class that holds the the ``(name, func)`` pairs we want to extract.
    :return: A dict of ``(name, signature(func))`` pairs extracted from class.

    One of the intended applications is to use ``dict_of_attribute_signatures`` as a
    decorator, like so:

    >>> @dict_of_attribute_signatures
    ... class names_and_signatures:
    ...     def foo(x: str, *, y=2) -> tuple: ...
    ...     def bar(z, /) -> float: ...
    >>> names_and_signatures
    {'foo': <Signature (x: str, *, y=2) -> tuple>, 'bar': <Signature (z, /) -> float>}
    """

    def gen():
        object_attr_names = set(vars(object))
        for attr_name, attr_val in vars(cls).items():
            if callable(attr_val):
                if attr_name not in object_attr_names:
                    # if the attr is a callable attribute that's not in all objects...
                    yield attr_name, signature(attr_val)

    return dict(gen())


@dict_of_attribute_signatures
class sigs_for_builtins:
    def __import__(name, globals=None, locals=None, fromlist=(), level=0):
        """__import__(name, globals=None, locals=None, fromlist=(), level=0) -> module"""

    def filter(function, iterable, /):
        """filter(function or None, iterable) --> filter object"""

    def map(func, iterable, /, *iterables):
        """map(func, *iterables) --> map object"""

    def print(*value, sep=" ", end="\n", file=sys.stdout, flush=False):
        """print(value, ..., sep=' ', end='\n', file=sys.stdout, flush=False)"""

    def zip(*iterables):
        """
        zip(*iterables) --> A zip object yielding tuples until an input is exhausted.
        """

    def bool(x: Any, /) -> bool: ...

    def bytearray(iterable_of_ints: Iterable[int], /): ...

    def classmethod(function: Callable, /): ...

    def int(x, base=10, /): ...

    def iter(callable: Callable, sentinel=None, /): ...

    def next(iterator: Iterator, default=None, /): ...

    def staticmethod(function: Callable, /): ...

    def str(bytes_or_buffer, encoding=None, errors=None, /): ...

    def super(type_, obj=None, /): ...

    # def type(name, bases=None, dict=None, /):
    #     ...


sigs_for_builtins = dict(
    sigs_for_builtins,
    **{
        "__build_class__": None,
        # __build_class__(func, name, /, *bases, [metaclass], **kwds) -> class
        # "bool": None,
        # bool(x) -> bool
        "breakpoint": None,
        # breakpoint(*args, **kws)
        # "bytearray": None,
        # bytearray(iterable_of_ints) -> bytearray
        # bytearray(string, encoding[, errors]) -> bytearray
        # bytearray(bytes_or_buffer) -> mutable copy of bytes_or_buffer
        # bytearray(int) -> bytes array of size given by the parameter initialized with
        # null bytes
        # bytearray() -> empty bytes array
        "bytes": None,
        # bytes(iterable_of_ints) -> bytes
        # bytes(string, encoding[, errors]) -> bytes
        # bytes(bytes_or_buffer) -> immutable copy of bytes_or_buffer
        # bytes(int) -> bytes object of size given by the parameter initialized with null
        # bytes
        # bytes() -> empty bytes object
        # "classmethod": None,
        # classmethod(function) -> method
        "dict": None,
        # dict() -> new empty dictionary
        # dict(mapping) -> new dictionary initialized from a mapping object's
        # dict(iterable) -> new dictionary initialized as if via:
        # dict(**kwargs) -> new dictionary initialized with the name=value pairs
        "dir": None,
        # dir([object]) -> list of strings
        "frozenset": None,
        # frozenset() -> empty frozenset object
        # frozenset(iterable) -> frozenset object
        "getattr": None,
        # getattr(object, name[, default]) -> value
        # "int": None,
        # int([x]) -> integer
        # int(x, base=10) -> integer
        # "iter": None,
        # iter(iterable) -> iterator
        # iter(callable, sentinel) -> iterator
        "max": None,
        # max(iterable, *[, default=obj, key=func]) -> value
        # max(arg1, arg2, *args, *[, key=func]) -> value
        "min": None,
        # min(iterable, *[, default=obj, key=func]) -> value
        # min(arg1, arg2, *args, *[, key=func]) -> value
        # "next": None,
        # next(iterator[, default])
        "range": None,
        # range(stop) -> range object
        # range(start, stop[, step]) -> range object
        "set": None,
        # set() -> new empty set object
        # set(iterable) -> new set object
        "slice": None,
        # slice(stop)
        # slice(start, stop[, step])
        # "staticmethod": None,
        # staticmethod(function) -> method
        # "str": None,
        # str(object='') -> str
        # str(bytes_or_buffer[, encoding[, errors]]) -> str
        # "super": None,
        # super() -> same as super(__class__, <first argument>)
        # super(type) -> unbound super object
        # super(type, obj) -> bound super object; requires isinstance(obj, type)
        # super(type, type2) -> bound super object; requires issubclass(type2, type)
        # "type": None,
        # type(object_or_name, bases, dict)
        # type(object) -> the object's type
        # type(name, bases, dict) -> a new type
        "vars": None,
        # vars([object]) -> dictionary
    },
)
# # Remove the None-valued elements (No, don't, because we distinguish
# # functions we listed but didn't associate a default signature, with those functions
# # we don't list at all.
# sigs_for_builtins = {
#     k: v for k, v in sigs_for_builtins.items() if v is not None
# }


# TODO: itemgetter, attrgetter and methodcaller use KT as their first argument, but
#  in reality both attrgetter and methodcaller are more restrictive: They need to be
#  valid attributes, therefore valid python identifiers. Any better typing for that?
# TODO: We take care of the MutableMapping dunders below, but some of these dunders
#  are not specific to MutableMapping. The signature used below is somewhat, but not
#  completely, specific to MutableMapping. For example, __contains__ is also defined
#  for the `set` type, but it's input is not called key, nor would the KT annotation
#  be completely correct. The signatures were sometimes made to be more general (such
#  as __setitem__ and __delitem__ returning an Any instead of None), but could be
#  made more (for example, annotating return of __iter__ as Iterator instead of
#  Iterator[KT]). We hope that the fact that all the signatures are positional-only
#  will at least mitigate the problem as far as name differences go.
@dict_of_attribute_signatures
class sigs_for_builtin_modules:
    """
    Below are the signatures, manually created to match those callables of the python
    standard library that don't have signatures (through ``inspect.signature``),
    """

    def __eq__(self, other, /) -> bool:
        """self.__eq__(other) <==> self==other"""

    def __ne__(self, other, /) -> bool:
        """self.__ne__(other) <==> self!=other"""

    def __iter__(self, /) -> Iterator[KT]:
        """self.__iter__() <==> iter(self)"""

    def __getitem__(self, key: KT, /) -> VT:
        """self.__getitem__(key) <==> self[key]"""

    def __len__(self, /) -> int:
        """self.__len__() <==> len(self)"""

    def __contains__(self, key: KT, /) -> bool:
        """self.__contains__(key) <==> key in self"""

    def __setitem__(self, key: KT, value: VT, /) -> Any:
        """self.__setitem__(key, value) <==> self[key] = value"""

    def __delitem__(self, key: KT, /) -> Any:
        """self.__delitem__(key) <==> del self[key]"""

    def itemgetter(item, /, *items) -> Callable[[Iterable[VT]], VT | tuple[VT]]:
        """itemgetter(item, ...) --> itemgetter object,"""

    def attrgetter(attr, /, *attrs) -> Callable[[Iterable[VT]], VT | tuple[VT]]:
        """attrgetter(item, ...) --> attrgetter object,"""

    def methodcaller(
        name: KT, /, *args: Iterable[VT], **kwargs: MappingType[str, Any]
    ) -> Callable[[Any], Any]:
        """methodcaller(name, ...) --> methodcaller object"""

    def partial(func: Callable, *args, **keywords) -> Callable:
        """``partial(func, *args, **keywords)`` - new function with partial application
        of the given arguments and keywords."""

    def partialmethod(func: Callable, *args, **keywords) -> Callable:
        """``functools.partialmethod(func, *args, **keywords)``"""


# Merge sigs_for_builtin_modules and sigs_for_builtins
sigs_for_sigless_builtin_name = dict(sigs_for_builtin_modules, **sigs_for_builtins)


@dict_of_attribute_signatures
class sigs_for_type_name:
    """
    Below are the signatures, manually created to match callable objects that are
    output by builtin functions or are instances of builtin classes, and that have no
    signatures (through ``inspect.signature``),
    """

    def itemgetter(iterable: Iterable[VT], /) -> VT | tuple[VT]: ...

    def attrgetter(iterable: Iterable[VT], /) -> VT | tuple[VT]: ...

    def methodcaller(obj: Any) -> Any: ...


############# Tools for testing #########################################################


def param_for_kind(
    name=None,
    kind="positional_or_keyword",
    with_default=False,
    annotation=Parameter.empty,
):
    """Function to easily and flexibly make inspect.Parameter objects for testing.

    It's annoying to have to compose parameters from scratch to testing things.
    This tool should help making it less annoying.

    >>> list(map(param_for_kind, param_kinds))
    [<Parameter "POSITIONAL_ONLY">, <Parameter "POSITIONAL_OR_KEYWORD">, <Parameter "VAR_POSITIONAL">, <Parameter "KEYWORD_ONLY">, <Parameter "VAR_KEYWORD">]
    >>> param_for_kind.positional_or_keyword()
    <Parameter "POSITIONAL_OR_KEYWORD">
    >>> param_for_kind.positional_or_keyword("foo")
    <Parameter "foo">
    >>> param_for_kind.keyword_only()
    <Parameter "KEYWORD_ONLY">
    >>> param_for_kind.keyword_only("baz", with_default=True)
    <Parameter "baz='dflt_keyword_only'">
    """
    name = name or f"{kind}"
    kind_obj = getattr(Parameter, str(kind).upper())
    kind = str(kind_obj).lower()
    default = (
        f"dflt_{kind}"
        if with_default and kind not in {"var_positional", "var_keyword"}
        else Parameter.empty
    )
    return Parameter(name=name, kind=kind_obj, default=default, annotation=annotation)


param_kinds = list(filter(lambda x: x.upper() == x, Parameter.__dict__))

for kind in param_kinds:
    lower_kind = kind.lower()
    setattr(param_for_kind, lower_kind, partial(param_for_kind, kind=kind))
    setattr(
        param_for_kind,
        "with_default",
        partial(param_for_kind, with_default=True),
    )
    setattr(
        getattr(param_for_kind, lower_kind),
        "with_default",
        partial(param_for_kind, kind=kind, with_default=True),
    )
    setattr(
        getattr(param_for_kind, "with_default"),
        lower_kind,
        partial(param_for_kind, kind=kind, with_default=True),
    )

########################################################################################
# Signature Comparison and Compatibility #
########################################################################################

Compared = TypeVar("Compared")
Comparison = TypeVar("Comparison")
Comparator = Callable[[Compared, Compared], Comparison]
Comparison.__doc__ = (
    "The return type of a Comparator. Typically a bool, or int, but can be anything."
    'In that sense it is more of a "collation" than I comparison'
)

# TODO: Make function that makes Comparator types according for different kinds of
#  compared types? (e.g. for comparing signatures, for comparing parameters, ...)
#  See HasAttr in https://github.com/i2mint/i2/blob/feb469acdc0bc8268877b400b9af6dda56de6292/i2/itypes.py#L164
#  for inspiration.
SignatureComparator = Callable[[Signature, Signature], Comparison]
ParamComparator = Callable[[Parameter, Parameter], Comparison]
CallableComparator = Callable[[Callable, Callable], Comparison]

ComparisonAggreg = Callable[[Iterable[Comparison]], Any]

CT = TypeVar("CT")  # some other Compared type (used to define KeyFunction
KeyFunction = Callable[[CT], Compared]
# KeyFunction.__doc__ = "Function that transforms one compared type to another"


def compare_signatures(func1, func2, signature_comparator: SignatureComparator = eq):
    return signature_comparator(Sig(func1), Sig(func2))


# TODO: Look into typing: Why does lint complain about this line of code?
def mk_func_comparator_based_on_signature_comparator(
    signature_comparator: SignatureComparator,
) -> CallableComparator:
    return partial(compare_signatures, signature_comparator=signature_comparator)


def _keyed_comparator(
    comparator: Comparator,
    key: KeyFunction,
    x: CT,
    y: CT,
) -> Comparison:
    """Apply a comparator after transforming inputs through a key function.

    >>> from operator import eq
    >>> parity = lambda x: x % 2
    >>> _keyed_comparator(eq, parity, 1, 3)
    True
    >>> _keyed_comparator(eq, parity, 1, 4)
    False
    """
    return comparator(key(x), key(y))


def keyed_comparator(
    comparator: Comparator,
    key: KeyFunction,
) -> Comparator:
    """Create a key-function enabled binary operator.

    In various places in python functionality is extended by allowing a key function.
    For example, the ``sorted`` function allows a key function to be passed, which is
    applied to each element before sorting. The keyed_comparator function allows a
    comparator to be extended in the same way. The returned comparator will apply the
    key function toeach input before applying the original comparator.

    >>> from operator import eq
    >>> parity = lambda x: x % 2
    >>> comparator = keyed_comparator(eq, parity)
    >>> list(map(comparator, [1, 1, 2, 2], [3, 4, 5, 6]))
    [True, False, False, True]
    """
    return partial(_keyed_comparator, comparator, key)


# For back-compatibility:
_key_function_enabled_operator = _keyed_comparator
_key_function_factory = keyed_comparator


# TODO: Show examples of how this can be used to produce precise error messages.
#  The way to do this is to have the attribute binary functions produce some info dicts
#  that can then be aggregated in aggreg to produce a final error message (or even
#  a final error object, which can even be raised) if there is indeed a mismatch at all.
#  Further more, we might want to make a function that will take a parametrized
#  param_binary_func and produce such a error raising function from it, using the
#  specific functions (extracted by Sig) to produce the error message.
def param_comparator(
    param1: Parameter,
    param2: Parameter,
    *,
    name: Comparator = eq,
    kind: Comparator = eq,
    default: Comparator = eq,
    annotation: Comparator = eq,
    aggreg: ComparisonAggreg = all,
) -> Comparison:
    """Compare two parameters.

    Note that by default, this function is strict, and will return False if
    any of the parameters are not equal. This is because the default
    aggregation function is `all` and the default comparison functions of the
    parameter's attributes are `eq` (meaning equality, not identity).

    But you can change that by passing different comparison functions and/or
    aggregation functions.

    In fact, the real purpose of this function is to be used as a factory of parameter
    binary functions, through parametrizing it with `functools.partial`.

    The parameter binary functions themselves are meant to be used to make signature
    binary functions.

    :param param1: first parameter
    :param param2: second parameter
    :param name: function to compare names
    :param kind: function to compare kinds
    :param default: function to compare defaults
    :param annotation: function to compare annotations
    :param aggreg: function to aggregate results

    >>> from inspect import Parameter
    >>> param1 = Parameter('x', Parameter.POSITIONAL_OR_KEYWORD)
    >>> param2 = Parameter('x', Parameter.POSITIONAL_OR_KEYWORD)
    >>> param_binary_func(param1, param2)
    True

    See https://github.com/i2mint/i2/issues/50#issuecomment-1381686812 for discussion.

    """
    return aggreg(
        (
            name(param1.name, param2.name),
            kind(param1.kind, param2.kind),
            default(param1.default, param2.default),
            annotation(param1.annotation, param2.annotation),
        )
    )


param_comparator: ParamComparator
param_binary_func = param_comparator  # back compatibility alias


def dflt1_is_empty_or_dflt2_is_not(dflt1, dflt2):
    """
    Why such a strange default comparison function?

    This is to be used as a default in is_call_compatible_with.

    Consider two functions func1 and func2 with a parameter p with default values
    dflt1 and dflt2 respectively.
    If dflt1 was not empty and dflt2 was, this would mean that func1 could be called
    without specifying p, but func2 couldn't.

    So to avoid this situation, we use dflt1_is_empty_or_dflt2_is_not as the default

    """
    return dflt1 is empty or dflt2 is not empty


# TODO: Implement annotation compatibility
def ignore_any_differences(x, y):
    return True


permissive_param_comparator = partial(
    param_comparator,
    name=ignore_any_differences,
    kind=ignore_any_differences,
    default=ignore_any_differences,
    annotation=ignore_any_differences,
)
permissive_param_comparator.__doc__ = """
Permissive version of param_comparator that ignores any differences of parameter 
attributes.

It is meant to be used with partial, but with a permissive base, contrary to the 
base param_comparator which requires strict equality (`eq`) for all attributes.
"""

dflt1_is_empty_or_dflt2_is_not_param_comparator = partial(
    permissive_param_comparator, default=dflt1_is_empty_or_dflt2_is_not
)


def return_tuple(x, y):
    return x, y


param_attribute_dict: ComparisonAggreg


def param_attribute_dict(name_kind_default_annotation: Iterable[Comparison]) -> dict:
    keys = ["name", "kind", "default", "annotation"]
    return {key: value for key, value in zip(keys, name_kind_default_annotation)}


param_comparison_dict = partial(
    param_comparator,
    name=return_tuple,
    kind=return_tuple,
    default=return_tuple,
    annotation=return_tuple,
    aggreg=param_attribute_dict,
)

param_comparison_dict.__doc__ = """
A ParamComparator that returns a dictionary with pairs parameter attributes.

>>> param1 = Sig('(a: int = 1)')['a']
>>> param2 = Sig('(a: str = 2)')['a']
>>> param_comparison_dict(param1, param2)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
{'name': ('a', 'a'), 'kind': ..., 'default': (1, 2), 'annotation': (<class 'int'>, <class 'str'>)}
"""


def param_differences_dict(
    param1: Parameter,
    param2: Parameter,
    *,
    name: Comparator = eq,
    kind: Comparator = eq,
    default: Comparator = eq,
    annotation: Comparator = eq,
):
    """Makes a dictionary exibiting the differences between two parameters.

    >>> param1 = Sig('(a: int = 1)')['a']
    >>> param2 = Sig('(a: str = 2)')['a']
    >>> param_differences_dict(param1, param2)
    {'default': (1, 2), 'annotation': (<class 'int'>, <class 'str'>)}
    >>> param_differences_dict(param1, param2, default=lambda x, y: isinstance(x, type(y)))
    {'annotation': (<class 'int'>, <class 'str'>)}
    """
    equality_vector = param_comparator(
        param1,
        param2,
        name=name,
        kind=kind,
        default=default,
        annotation=annotation,
        aggreg=tuple,
    )
    comparison_dict = param_comparison_dict(param1, param2)
    return {
        key: comparison_dict[key]
        for key, equal in zip(comparison_dict, equality_vector)
        if not equal
    }


def defaults_are_the_same_when_not_empty(dflt1, dflt2):
    """
    Check if two defaults are the same when they are not empty.

    # >>> defaults_are_the_same_when_not_empty(1, 1)
    # True
    # >>> defaults_are_the_same_when_not_empty(1, 2)
    # False
    # >>> defaults_are_the_same_when_not_empty(1, None)
    # False
    # >>> defaults_are_the_same_when_not_empty(1, Parameter.empty)
    # True
    """
    return dflt1 is empty or dflt2 is empty or dflt1 == dflt2


def postprocess(egress: Callable):
    """A decorator that will process the output of the wrapped function with egress"""

    # Note: Vendorized version equivalent ones in i2.deco and i2.wrapper
    def postprocessed(func):
        @wraps(func)
        def wrapped_func(*args, **kwargs):
            original_output = func(*args, **kwargs)
            return egress(original_output)

        return wrapped_func

    return postprocessed


# TODO: It seems like param_comparator is really only used to compare parameters on defaults.
#   This may be due to the fact that is_call_compatible_with was developed independently
#   from the other general param_comparator functionality that was developed (see above)
#   The code of is_call_compatible_with should be reviwed and refactored to use general
#   tools.
@postprocess(
    all
)  # see "Use of postprocess" in https://github.com/i2mint/i2/discussions/63#discussioncomment-10394910
def is_call_compatible_with(
    sig1: Sig,
    sig2: Sig,
    *,
    param_comparator: ParamComparator | None = None,
) -> bool:
    """Return True if ``sig1`` is compatible with ``sig2``. Meaning that all valid ways
    to call ``sig1`` are valid for ``sig2``.

    :param sig1: The main signature.
    :param sig2: The signature to be compared with.
    :param param_comparator: The function used to compare two parameters

    >>> is_call_compatible_with(
    ...     Sig('(a, /, b, *, c)'),
    ...     Sig('(a, b, c)')
    ... )
    True
    >>> is_call_compatible_with(
    ...     Sig('()'),
    ...     Sig('(a)')
    ... )
    False
    >>> is_call_compatible_with(
    ...     Sig('()'),
    ...     Sig('(a=0)')
    ... )
    True
    >>> is_call_compatible_with(
    ...     Sig('(a, /, *, c)'),
    ...     Sig('(a, /, b, *, c)')
    ... )
    False
    >>> is_call_compatible_with(
    ...     Sig('(a, /, *, c)'),
    ...     Sig('(a, /, b=0, *, c)')
    ... )
    True
    >>> is_call_compatible_with(
    ...     Sig('(a, /, b)'),
    ...     Sig('(a, /, b, *, c)')
    ... )
    False
    >>> is_call_compatible_with(
    ...     Sig('(a, /, b)'),
    ...     Sig('(a, /, b, *, c=0)')
    ... )
    True
    >>> is_call_compatible_with(
    ...     Sig('(a, /, b, *, c)'),
    ...     Sig('(*args, **kwargs)')
    ... )
    True
    """

    # Note: In case you're tempted to put this default function as an argument default,
    #  don't. Yes, it's preferable in many ways, but makes the "one source of truth"
    #  principle harder to maintain, since this default has to be the same anywhere
    #  the current function is called. Better signature/docs injection functionality
    #  would be warranted.
    #  See https://stackoverflow.com/questions/78874506/how-can-i-avoid-interface-repetition-in-python-function-signatures-and-docstring
    param_comparator = (
        param_comparator or dflt1_is_empty_or_dflt2_is_not_param_comparator
    )

    def validate_variadics():
        return (
            # sig1 can only have a VP if sig2 also has one
            (vp1 is None or vp2 is not None)
            and
            # sig1 can only have a VK if sig2 also has one
            (vk1 is None or vk2 is not None)
        )

    def validate_param_counts():
        # sig1 cannot have more positional params than sig2
        if len(ps1) > len(ps2) and not vp2:
            return False
        # sig1 cannot have keyword params that do not exist in sig2
        if len([n for n in ks1 if n not in ks2]) > 0 and not vk2:
            return False
        return True

    def validate_extra_params():
        # Any extra PO in sig2 must have a default value
        if len(pos1) < len(pos2) and not all(
            sig2.parameters[n].default is not empty for n in pos2[len(pos1) :]
        ):
            return False
        # Any extra PK in sig2 must have its corresponding PO or KO in sig1, or a
        # default value
        for i, n in enumerate(pks2):
            if (
                n not in pks1
                and len(pos1) <= len(pos2) + i
                and n not in kos1
                and sig2.parameters[n].default is empty
            ):
                return False
        # Any extra KO in sig2 must have a default value
        for n in kos2:
            if n not in kos1 and sig2.parameters[n].default == empty:
                return False
        return True

    def validate_param_positions():
        for i, n2 in enumerate(ps2):
            for j, n1 in enumerate(ks1):
                if n1 == n2:
                    if (
                        # It can be a PK in sig1 and a P (PO or PK) in sig2 only if
                        # its position in sig2 is >= to its position in sig1
                        (n1 in pks1 and i < len(pos1) + j)
                        or (
                            n1 in kos1
                            and (
                                # Cannot be a KO in sig1 and a PO in sig2
                                n2 in pos2
                                or
                                # It can be a KO in sig1 and a PK in sig2 only if its
                                # position in sig2 is > than the total number of POs
                                # and PKs in sig1
                                i < len(ps1)
                            )
                        )
                    ):
                        return False
        return True

    def validate_param_compatibility():
        # Every positional param in sig1 must be compatible with its
        # correspondant param in sig2 (at the same index).
        for i in range(len(ps1)):
            if i < len(ps2) and not param_comparator(sig1.params[i], sig2.params[i]):
                return False
        # Every keyword param in sig1 must be compatible with its
        # correspondant param in sig2 (with the same name).
        for n in ks1:
            if n in ks2 and not param_comparator(
                sig1.parameters[n], sig2.parameters[n]
            ):
                return False
        return True

    pos1, pks1, vp1, kos1, vk1 = sig1.detail_names_by_kind()
    ps1 = pos1 + pks1
    ks1 = pks1 + kos1
    pos2, pks2, vp2, kos2, vk2 = sig2.detail_names_by_kind()
    ps2 = pos2 + pks2
    ks2 = pks2 + kos2

    if vp1:
        sig1 -= vp1
    if vk1:
        sig1 -= vk1
    if vp2:
        sig2 -= vp2
    if vk2:
        sig2 -= vk2

    return (
        f()
        for f in [
            validate_variadics,
            validate_param_counts,
            validate_extra_params,
            validate_param_positions,
            validate_param_compatibility,
        ]
    )


from dataclasses import dataclass

from functools import cached_property
from dataclasses import dataclass
from inspect import Parameter


@dataclass
class SigPair:
    """
    Class that operates on a pair of signatures.

    For example, offers methods to compare two signatures in various ways.

    :param sig1: First signature or signature-able object.
    :param sig2: Second signature or signature-able object.

    >>> from pprint import pprint
    >>> def three(a, b: int, c=3): ...
    >>> def little(a, *, b=2, d=4) -> int: ...
    >>> def pigs(a, b) -> int: ...
    >>> sig_pair = SigPair(three, little)
    >>>
    >>> sig_pair.shared_names
    ['a', 'b']
    >>> sig_pair.names_missing_in_sig1
    ['d']
    >>> sig_pair.names_missing_in_sig2
    ['c']
    >>> sig_pair.param_comparison()
    False
    >>> pprint(sig_pair.diff())  # doctest: +NORMALIZE_WHITESPACE
    {'names_missing_in_sig1': ['d'],
    'names_missing_in_sig2': ['c'],
    'param_differences': {'b': {'annotation': (<class 'int'>,
                                                <class 'inspect._empty'>),
                                'default': (<class 'inspect._empty'>, 2),
                                'kind': (<_ParameterKind.POSITIONAL_OR_KEYWORD: 1>,
                                        <_ParameterKind.KEYWORD_ONLY: 3>)}},
    'return_annotation': (<class 'inspect._empty'>, <class 'int'>)}

    Call compatibility says that any arguments leading to a valid call to a function
    having the first signature, will also lead to a valid call to a function having the
    second signature. This is not the case for the signatures of `three` and `little`:

    >>> sig_pair.are_call_compatible()
    False

    But we don't need to have equal signatures to have call compatibility. For example,

    >>> SigPair(three, lambda a, b=2, c=30: None).are_call_compatible()
    True

    Note that call-compatibility is not symmetric. For example, `pigs` is call
    compatible with `three`, since any arguments that are valid for `pigs` are valid
    for `three`:

    >>> SigPair(pigs, three).are_call_compatible()
    True

    But `three` is not call-compatible with `pigs` since `three` requires could include
    a `c` argument, which `pigs` would choke on.

    >>> SigPair(three, pigs).are_call_compatible()
    False

    """

    sig1: Callable | Sig
    sig2: Callable | Sig

    def __post_init__(self):
        self.sig1 = Sig(self.sig1)
        self.sig2 = Sig(self.sig2)

    @cached_property
    def shared_names(self):
        """
        List of names that are common to both signatures, in the order of sig1.

        >>> sig1 = Sig(lambda a, b, c: None)
        >>> sig2 = Sig(lambda b, c, d: None)
        >>> comp = SigPair(sig1, sig2)
        >>> comp.shared_names
        ['b', 'c']
        """
        return [name for name in self.sig1.names if name in self.sig2.names]

    @cached_property
    def names_missing_in_sig2(self):
        """
        List of names that are in the sig1 signature but not in sig2.

        >>> sig1 = Sig(lambda a, b, c: None)
        >>> sig2 = Sig(lambda b, c, d: None)
        >>> comp = SigPair(sig1, sig2)
        >>> comp.names_missing_in_sig2
        ['a']
        """
        return [name for name in self.sig1.names if name not in self.sig2.names]

    @cached_property
    def names_missing_in_sig1(self):
        """
        List of names that are in the sig2 signature but not in sig1.

        >>> sig1 = Sig(lambda a, b, c: None)
        >>> sig2 = Sig(lambda b, c, d: None)
        >>> comp = SigPair(sig1, sig2)
        >>> comp.names_missing_in_sig1
        ['d']
        """
        return [name for name in self.sig2.names if name not in self.sig1.names]

    # TODO: Verify that the doctests are correct!
    def are_call_compatible(self, param_comparator=None) -> bool:
        """
        Check if the signatures are call-compatible.

        Returns True if sig1 can be used to call sig2 or vice versa.

        >>> sig1 = Sig(lambda a, b, c=3: None)
        >>> sig2 = Sig(lambda a, b: None)
        >>> comp = SigPair(sig1, sig2)
        >>> comp.are_call_compatible()
        False

        >>> comp = SigPair(sig2, sig1)
        >>> comp.are_call_compatible()
        True
        """
        return is_call_compatible_with(
            self.sig1, self.sig2, param_comparator=param_comparator
        )

    def param_comparison(self, comparator=param_comparator, aggregation=all) -> bool:
        """
        Compare parameters between the two signatures using the provided comparator function.

        :param comparator: A function to compare two parameters.
        :param aggregation: A function to aggregate the results of the comparisons.
        :return: Boolean result of the aggregated comparisons.

        >>> sig1 = Sig('(a, b: int, c=3)')
        >>> sig2 = Sig('(a, *, b=2, d=4)')
        >>> comp = SigPair(sig1, sig2)
        >>> comp.param_comparison()
        False
        """
        results = [
            comparator(self.sig1.parameters[name], self.sig2.parameters[name])
            for name in self.shared_names
        ]
        return aggregation(results)

    def param_differences(self) -> dict:
        """
        Get a dictionary of parameter differences between the two signatures.

        :return: A dict containing differences for each shared param that has any.

        >>> sig1 = Sig('(a, b: int, c=3)')
        >>> sig2 = Sig('(a, *, b=2, d=4)')
        >>> comp = SigPair(sig1, sig2)
        >>> result = comp.param_differences()
        >>> expected = {
        ...     'b': {
        ...         'kind': (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY),
        ...         'default': (Parameter.empty, 2),
        ...         'annotation': (int, Parameter.empty),
        ...     }
        ... }
        >>> result == expected
        True
        """

        def diff_pairs():
            for name in self.shared_names:
                diff_dict = param_differences_dict(
                    self.sig1.parameters[name], self.sig2.parameters[name]
                )
                if diff_dict:
                    yield name, diff_dict

        return dict(diff_pairs())

    def diff(self) -> dict:
        """
        Get a dictionary of differences between the two signatures.

        >>> from pprint import pprint
        >>> def three(a, b: int, c=3): ...
        >>> def little(a, *, b=2, d=4) -> int: ...
        >>> def pigs(a, b: int = 2) -> int: ...
        >>> pprint(SigPair(three, little).diff())  # doctest: +NORMALIZE_WHITESPACE
        {'names_missing_in_sig1': ['d'],
        'names_missing_in_sig2': ['c'],
        'param_differences': {'b': {'annotation': (<class 'int'>,
                                                    <class 'inspect._empty'>),
                                    'default': (<class 'inspect._empty'>, 2),
                                    'kind': (<_ParameterKind.POSITIONAL_OR_KEYWORD: 1>,
                                            <_ParameterKind.KEYWORD_ONLY: 3>)}},
        'return_annotation': (<class 'inspect._empty'>, <class 'int'>)}
        >>> pprint(SigPair(three, pigs).diff())  # doctest: +NORMALIZE_WHITESPACE
        {'names_missing_in_sig2': ['c'],
        'param_differences': {'b': {'default': (<class 'inspect._empty'>, 2)}},
        'return_annotation': (<class 'inspect._empty'>, <class 'int'>)}
        >>> pprint(SigPair(three, three).diff())
        {}
        """
        d = {
            key: value
            for key, value in {
                "names_missing_in_sig1": self.names_missing_in_sig1,
                "names_missing_in_sig2": self.names_missing_in_sig2,
                "param_differences": self.param_differences(),
            }.items()
            if value
        }
        # add the return_annotation difference, if any
        if self.sig1.return_annotation != self.sig2.return_annotation:
            d["return_annotation"] = (
                self.sig1.return_annotation,
                self.sig2.return_annotation,
            )
        return d

    def diff_str(self) -> str:
        """
        Get a string representation of the differences between the two signatures.
        """
        from pprint import pformat

        return pformat(self.diff())
```

## tests/__init__.py

```python
"""Test modules"""
```

## tests/footprints_test.py

```python
try:
    import pytest
except ImportError as err:
    import warnings

    warnings.warn(
        "You don't seem to have pytest, so I can't use it to test. Shame. pytest is nice."
    )
    warnings.warn(f"Error was: {err}")


def func(obj):
    return obj.a + obj.b


class A:
    e = 2

    def __init__(self, a=1, b=0, c=1, d=10):
        self.a = a
        self.b = b
        self.c = c
        self.d = d

    def target_method(self, x):
        """Accesses ['a', 'b', 'c', 'e']"""
        t = func(self)  # and this function will access some attributes!
        tt = self.other_method(t)
        return x * tt / self.e

    def other_method(self, x=1):
        """Accesses ['c', 'e']"""
        w = self.c * 2  # c is accessed first
        return self.e + self.c * x - w  # and c is accessed again

    @classmethod
    def a_class_method(cls, y):
        """Accesses ['e']"""
        return cls.e + y


class B:
    x = 1
    y = 2
    b = "i am b"
    c = "i am not b"

    def __init__(self, a, greeting="hello", y=3):
        self.a = a
        self.greeting = greeting
        self.y = y  # instance y overwrites class y

    def greet(self, person="world"):
        """Accesses a, x, and y"""
        return str(self.a) + " " + person + "! " + "x+y=" + str(self.x + self.y)

    @property
    def z(self):
        """Accesses x and y"""
        return self.x + self.y

    def accessing_property_method(self):
        """Accesses b and z (and through z, x, and y)"""
        return len(self.b) + self.z

    def with_f_string(self):
        """Accesses greeting and x, but through f-string"""
        return f"{self.greeting} {self.x}"

    def writing_to_an_attribute(self, x_val, a_val):
        """Accesses x (class attr) and a (instance attr), but only to write in it.
        Should it be listed as "accessed".
        I say: Yes, if not too hard to do, but only if requested (what is NECESSARY) for
        the method to be computed is what is needed the most. Therefore only attrs that are getted, not setted.
        """
        self.x = x_val
        self.a = a_val


def convert_output():
    pass


def test_attrs_used_by_method():
    from i2.footprints import attrs_used_by_method

    # TODO: Stopped working in 3.12 (worked in 3.10). See why
    # assert attrs_used_by_method(A.target_method) == {"a", "b", "e"}

    assert attrs_used_by_method(A.other_method) == {"c", "e"}
    assert attrs_used_by_method(A.a_class_method) == {"e"}

    assert attrs_used_by_method(B.greet) == {"a", "x", "y"}
    # assert attrs_used_by_method(B.z) == {'x', 'y'}
    # assert attrs_used_by_method(B.accessing_property_method) == {'b', 'z', 'x', 'y'}  # perhaps z should not be here?
    assert attrs_used_by_method(B.with_f_string) == {"greeting", "x"}
    # assert attrs_used_by_method(B.writing_to_an_attribute) == {'x', 'a'}


def test_attrs_used_by_method_computation():
    from i2.footprints import attrs_used_by_method_computation

    assert attrs_used_by_method_computation(A.target_method, {}, {"x": 3}) == {
        "a",
        "b",
        "c",
        "e",
    }
    assert attrs_used_by_method_computation(A.other_method, {}) == {"c", "e"}
    # assert attrs_used_by_method_computation(A.a_class_method, {}, {'y': 3}) == {'e'}  # fails (returns {})

    init_kws = dict(a=100)
    assert attrs_used_by_method_computation(B.greet, init_kws) == {
        "a",
        "x",
        "y",
    }
    assert attrs_used_by_method_computation(B.z, init_kws) == {"x", "y"}
    assert attrs_used_by_method_computation(B.accessing_property_method, init_kws) == {
        "b",
        "z",
        "x",
        "y",
    }  # z or not z?
    assert attrs_used_by_method_computation(B.with_f_string, init_kws) == {
        "greeting",
        "x",
    }
    assert attrs_used_by_method_computation(
        B.writing_to_an_attribute, init_kws, dict(x_val=0, a_val=1)
    ) == {"x", "a"}


def test_object_dependencies():
    from functools import cached_property
    from i2.footprints import object_dependencies

    class A:
        def foo(self):
            x = self.bar()
            y = self.baz()
            return x + y + self.prop + self.cached_prop

        def bar(instance_name_could_be_anything):
            return 2 * instance_name_could_be_anything.baz()

        def baz(instance):
            return 1

        @cached_property
        def cached_prop(self):
            return 10 * self.baz()

        @property
        def prop(self):
            return 100 * self.bar()

        simple_prop = 1

    expected = {
        "bar": {"baz"},
        "baz": set(),
        "foo": {"bar", "baz", "prop", "cached_prop"},
        "cached_prop": {"baz"},
        "prop": {"bar"},
    }
    assert (
        object_dependencies(A) == expected
    ), f"Expected:\n\t{expected}\n\nGot:\n\t{object_dependencies(A)=}"


## Order conserving
# def test_attrs_used_by_method():
#     from py2mint.footprints import attrs_used_by_method
#
#     assert attrs_used_by_method(A.target_method) == ['a', 'b', 'c', 'e']
#     assert attrs_used_by_method(A.other_method) == ['c', 'e']
#     assert attrs_used_by_method(A.a_class_method) == ['e']
#
#     assert attrs_used_by_method(B.greet) == ['a', 'x', 'y']
#     assert attrs_used_by_method(B.z) == ['x', 'y']
#     assert attrs_used_by_method(B.accessing_property_method) == ['b', 'z', 'x', 'y']  # perhaps z should not be here?
#     assert attrs_used_by_method(B.with_f_string) == ['greeting', 'x']
#     assert attrs_used_by_method(B.writing_to_an_attribute) == ['x', 'a']
#
#
# def test_attrs_used_by_method_computation():
#     from py2mint.footprints import attrs_used_by_method_computation
#
#     assert attrs_used_by_method_computation(A.target_method, {}, {'x': 3}) == ['a', 'b', 'c', 'e']
#     assert attrs_used_by_method_computation(A.other_method, {}) == ['c', 'e']
#     assert attrs_used_by_method_computation(A.a_class_method, {}) == ['e']
#
#     init_kws = dict(a=100)
#     assert attrs_used_by_method_computation(B.greet, init_kws) == ['a', 'x', 'y']
#     assert attrs_used_by_method_computation(B.z, init_kws) == ['x', 'y']
#     assert attrs_used_by_method_computation(
#         B.accessing_property_method, init_kws) == ['b', 'z', 'x', 'y']  # z or not z?
#     assert attrs_used_by_method_computation(B.with_f_string, init_kws) == ['greeting', 'x']
#     assert attrs_used_by_method_computation(
#         B.writing_to_an_attribute, init_kws, dict(x_val=0, y_val=1)) == ['x', 'a']
```

## tests/objects_for_testing.py

```python
"""
A module containing a bunch of objects for testing.

See also: https://github.com/i2mint/py2http/blob/master/py2http/tests/objects_for_testing.py
"""


###############################################################################
# Empty callables, meant to give an example of different kinds of callables ###
# Note that x can be applied to a number, string, or list #####################
def f(x):
    """Note that x can be applied to a number, string, or list"""
    return x * 2


class C:
    @classmethod
    def c(cls, x):
        return f(x)

    @staticmethod
    def s(x):
        return f(x)

    # instance method
    def i(self, x):
        return f(x)

    # making an instance itself callable
    def __call__(self, x):
        return f(x)


###############################################################################


def times_2(x):
    return x * 2


def plus_1(z):
    return z + 1


def add(a, b: float = 0.0) -> float:
    """Adds numbers"""
    return a + b


def mult(x: float, y=1):
    return x * y


def formula1(w, /, x: float, y=1, *, z: int = 1):
    """Has 3 kinds & every of the 4 combinations of (default y/n, annotated y/n)"""
    return ((w + x) * y) ** z


def formula2(w, /, x: float, y=2, *, z: int = 3):
    """Has 3 kinds & every of the 4 combinations of (default y/n, annotated y/n)

    >>> formula2(0, 1, 2)
    '(w:=0) + (x:=1) * (y:=2) ** (z:=3) == 8'
    """
    return f"(w:={w}) + (x:={x}) * (y:={y}) ** (z:={z}) == {w + x * y ** z}"


# Only *args and **kwargs. Also has a doc with a doc test
def sum_of_args(*args, **kwargs):
    """Sums the args, and returns a copy of kwargs with the sum of args added to each value.

    >>> sum_of_args(1,2,3, a=0, b=10)
    {'a': 6, 'b': 16}
    """
    t = sum(args)
    return {k: t + v for k, v in kwargs.items()}


def demo_func_1(
    any_var,
    a_list: list,
    a_dict: dict,
    an_int: int,
    a_float: float = 3.14,
    a_bool=True,
    a_str: str = None,
) -> str:
    """
    This is the first line,
    continued on the next.

    This function is meant to demo several combinations of type declaration (in signature or/and in docs),
    as well as with or without defaults.

    :param any_var: no type declared in either signature of doc
    :param list a_list: type declared in signature AND doc
    :param a_dict: type declared in signature but NOT in doc
    :param int an_int: type declared in doc but NOT in signature
    :param float a_float: type declared in signature AND doc, and has a default of the desired type
    :param a_bool: type declared in signature but NOT in doc, and has a default of the desired type
    :param str a_str: type declared in signature AND doc, but has a default that is not of that type
    line description
    :return str: Will just return the 'demo_func_1 returned' string
    :tags foo, bar

    >>> # here are sometests
    >>> demo_func_1('anything', [1, 2], {'this': 0, 'that': 'foo'}, 1, 1.1, a_bool=True, a_str='hello')
    'demo_func_1 returned'
    """
    # This is the first comment
    return "demo_func_1 returned"  # this is a comment on the same line as some code


class SomeClass:
    pass


class SomeOtherClass:
    pass


def demo_func_2(
    any_var,
    an_int: int,
    a_float=0.1,
    a_tuple: tuple = (),
    an_obj: SomeClass = SomeClass(),
    another_obj=SomeOtherClass(),
):
    """
    This is the first line,
    continued on the next.

    And this is some more information about the function:
    Blah
    Blah
    Bloo

    :param any_var: foo
    :param an_int: you again?!
    :param a_float:
    :param a_tuple: For real
    :param an_obj: Does nothing
    :param another_obj: Multi-
    line
    :return: Just pi
    :tags any, old, tag
    :keyword one_keyword
    :keyword keyword1, keyword2
    """

    """
    This is the first line,
    continued on the next.

    And this is some more information about the function:
    Blah
    Blah
    Bloo

    :param any_var: this can be of any type
    :param an_int: an int
    :param a_float: a float
    :param a_bool: a bool
    :param a_str: a str
    :param a_list:
    :param a_dict: An example of a continued
    line description
    :return:

    >>> # here are sometests
    >>> demo_func_1('anything', 1, 1.1, True, 'hello', [1, 2], {'this': 0, 'that': 'foo'})
    'returned'
    """
    return 3.14


class AClass:
    def __init__(self, a=1, b=0):
        """The doc of the __init__"""
        self.a = a
        self.b = b

    def __call__(self, x: float) -> float:
        """The actual callable"""
        return self.a * x + self.b


class TestObj:
    constarg: str = ""

    def __init__(self, constarg: str):
        print(constarg)
        self.constarg = constarg

    def methodnum(self, methodarg1: int) -> int:
        print(methodarg1)
        return methodarg1 + 1

    def methodstr(self, methodarg2: str = "hi") -> str:
        print(methodarg2)
        return methodarg2 + " test " + self.constarg


from contextlib import suppress

with suppress(ModuleNotFoundError, ImportError):
    from numpy import ndarray, matrix
    from pandas import DataFrame, Series

    def a_func_with_df_in_and_sr_out(df: DataFrame) -> Series:
        return df.sum()

    def a_func_with_np_and_df(
        df: DataFrame, sr: Series, arr: ndarray, mat: matrix
    ) -> matrix:
        return (df.sum().sum() + sr.sum() + arr.sum()) * mat
```

## tests/test_castgraph.py

```python
# test_castgraph.py
# Pytest test suite for i2.castgraph

import json
import types
import pytest

# Import the public API from the target module
from i2.castgraph import (
    ConversionRegistry,
    ConversionError,
    design_guidelines,
)


# --- Helper marker types for tests -------------------------------------------------


class Path(str):
    """Marker type for file paths (to distinguish from arbitrary strings)."""


class Text(str):
    """Marker type for text payloads."""


class JSONDict(dict):
    """Marker type for dicts with JSON semantics."""


class CanonicalRecord(dict):
    """Canonical in-memory record format."""


# --- Tests -------------------------------------------------------------------------


def test_register_and_convert_direct():
    reg = ConversionRegistry()

    @reg.register(Text, JSONDict)
    def text_to_json(t, ctx):
        return JSONDict(json.loads(t or "{}"))

    x = Text('{"a": 1}')
    out = reg.convert(x, JSONDict)
    assert isinstance(out, JSONDict)
    assert out["a"] == 1


def test_multi_hop_routing_when_no_direct_edge():
    reg = ConversionRegistry()

    @reg.register(Path, Text)
    def path_to_text(p, ctx):
        fs = (ctx or {}).get("fs", {})
        return Text(fs.get(str(p), ""))

    @reg.register(Text, JSONDict)
    def text_to_json(t, ctx):
        return JSONDict(json.loads(t or "{}"))

    # No direct Path -> JSONDict registered; must route Path -> Text -> JSONDict
    ctx = {"fs": {"/tmp/data.json": '{"x": 42}'}}
    out = reg.convert(Path("/tmp/data.json"), JSONDict, context=ctx)
    assert isinstance(out, JSONDict)
    assert out["x"] == 42


def test_cost_based_route_selection_prefers_cheaper_edge():
    reg = ConversionRegistry()

    @reg.register(Text, JSONDict, cost=1.0)
    def text_to_json(t, ctx):
        return JSONDict(json.loads(t or "{}"))

    @reg.register(JSONDict, CanonicalRecord, cost=1.0)
    def json_to_canonical(d, ctx):
        return CanonicalRecord(d)

    # Cheaper direct Text -> CanonicalRecord should be preferred over Text -> JSON -> Canonical
    chosen = {}

    @reg.register(Text, CanonicalRecord, cost=0.5)
    def text_to_canonical(t, ctx):
        chosen["direct"] = True
        return CanonicalRecord(json.loads(t or "{}"))

    out = reg.convert(Text('{"k": "v"}'), CanonicalRecord)
    assert isinstance(out, CanonicalRecord)
    assert out["k"] == "v"
    assert chosen.get("direct") is True  # ensure cheaper route chosen


def test_mro_fallback_for_source_type():
    reg = ConversionRegistry()

    class Base: ...

    class Sub(Base): ...

    class Out: ...

    @reg.register(Base, Out)
    def base_to_out(b, ctx):
        return Out()

    # No direct Sub -> Out converter; should use Base -> Out via MRO
    result = reg.convert(Sub(), Out)
    assert isinstance(result, Out)


def test_identity_short_circuit_when_already_target_type():
    reg = ConversionRegistry()
    obj = CanonicalRecord({"a": 1})
    out = reg.convert(obj, CanonicalRecord)
    # Should return the exact same object instance (identity)
    assert out is obj


def test_result_caching_by_object_id_and_type():
    reg = ConversionRegistry()
    calls = {"count": 0}

    @reg.register(Text, JSONDict)
    def text_to_json(t, ctx):
        calls["count"] += 1
        return JSONDict(json.loads(t or "{}"))

    data = Text('{"a":1}')
    # Invoke twice with result caching enabled; converter should run once
    out1 = reg.convert(data, JSONDict, use_result_cache=True)
    out2 = reg.convert(data, JSONDict, use_result_cache=True)
    assert out1 == out2
    assert calls["count"] == 1


def test_context_propagation_for_io_injection():
    reg = ConversionRegistry()

    @reg.register(Path, Text)
    def path_to_text(p, ctx):
        fs = (ctx or {}).get("fs", {})
        return Text(fs.get(str(p), ""))

    @reg.register(Text, JSONDict)
    def text_to_json(t, ctx):
        return JSONDict(json.loads(t or "{}"))

    ctx = {"fs": {"/a/b.json": '{"ok": true}'}}
    out = reg.convert(Path("/a/b.json"), JSONDict, context=ctx)
    assert out["ok"] is True


def test_no_route_raises_conversion_error():
    reg = ConversionRegistry()

    class A: ...

    class B: ...

    a = A()
    with pytest.raises(ConversionError):
        reg.convert(a, B)


def test_path_cache_is_used_across_calls():
    # Indirectly test that repeated conversions reuse path discovery
    reg = ConversionRegistry()
    calls = {"edges": 0}

    class A: ...

    class B: ...

    class C: ...

    @reg.register(A, B)
    def a_to_b(a, ctx):
        calls["edges"] += 1
        return B()

    @reg.register(B, C)
    def b_to_c(b, ctx):
        calls["edges"] += 1
        return C()

    # Two conversions A -> C; path discovery should be cached
    _ = reg.convert(A(), C)
    _ = reg.convert(A(), C)
    # Edge functions run each time (we are not caching results here),
    # but path discovery should not re-run; we can't directly assert path cache,
    # so we at least ensure conversion succeeds without excessive overhead.
    assert isinstance(reg.convert(A(), C), C)


def test_design_guidelines_mentions_registry_and_cache():
    text = design_guidelines().lower()
    assert "registry" in text
    assert "cache" in text


def test_converter_function_signature_accepts_context_even_if_unused():
    reg = ConversionRegistry()

    class Src: ...

    class Dst: ...

    @reg.register(Src, Dst)
    def s_to_d(s, _ctx):  # underscore to indicate unused context
        return Dst()

    assert isinstance(reg.convert(Src(), Dst), Dst)


def test_error_message_contains_types_for_debugging():
    reg = ConversionRegistry()

    class Foo: ...

    class Bar: ...

    with pytest.raises(ConversionError) as excinfo:
        reg.convert(Foo(), Bar)
    msg = str(excinfo.value)
    assert "Foo" in msg and "Bar" in msg


# Optional: smoke test showing a tiny "canonical hub" usage style
def test_canonical_hub_style_is_ergonomic():
    reg = ConversionRegistry()

    class CSVText(str): ...

    class Rows(list): ...

    class Canon(dict): ...

    @reg.register(CSVText, Rows)
    def csv_to_rows(s, ctx):
        # naive CSV: lines of "k,v"
        rows = []
        for line in (s or "").strip().splitlines():
            if not line.strip():
                continue
            k, v = line.split(",", 1)
            rows.append((k.strip(), v.strip()))
        return Rows(rows)

    @reg.register(Rows, Canon)
    def rows_to_canon(rows, ctx):
        return Canon({k: v for k, v in rows})

    out = reg.convert(CSVText("a,1\nb,2"), Canon)
    assert isinstance(out, Canon) and out == {"a": "1", "b": "2"}


def test_single_arg_converter_is_wrapped():
    reg = ConversionRegistry()
    calls = {"n": 0}

    class Src: ...

    class Dst: ...

    @reg.register(Src, Dst)
    def s_to_d(s):
        calls["n"] += 1
        return Dst()

    out = reg.convert(Src(), Dst)
    assert isinstance(out, Dst)
    assert calls["n"] == 1


def test_infer_types_from_annotations_with_and_without_context():
    reg = ConversionRegistry()

    class ASrc: ...

    class ADst: ...

    # With context parameter present
    @reg.register()
    def a_to_b(x: ASrc, ctx) -> ADst:
        return ADst()

    assert isinstance(reg.convert(ASrc(), ADst), ADst)

    # Without context parameter (single-arg converter should be wrapped)
    class BSrc: ...

    class BDst: ...

    @reg.register()
    def b_to_d(x: BSrc) -> BDst:
        return BDst()

    assert isinstance(reg.convert(BSrc(), BDst), BDst)


def test_annotation_conflict_warns_and_uses_explicit():
    reg = ConversionRegistry()

    class A1: ...

    class B1: ...

    class C1: ...

    # function annotated as A1 -> B1, but register explicitly as C1 -> B1
    with pytest.warns(UserWarning):

        @reg.register(C1, B1)
        def annotated(a: A1, ctx) -> B1:
            return B1()

    # Ensure the explicit registration is used
    assert isinstance(reg.convert(C1(), B1), B1)


def test_missing_annotations_raises_at_registration_time():
    reg = ConversionRegistry()

    class M1: ...

    class M2: ...

    with pytest.raises(ValueError):

        @reg.register()
        def bad(x):
            return None
```

## tests/test_castgraph_kinds.py

```python
# test_castgraph_kinds.py
# Comprehensive tests for the new kind-based interface in i2.castgraph

import json
import warnings
import pytest

from i2.castgraph import (
    TransformationGraph,
    ConversionError,
    Kind,
    KindMatch,
)


# --- Tests for KindMatch ---


def test_kind_match_is_truthy():
    """KindMatch should evaluate to True."""
    match = KindMatch()
    assert bool(match) is True
    assert match


def test_kind_match_with_metadata():
    """KindMatch can carry metadata."""
    match = KindMatch({'encoding': 'utf-8', 'analyzed': True})
    assert match.metadata == {'encoding': 'utf-8', 'analyzed': True}
    assert bool(match) is True


def test_kind_match_repr():
    """KindMatch has readable repr."""
    match = KindMatch({'foo': 'bar'})
    assert 'KindMatch' in repr(match)
    assert 'foo' in repr(match)


# --- Tests for Kind class ---


def test_kind_with_identifier():
    """Kind wraps a hashable identifier."""
    k = Kind('text')
    assert k.identifier == 'text'


def test_kind_with_type_identifier_has_implicit_isa():
    """Kind with type identifier automatically uses isinstance."""
    k = Kind(str)
    assert k.isa("hello") is True
    assert k.isa(123) is False


def test_kind_with_custom_isa():
    """Kind can have a custom predicate."""
    k = Kind('json_str', isa=lambda x: isinstance(x, str) and x.startswith('{'))
    assert k.isa('{"key": "value"}')
    assert not k.isa('plain text')


def test_kind_equality():
    """Kinds are equal if identifiers match."""
    k1 = Kind('text')
    k2 = Kind('text')
    assert k1 == k2
    # Can also compare with raw identifier
    assert k1 == 'text'


def test_kind_hashable():
    """Kinds are hashable."""
    k1 = Kind('text')
    k2 = Kind('json')
    d = {k1: 'value1', k2: 'value2'}
    assert d[k1] == 'value1'
    assert d[k2] == 'value2'


# --- Tests for TransformationGraph basic operations ---


def test_add_node_with_string_kind():
    """Can add nodes with string identifiers."""
    graph = TransformationGraph()
    graph.add_node('text', isa=lambda x: isinstance(x, str))
    assert 'text' in graph.kinds()


def test_add_node_with_type_kind():
    """Can add nodes with type identifiers."""
    graph = TransformationGraph()
    graph.add_node(str)
    assert str in graph.kinds()


def test_add_node_with_kind_object():
    """Can add nodes with Kind objects."""
    graph = TransformationGraph()
    text_kind = Kind('text', isa=lambda x: isinstance(x, str))
    graph.add_node(text_kind)
    assert 'text' in graph.kinds()


def test_add_edge_basic():
    """Can add transformation edges."""
    graph = TransformationGraph()

    def text_to_int(s, ctx):
        return int(s)

    graph.add_edge('text', int, text_to_int)
    assert 'text' in graph.kinds()
    assert int in graph.kinds()


def test_register_edge_decorator():
    """Can register edges using decorator."""
    graph = TransformationGraph()

    @graph.register_edge('text', int)
    def text_to_int(s, ctx):
        return int(s)

    # Should have added both nodes
    assert 'text' in graph.kinds()
    assert int in graph.kinds()


def test_register_edge_infers_from_annotations():
    """register_edge can infer kinds from annotations."""
    graph = TransformationGraph()

    @graph.register_edge()
    def str_to_int(s: str, ctx) -> int:
        return int(s)

    # Should have added both types as nodes
    assert str in graph.kinds()
    assert int in graph.kinds()


# --- Tests for transform() ---


def test_transform_basic():
    """Basic transformation works."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int(s, ctx):
        return int(s)

    result = graph.transform("42", int)
    assert result == 42
    assert isinstance(result, int)


def test_transform_with_string_kinds():
    """Can transform using string kinds."""
    graph = TransformationGraph()
    graph.add_node('text', isa=lambda x: isinstance(x, str))
    graph.add_node('number', isa=lambda x: isinstance(x, int))

    @graph.register_edge('text', 'number')
    def text_to_number(t, ctx):
        return int(t)

    result = graph.transform("123", 'number', from_kind='text')
    assert result == 123


def test_transform_multi_hop():
    """Multi-hop transformations work."""
    graph = TransformationGraph()

    @graph.register_edge(str, float)
    def str_to_float(s, ctx):
        return float(s)

    @graph.register_edge(float, int)
    def float_to_int(f, ctx):
        return int(f)

    # Should route str -> float -> int
    result = graph.transform("42.7", int)
    assert result == 42
    assert isinstance(result, int)


def test_transform_with_context():
    """Context is passed through transformations."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int_with_base(s, ctx):
        base = (ctx or {}).get('base', 10)
        return int(s, base)

    result = graph.transform("FF", int, context={'base': 16})
    assert result == 255


def test_transform_identity():
    """Identity transformations return same object."""
    graph = TransformationGraph()
    graph.add_node(int)

    obj = 42
    result = graph.transform(obj, int)
    assert result is obj


def test_transform_raises_on_no_path():
    """Raises ConversionError when no path exists."""
    graph = TransformationGraph()
    graph.add_node(str)
    graph.add_node(int)
    # No edge between them

    with pytest.raises(ConversionError):
        graph.transform("42", int)


# --- Tests for get_transformer() ---


def test_get_transformer_returns_callable():
    """get_transformer returns a callable."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int(s, ctx):
        return int(s)

    transformer = graph.get_transformer(str, int)
    assert callable(transformer)
    assert transformer("42") == 42


def test_get_transformer_with_baked_context():
    """get_transformer can bake in context."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int(s, ctx):
        base = (ctx or {}).get('base', 10)
        return int(s, base)

    transformer = graph.get_transformer(str, int, context={'base': 16})
    assert transformer("FF") == 255


# --- Tests for kind detection ---


def test_detect_kind_with_predicates():
    """detect_kind uses registered predicates."""
    graph = TransformationGraph()
    graph.add_node('json_str', isa=lambda x: isinstance(x, str) and x.startswith('{'))
    graph.add_node('text', isa=lambda x: isinstance(x, str))

    # Should match json_str (first matching predicate)
    assert graph.detect_kind('{"x": 1}') == 'json_str'


def test_detect_kind_with_custom_detector():
    """detect_kind can use a custom detector function."""
    graph = TransformationGraph()

    def my_detector(obj):
        if isinstance(obj, str) and len(obj) > 10:
            return 'long_text'
        return None

    graph.set_kind_detector(my_detector)
    assert graph.detect_kind("short") is None
    assert graph.detect_kind("this is a long string") == 'long_text'


def test_detect_kind_returns_none_on_no_match():
    """detect_kind returns None if no kind matches."""
    graph = TransformationGraph()
    graph.add_node('text', isa=lambda x: isinstance(x, str))

    assert graph.detect_kind(123) is None


def test_detect_kind_fallback_to_type():
    """detect_kind falls back to type(obj) if registered."""
    graph = TransformationGraph()
    graph.add_node(int)

    assert graph.detect_kind(42) == int


# --- Tests for transform_any() ---


def test_transform_any_with_detection():
    """transform_any detects source kind automatically."""
    graph = TransformationGraph()
    graph.add_node('text', isa=lambda x: isinstance(x, str))
    graph.add_node(int)

    @graph.register_edge('text', int)
    def text_to_int(s, ctx):
        return int(s)

    result = graph.transform_any("42", int)
    assert result == 42


def test_transform_any_raises_if_kind_not_detected():
    """transform_any raises if kind cannot be detected."""
    graph = TransformationGraph()
    graph.add_node(int)

    with pytest.raises(ConversionError):
        graph.transform_any("42", int)  # "42" is str, not registered


# --- Tests for introspection methods ---


def test_reachable_from():
    """reachable_from returns all reachable kinds."""
    graph = TransformationGraph()

    @graph.register_edge(str, float)
    def str_to_float(s, ctx):
        return float(s)

    @graph.register_edge(float, int)
    def float_to_int(f, ctx):
        return int(f)

    reachable = graph.reachable_from(str)
    assert float in reachable
    assert int in reachable


def test_sources_for():
    """sources_for returns all source kinds."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int(s, ctx):
        return int(s)

    @graph.register_edge(float, int)
    def float_to_int(f, ctx):
        return int(f)

    sources = graph.sources_for(int)
    assert str in sources
    assert float in sources


def test_kinds_returns_all_registered():
    """kinds() returns all registered kind identifiers."""
    graph = TransformationGraph()
    graph.add_node('text')
    graph.add_node(int)
    graph.add_node('json')

    all_kinds = graph.kinds()
    assert 'text' in all_kinds
    assert int in all_kinds
    assert 'json' in all_kinds


# --- Tests for cost-based routing ---


def test_cost_based_routing_prefers_cheaper():
    """Routing prefers lower-cost paths."""
    graph = TransformationGraph()
    chosen = {}

    # Expensive two-hop path: str -> float -> int (cost 2.0)
    @graph.register_edge(str, float, cost=1.0)
    def str_to_float(s, ctx):
        return float(s)

    @graph.register_edge(float, int, cost=1.0)
    def float_to_int(f, ctx):
        return int(f)

    # Cheap direct path: str -> int (cost 0.5)
    @graph.register_edge(str, int, cost=0.5)
    def str_to_int_direct(s, ctx):
        chosen['direct'] = True
        return int(s)

    result = graph.transform("42", int)
    assert result == 42
    assert chosen.get('direct') is True  # Should use cheaper path


# --- Tests for backward compatibility ---


def test_deprecated_register_warns():
    """Deprecated register() method warns."""
    graph = TransformationGraph()

    with pytest.warns(DeprecationWarning):

        @graph.register(str, int)
        def str_to_int(s, ctx):
            return int(s)


def test_deprecated_convert_warns():
    """Deprecated convert() method warns."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int(s, ctx):
        return int(s)

    with pytest.warns(DeprecationWarning):
        result = graph.convert("42", int)

    assert result == 42


def test_old_conversionregistry_still_works():
    """Old ConversionRegistry still works (for now)."""
    from i2.castgraph import ConversionRegistry

    reg = ConversionRegistry()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        @reg.register(str, int)
        def str_to_int(s, ctx):
            return int(s)

        result = reg.convert("42", int)

    assert result == 42


# --- Tests for single-arg converters ---


def test_single_arg_converter_wrapped():
    """Single-arg converters are automatically wrapped."""
    graph = TransformationGraph()

    @graph.register_edge(str, int)
    def str_to_int(s):  # No context parameter
        return int(s)

    result = graph.transform("42", int)
    assert result == 42


# --- Tests with Kind objects ---


def test_using_kind_objects_for_edges():
    """Can use Kind objects when adding edges."""
    graph = TransformationGraph()

    text_kind = Kind('text', isa=lambda x: isinstance(x, str))
    number_kind = Kind('number', isa=lambda x: isinstance(x, int))

    @graph.register_edge(text_kind, number_kind)
    def text_to_number(t, ctx):
        return int(t)

    result = graph.transform("123", 'number', from_kind='text')
    assert result == 123


# --- Tests for edge cases ---


def test_transform_with_result_cache():
    """Result caching works."""
    graph = TransformationGraph()
    calls = {'count': 0}

    @graph.register_edge(str, int)
    def str_to_int(s, ctx):
        calls['count'] += 1
        return int(s)

    data = "42"
    result1 = graph.transform(data, int, use_result_cache=True)
    result2 = graph.transform(data, int, use_result_cache=True)

    assert result1 == result2 == 42
    assert calls['count'] == 1  # Should only call once


def test_kind_match_in_predicate():
    """Predicates can return KindMatch with metadata."""
    graph = TransformationGraph()

    def json_predicate(obj):
        if isinstance(obj, str) and obj.startswith('{'):
            try:
                json.loads(obj)
                return KindMatch({'valid_json': True})
            except:
                return False
        return False

    graph.add_node('json', isa=json_predicate)

    kind = graph.detect_kind('{"x": 1}')
    assert kind == 'json'


# --- Integration test: realistic scenario ---


def test_realistic_file_to_data_pipeline():
    """Test a realistic multi-format transformation pipeline."""
    graph = TransformationGraph()

    # Define kinds
    graph.add_node('filepath', isa=lambda x: isinstance(x, str) and '/' in x)
    graph.add_node('json_text', isa=lambda x: isinstance(x, str) and x.startswith('{'))
    graph.add_node('data_dict', isa=lambda x: isinstance(x, dict))

    # Define transformations
    @graph.register_edge('filepath', 'json_text')
    def read_file(path, ctx):
        fs = (ctx or {}).get('fs', {})
        return fs.get(path, '{}')

    @graph.register_edge('json_text', 'data_dict')
    def parse_json(text, ctx):
        return json.loads(text)

    # Mock filesystem
    ctx = {'fs': {'/data/config.json': '{"setting": "value"}'}}

    # Multi-hop: filepath -> json_text -> data_dict
    result = graph.transform(
        '/data/config.json', 'data_dict', from_kind='filepath', context=ctx
    )

    assert isinstance(result, dict)
    assert result['setting'] == 'value'


# --- Test for MRO fallback (type-based) ---


def test_mro_fallback_for_types():
    """MRO fallback works for subclasses."""
    graph = TransformationGraph()

    class Base:
        pass

    class Sub(Base):
        pass

    class Out:
        pass

    @graph.register_edge(Base, Out)
    def base_to_out(b, ctx):
        return Out()

    # No direct Sub -> Out, should use Base -> Out via MRO
    result = graph.transform(Sub(), Out)
    assert isinstance(result, Out)
```

## tests/test_signatures.py

```python
"""Test signatures module


# Notes to the reader

Both in the code and in the docs, we'll use short hands for parameter (argument) kind.
    PK = Parameter.POSITIONAL_OR_KEYWORD
    VP = Parameter.VAR_POSITIONAL
    VK = Parameter.VAR_KEYWORD
    PO = Parameter.POSITIONAL_ONLY
    KO = Parameter.KEYWORD_ONLY

"""

import pytest
from functools import reduce
from typing import Any
import sys

from i2 import Sig
from i2.signatures import *
from i2.signatures import (
    normalized_func,
    sigs_for_sigless_builtin_name,
    _robust_signature_of_callable,
)

from i2.tests.util import (
    call_and_return_error,
    sig_to_inputs,
    trace_call,
    function_is_compatible_with_signature,
)


def test_class_attribute_signatures():
    class Klass:
        def leave(self): ...

        @property
        def no(self): ...

        @cached_property
        def stone(self): ...

        unturned = partial(lambda self: self)

    assert str(Sig(Klass.leave)) == "(self)"
    assert str(Sig(Klass.no)) == "(self)"
    assert str(Sig(Klass.stone)) == "(self)"
    assert str(Sig(Klass.unturned)) == "(self)"

    instance = Klass()

    assert str(Sig(instance.leave)) == "()"
    assert str(Sig(instance.no)) == "()"
    assert str(Sig(instance.stone)) == "()"
    assert str(Sig(instance.unturned)) == "(self)"


def test_add_optional_keywords():
    @Sig.add_optional_keywords({"c": 2, "d": 3}, {"c": int})
    def foo(a, *, b=1, **kwargs):
        return f"{a=}, {b=}, {kwargs=}"

    assert str(Sig(foo)) == "(a, *, c: int = 2, d=3, b=1, **kwargs)"
    assert foo(0, d=10) == "a=0, b=1, kwargs={'d': 10}"

    # Testing when kwarg_annotations is used with keyword argument
    @Sig.add_optional_keywords({"c": 2, "d": 3}, kwarg_annotations={"c": int})
    def foo(a, *, b=1, **kwargs):
        pass

    assert str(Sig(foo)) == "(a, *, c: int = 2, d=3, b=1, **kwargs)"

    # Testing when both kwarg_and_defaults and kwarg_annotations are used
    # with keyword argument
    @Sig.add_optional_keywords(
        kwarg_and_defaults={"c": 2, "d": 3}, kwarg_annotations={"c": int}
    )
    def foo(a, *, b=1, **kwargs):
        pass

    assert str(Sig(foo)) == "(a, *, c: int = 2, d=3, b=1, **kwargs)"

    # Testing when add_optional_keywords is used as instance method
    def foo(a, *, b=1, **kwargs):
        return f"{a=}, {b=}, {kwargs=}"

    assert foo(0, d=10) == "a=0, b=1, kwargs={'d': 10}"

    sig = Sig(foo)
    new_sig = sig.add_optional_keywords({"c": 2, "d": 3}, {"c": int})

    assert str(new_sig) == ("(a, *, c: int = 2, d=3, b=1, **kwargs)")

    sig(foo)  # decorate foo, and see that it still works
    assert foo(0, d=10) == "a=0, b=1, kwargs={'d': 10}"
    f = sig(foo)  # decorate foo, returning a pointer to foo called f
    assert f(0, d=10) == "a=0, b=1, kwargs={'d': 10}"


def test_signature_equality_and_hashing():
    import pickle

    def foo(x, y, z=3):
        pass

    def bar(x, y, z=3):
        pass

    # different instances of (should be a) same signature
    ref_sig = Sig(foo)

    sigs = [
        ref_sig,
        Sig(foo),  # another instance of the same Sig(foo)
        Sig("(x, y, z=3)"),  # signature made explicitly from a string
        Sig("x") + Sig("(y, z=3)"),  # signature made from an add operation
        Sig(bar),  # different function, same signature
        pickle.loads(pickle.dumps(ref_sig)),  # un-pickled signature
    ]

    assert all(
        sig == ref_sig for sig in sigs
    ), "sigs should be equal from a == point of view"

    # Let's see that
    assert all(
        hash(sig) == hash(ref_sig) for sig in sigs
    ), "sigs should have the same hash"

    # ... and if that wasn't convincing, let's see how the sigs behave as dict keys:
    # If sigs were different, the following would add key-value pairs to the base dict.
    # But it doesn't. You always get the same one key (same signature) with different
    # values:
    t = {ref_sig: 0}
    assert t == {ref_sig: 0}
    t[sigs[2]] = 2
    assert t == {ref_sig: 2}  # same signature with new 2 value
    t[sigs[3]] = 3
    assert t == {ref_sig: 3}  # same signature with new 3 value

    # What if we just have a return annotation?
    def baz(x, y, z=3) -> None:
        pass

    assert Sig(baz) != ref_sig, "return annotation of baz should make it different"


def test_signature_of_partial():
    from functools import partial

    def foo(a, b, c=3) -> int:
        return a + b * c

    assert str(Sig(foo)) == "(a, b, c=3) -> int"
    assert str(Sig(partial(foo, 1))) == "(b, c=3) -> int"
    assert str(Sig(partial(foo, a=1, b=2))) == "(*, a=1, b=2, c=3) -> int"
    assert str(Sig(partial(foo, 1, 2))) == "(c=3) -> int"
    assert str(Sig(partial(foo, 1, b=2))) == "(*, b=2, c=3) -> int"


def test_some_edge_cases_of_sig():
    from operator import itemgetter, attrgetter, methodcaller

    assert Sig(itemgetter).names == ["item", "items"]
    assert Sig(attrgetter).names == ["attr", "attrs"]
    assert Sig(methodcaller).names == ["name", "args", "kwargs"]

    # These should now work consistently across Python 3.10 and 3.12
    assert Sig(itemgetter(1)).names == ["iterable"]
    assert Sig(itemgetter(1, 2)).names == ["iterable"]
    assert Sig(attrgetter("foo")).names == ["iterable"]
    assert Sig(attrgetter("foo", "bar")).names == ["iterable"]
    assert Sig(methodcaller("foo")).names == ["obj"]


def test_sig_wrap_edge_cases():
    """Tests some edge cases involving simultaneous changes of defaults and kinds.

    Current Sig.wrap design allows you to change defaults, and this will have the
    effect of changing the ``__defaults__`` and ``__kwdefaults__`` of the wrapped
    function.

    But, when we change the default of a parameter and change it's kind at the same
    time, bad things happen (see https://github.com/i2mint/i2/issues/16).

    So `Sig.wrap` makes a few checks and raises an error if it's thinks it's not
    safe to do the wrapping.

    Some of these facts may change with new designs. When this is the case,
    this test should be invalidated."""

    # Non edge-case tests:

    def foo(x, y, z=0):
        return x + y * z

    assert foo.__defaults__ == (0,)
    assert foo(1, 2) == 1

    @Sig(lambda x, y, z=3: None)
    def foo(x, y, z=0):
        return x + y * z

    assert Sig(foo)._defaults_ == (3,)

    assert foo(1, 2) == 7
    #    works because Sig also changed __defaults__:
    assert foo.__defaults__ == (3,)

    @Sig(lambda x, y, *, z=3: None)
    def foo(x, y, *, z=0):
        return x + y * z

    assert foo(1, 2) == 7
    #    works because Sig also changed __defaults__ and __kwdefaults__:
    assert foo.__defaults__ == ()
    assert foo.__kwdefaults__ == {"z": 3}
    assert Sig(foo)._defaults_ == ()
    assert Sig(foo)._kwdefaults_ == {"z": 3}

    # The following (where we go from a (same kind) param not having a default,
    # to having one, also work

    @Sig(lambda x, y, z=3: None)
    def foo(x, y, z):
        return x + y * z

    assert foo(1, 2) == 7
    assert foo(1, 2, z=10) == 21

    @Sig(lambda x, y, *, z=3: None)
    def foo(x, y, *, z):
        return x + y * z

    assert foo(1, 2) == 7
    assert foo(1, 2, z=10) == 21
    assert Sig(foo)._defaults_ == ()
    assert Sig(foo)._kwdefaults_ == {"z": 3}


def test_tuple_the_args():
    from i2.signatures import tuple_the_args

    def func(a, *args, bar):
        return trace_call(func, locals())

    assert func(1, 2, 3, bar=4) == "func(a=1, args=(2, 3), bar=4)"

    wfunc = tuple_the_args(func)

    # here, not that (1) args is specified as one iterable ([2, 3] instead of 2,
    # 3) and (2) the function name is the same as the wrapped (func)
    assert wfunc(1, [2, 3], bar=4) == "func(a=1, args=(2, 3), bar=4)"

    # See the func itself hasn't changed
    assert func(1, 2, 3, bar=4) == "func(a=1, args=(2, 3), bar=4)"

    assert str(Sig(func)) == "(a, *args, bar)"
    # See that args is now a PK kind with a default of (). Also, bar became KO.
    assert str(Sig(wfunc)) == "(a, args=(), *, bar)"

    # -----------------------------------------------------------------------------------
    # Let's see what happens when we give bar a default value

    def func2(a, *args, bar=10):
        return trace_call(func2, locals())

    wfunc = tuple_the_args(func2)
    assert wfunc(1, [2, 3]) == "func2(a=1, args=(2, 3), bar=10)"
    assert wfunc(1, [2, 3], bar=4) == "func2(a=1, args=(2, 3), bar=4)"

    # On the other hand, specifying bar as a positional won't work.
    # The reason is: args was a variadic, so everything after it should be KO or VK
    # The tuple_the_args doesn't change those signatures.
    #
    with pytest.raises(FuncCallNotMatchingSignature) as e_info:
        wfunc(1, [2, 3], 4)
        assert e_info.value == (
            "There should be only keyword arguments after the Variadic args. "
            "Function was called with (positional=(1, [2, 3], 4), keywords={})"
        )

    # pytest.raises()


@pytest.mark.xfail
def test_normalize_func_simply(function_normalizer=normalized_func):
    # -----------------------------------------------------------------------------------
    def p0113(po1, /, pk1, pk2, *, ko1):
        return f"{po1=}, {pk1=}, {pk2=}, {ko1=}"

    func = p0113
    po1, pk1, pk2, ko1 = 1, 2, 3, 4

    norm_func = function_normalizer(func)

    func_output = func(po1, pk1, pk2=pk2, ko1=ko1)

    norm_func_output = norm_func(po1, pk1, pk2, ko1)

    assert norm_func_output == func_output
    norm_func_output = norm_func(po1, pk1, pk2, ko1=ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1, pk1, pk2=pk2, ko1=ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1, pk1=pk1, pk2=pk2, ko1=ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1=po1, pk1=pk1, pk2=pk2, ko1=ko1)
    assert norm_func_output == func_output

    # -----------------------------------------------------------------------------------
    def p1234(pka, *vpa, koa, **vka):
        return f"{pka=}, {vpa=}, {koa=}, {vka=}"

    pka, vpa, koa, vka = 1, (2, 3), 4, {"a": "b", "c": "d"}

    func = p1234
    norm_func = function_normalizer(func)

    func_output = func(pka, *vpa, koa, **vka)
    norm_func_output = norm_func(pka, vpa, koa, vka)

    assert norm_func_output == func_output


# -----------------------------------------------------------------------------------


def p1234(pka, *vpa, koa, **vka):
    return f"{pka=}, {vpa=}, {koa=}, {vka=}"


@pytest.mark.xfail
def test_normalize_func_combinatorially(function_normalizer=normalized_func):
    # -----------------------------------------------------------------------------------
    def p0113(po1, /, pk1, pk2, *, ko1):
        return f"{po1=}, {pk1=}, {pk2=}, {ko1=}"

    func = p0113
    po1, pk1, pk2, ko1 = 1, 2, 3, 4

    poa = [po1]
    ppka, kpka = [pk1], {"pk2": pk2}
    vpa = []  # no VP argument
    koa = {"ko1": ko1}
    vka = {}  # no VK argument

    norm_func = function_normalizer(func)

    func_output = func(*poa, *ppka, *vpa, **kpka, **koa, **vka)
    norm_func_output = norm_func(po1, pk1, pk2, ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1, pk1, pk2, ko1=ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1, pk1, pk2=pk2, ko1=ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1, pk1=pk1, pk2=pk2, ko1=ko1)
    assert norm_func_output == func_output
    norm_func_output = norm_func(po1=po1, pk1=pk1, pk2=pk2, ko1=ko1)
    assert norm_func_output == func_output

    # -----------------------------------------------------------------------------------
    def p1234(pka, *vpa, koa, **vka):
        return f"{pka=}, {vpa=}, {koa=}, {vka=}"

    pka, vpa, koa, vka = 1, (2, 3), 4, {"a": "b", "c": "d"}

    func = p1234
    norm_func = function_normalizer(func)

    func_output = func(pka, *vpa, koa, **vka)
    norm_func_output = norm_func(pka, vpa, koa, vka)

    assert norm_func_output == func_output

    # -----------------------------------------------------------------------------------


# TODO: It seems in some cases, the better choice would be to oblige the user to deal
#  with return annotation explicitly


def mk_sig(
    obj: Union[Signature, Callable, Mapping, None] = None,
    return_annotations=empty,
    **annotations,
):
    """Convenience function to make a signature or inject annotations to an existing one.

    >>> s = mk_sig(lambda a, b, c=1, d='bar': ..., b=int, d=str)
    >>> s
    <Signature (a, b: int, c=1, d: str = 'bar')>
    >>> # showing that sig can take a signature input, and overwrite an existing annotation:
    >>> mk_sig(s, a=list, b=float)  # note the b=float
    <Signature (a: list, b: float, c=1, d: str = 'bar')>
    >>> mk_sig()
    <Signature ()>

    Trying to annotate an argument that doesn't exist will lead to an AssertionError:

    >>> mk_sig(lambda a, b=2, c=3: ..., d=int)  # doctest: +SKIP
    Traceback (most recent call last):
    ...
    AssertionError: These argument names weren't found in the signature: {'d'}
    """
    if obj is None:
        return Signature()
    if callable(obj):
        obj = Signature.from_callable(obj)  # get a signature object from a callable
    if isinstance(obj, Signature):
        obj = obj.parameters  # get the parameters attribute from a signature
    params = dict(obj)  # get a writable copy of parameters
    if not annotations:
        return Signature(params.values(), return_annotation=return_annotations)
    else:
        assert set(annotations) <= set(
            params
        ), f"These argument names weren't found in the signature: {set(annotations) - set(params)}"
        for name, annotation in annotations.items():
            p = params[name]
            params[name] = Parameter(
                name=name,
                kind=p.kind,
                default=p.default,
                annotation=annotation,
            )
        return Signature(params.values(), return_annotation=return_annotations)


def mk_signature(parameters, *, return_annotation=empty, __validate_parameters__=True):
    """Make an inspect.Signature object with less boilerplate verbosity.
    Args:
        signature: A list of parameter specifications. This could be an inspect.Parameter object or anything that
            the mk_param function can resolve into an inspect.Parameter object.
        return_annotation: Passed on to inspect.Signature.
        __validate_parameters__: Passed on to inspect.Signature.

    Returns:
        An inspect.Signature object

    # >>> mk_signature(['a', 'b', 'c'])
    # <Signature (a, b, c)>
    # >>> mk_signature(['a', ('b', None), ('c', 42, int)])  # specifying defaults and annotations
    # <Signature (a, b=None, c: int = 42)>
    # >>> import inspect
    # >>> mk_signature(['a', ('b', inspect._empty, int)])  # specifying an annotation without a default
    # <Signature (a, b: int)>
    # >>> mk_signature(['a', 'b', 'c'], return_annotation=str)  # specifying return annotation
    # <Signature (a, b, c) -> str>
    # >>>
    # >>> # But you can always specify parameters the "long" way
    # >>> mk_signature([inspect.Parameter(name='kws', kind=inspect.Parameter.VAR_KEYWORD)], return_annotation=str)
    # <Signature (**kws) -> str>
    # >>>
    # >>> # Note that mk_signature is an inverse of signature_to_dict:
    # >>> def foo(a, b: int=0, c=None) -> int: ...
    # >>> sig_foo = signature(foo)
    # >>> assert mk_signature(**signature_to_dict(sig_foo)) == sig_foo

    """
    return Sig(parameters, return_annotation=return_annotation)


# PATTERN: tree crud pattern
def signature_to_dict(sig: Signature):
    # warn("Use Sig instead", DeprecationWarning)
    # return Sig(sig).to_simple_signature()
    return {
        "parameters": sig.parameters,
        "return_annotation": sig.return_annotation,
    }


def _merge_sig_dicts(sig1_dict, sig2_dict):
    """Merge two signature dicts. A in dict.update(sig1_dict, **sig2_dict),
    but specialized for signature dicts.
    If sig1_dict and sig2_dict both define a parameter or return annotation,
    sig2_dict decides on what the output is.
    """
    return {
        "parameters": dict(sig1_dict["parameters"], **sig2_dict["parameters"]),
        "return_annotation": sig2_dict["return_annotation"]
        or sig1_dict["return_annotation"],
    }


def _merge_signatures(sig1, sig2):
    """Get the merged signatures of two signatures (sig2 is the final decider of conflics)

    >>> def foo(a='a', b: int=0, c=None) -> int: ...
    >>> def bar(b: float=0.0, d: str='hi') -> float: ...
    >>> foo_sig = signature(foo)
    >>> bar_sig = signature(bar)
    >>> foo_sig
    <Signature (a='a', b: int = 0, c=None) -> int>
    >>> bar_sig
    <Signature (b: float = 0.0, d: str = 'hi') -> float>
    >>> _merge_signatures(foo_sig, bar_sig)
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi') -> float>
    >>> _merge_signatures(bar_sig, foo_sig)
    <Signature (b: int = 0, d: str = 'hi', a='a', c=None) -> int>
    """
    # sig1_dict = Sig(sig1).to_simple_signature()
    # sig1_dict = signature_to_dict(sig1)
    # # remove variadic kinds from sig1
    # sig1_dict['parameters'] = {k: v for k, v in sig1_dict['parameters'].items() if v.kind not in var_param_kinds}
    # return Sig(**_merge_sig_dicts(sig1_dict, Sig(sig2).to_simple_dict()))
    sig1_dict = signature_to_dict(sig1)
    # remove variadic kinds from sig1
    sig1_dict["parameters"] = {
        k: v
        for k, v in sig1_dict["parameters"].items()
        if v.kind not in var_param_kinds
    }
    kws = _merge_sig_dicts(sig1_dict, signature_to_dict(sig2))
    kws["obj"] = kws.pop("parameters")
    return Sig(**kws).to_simple_signature()


def _merge_signatures_of_funcs(func1, func2):
    """Get the merged signatures of two functions (func2 is the final decider of conflics)

    >>> def foo(a='a', b: int=0, c=None) -> int: ...
    >>> def bar(b: float=0.0, d: str='hi') -> float: ...
    >>> _merge_signatures_of_funcs(foo, bar)
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi') -> float>
    >>> _merge_signatures_of_funcs(bar, foo)
    <Signature (b: int = 0, d: str = 'hi', a='a', c=None) -> int>
    """
    return _merge_signatures(signature(func1), signature(func2))


def _merged_signatures_of_func_list(funcs, return_annotation: Any = empty):
    """

    >>> def foo(a='a', b: int=0, c=None) -> int: ...
    >>> def bar(b: float=0.0, d: str='hi') -> float: ...
    >>> def hello(x: str='hi', y=1) -> str: ...
    >>>
    >>> # Test how the order of the functions affect the order of the parameters
    >>> _merged_signatures_of_func_list([foo, bar, hello])
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi', x: str = 'hi', y=1)>
    >>> _merged_signatures_of_func_list([hello, foo, bar])
    <Signature (x: str = 'hi', y=1, a='a', b: float = 0.0, c=None, d: str = 'hi')>
    >>> _merged_signatures_of_func_list([foo, bar, hello])
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi', x: str = 'hi', y=1)>
    >>>
    >>> # Test the return_annotation argument
    >>> _merged_signatures_of_func_list([foo, bar], list)  # specifying that the return type is a list
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi') -> list>
    >>> _merged_signatures_of_func_list([foo, bar], foo)  # specifying that the return type is a list
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi') -> int>
    >>> _merged_signatures_of_func_list([foo, bar], bar)  # specifying that the return type is a list
    <Signature (a='a', b: float = 0.0, c=None, d: str = 'hi') -> float>
    """

    s = reduce(_merge_signatures, map(signature, funcs))
    # s = Sig.from_objs(*funcs).to_simple_signature()

    if (
        return_annotation in funcs
    ):  # then you want the return annotation of a specific func of funcs
        return_annotation = signature(return_annotation).return_annotation

    return s.replace(return_annotation=return_annotation)


# TODO: will we need more options for the priority argument? Like position?
def update_signature_with_signatures_from_funcs(*funcs, priority: str = "last"):
    """Make a decorator that will merge the signatures of given funcs to the signature of the wrapped func.
    By default, the funcs signatures will be placed last, but can be given priority by asking priority = 'first'

    >>> def foo(a='a', b: int=0, c=None) -> int: ...
    >>> def bar(b: float=0.0, d: str='hi') -> float: ...
    >>> def something(y=(1, 2)): ...
    >>> def another(y=10): ...
    >>> @update_signature_with_signatures_from_funcs(foo, bar)
    ... def hello(x: str='hi', y=1) -> str:
    ...     pass
    >>> signature(hello)
    <Signature (x: str = 'hi', y=1, a='a', b: float = 0.0, c=None, d: str = 'hi')>
    >>>
    >>> # Try a different order and priority == 'first'. Notice the b arg type and default!
    >>> add_foobar_to_signature_first = update_signature_with_signatures_from_funcs(
    ...     bar, foo, priority='first'
    ... )
    >>> bar_foo_something = add_foobar_to_signature_first(something)
    >>> signature(bar_foo_something)
    <Signature (b: int = 0, d: str = 'hi', a='a', c=None, y=(1, 2))>
    >>> # See how you can reuse the decorator several times
    >>> bar_foo_another = add_foobar_to_signature_first(another)
    >>> signature(bar_foo_another)
    <Signature (b: int = 0, d: str = 'hi', a='a', c=None, y=10)>
    """
    if not isinstance(priority, str):
        raise TypeError("priority should be a string")

    if priority == "last":

        def transform_signature(func):
            # func.__signature__ = Sig.from_objs(func, *funcs).to_simple_signature()
            func.__signature__ = _merged_signatures_of_func_list([func] + list(funcs))
            return func

    elif priority == "first":

        def transform_signature(func):
            # func.__signature__ = Sig.from_objs(*funcs, func).to_simple_signature()
            func.__signature__ = _merged_signatures_of_func_list(list(funcs) + [func])
            return func

    else:
        raise ValueError("priority should be 'last' or 'first'")

    return transform_signature


@pytest.mark.parametrize(
    "sig_spec",
    [
        ("(po, /)"),
        ("(po=0, /)"),
        ("(pk)"),
        ("(pk=0)"),
        ("(*, ko)"),
        ("(*, ko=0)"),
        ("(po, /, pk, *, ko)"),
        ("(po=0, /, pk=0, *, ko=0)"),
        ("(*args)"),
        ("(**kwargs)"),
        ("(*args, **kwargs)"),
        ("(po, /, pk, *args, ko)"),
        ("(po=0, /, pk=0, *args, ko=0)"),
        ("(po, /, pk, *, ko, **kwargs)"),
        ("(po=0, /, pk=0, *, ko=0, **kwargs)"),
        ("(po, /, pk, *args, ko, **kwargs)"),
        ("(po=0, /, pk=0, *args, ko=0, **kwargs)"),
        ("(po1, po2, /)"),
        ("(pk1, pk2)"),
        ("(*, ko1, ko2)"),
        ("(po1, po2, /, pk1, pk2, *, ko1, ko2)"),
        ("(po1, po2, /, pk1, pk2, *args, ko1, ko2, **kwargs)"),
        ("(po1=0, po2=0, /, pk1=0, pk2=0, *args, ko1=0, ko2=0, **kwargs)"),
    ],
)
def test_call_forgivingly(sig_spec):
    sig = Sig(sig_spec)

    @sig
    def foo(*args, **kwargs):
        return args, kwargs

    def validate_call_forgivingly(*args, **kwargs):
        expected_output_kwargs = (
            kwargs
            if VK in sig.kinds.values()
            else {k: v for k, v in kwargs.items() if k in sig}
        )
        pk_in_kwargs_count = sum(
            kind == PK
            for param, kind in sig.kinds.items()
            if param in expected_output_kwargs
        )
        expected_output_args_count = (
            len(args)
            if VP in sig.kinds.values()
            else sum(kind <= PK for kind in sig.kinds.values()) - pk_in_kwargs_count
        )
        expected_output_args = args[:expected_output_args_count]
        expected_output = (expected_output_args, expected_output_kwargs)
        output = call_forgivingly(foo, *args, **kwargs)
        print()
        print(args, kwargs)
        print(expected_output)
        print(output)
        assert output == expected_output

    for args, kwargs in sig_to_inputs(sig, variadics_source=((), {})):
        kwargs = dict(kwargs, **{"some": "extra", "added": "kwargs"})
        po_pk_count = sum(kind <= PK for kind in sig.kinds.values())
        if len(args) == po_pk_count:
            args = args + ("some", "extra", "args")

        validate_call_forgivingly(*args, **kwargs)


@pytest.mark.parametrize(
    "sig_spec1, sig_spec2",
    [
        ("()", "(a)"),
        ("()", "(a=0)"),
        ("(a, /, *, c)", "(a, /, b, *, c)"),
        ("(a, /, *, c)", "(a, /, b=0, *, c)"),
        ("(a, /, b)", "(a, /, b, *, c)"),
        ("(a, /, b)", "(a, /, b, *, c=0)"),
        ("(a, /, b, *, c)", "(*args)"),
        ("(a, /, b, *, c)", "(**kwargs)"),
        ("(a, /, b, *, c)", "(*args, **kwargs)"),
        ("(a, /, b, *, c)", "(a, /, b, *args, c, **kwargs)"),
        ("(a, /, b, *, c)", "(a, b, c)"),
        ("(a, /, b, *, c)", "(a, b, /, *, c)"),
        ("(a, /, b, *, c)", "(a, /, *, b, c)"),
        ("(a, /, b, *, c)", "(a, b, /, c)"),
        ("(a, /, b, *, c)", "(a, *, b, c)"),
        (
            "(a, /, b, *, c)",
            "(x, /, b, *, c)",
        ),
        (
            "(a, /, b, *, c)",
            "(a, /, x, *, c)",
        ),
        (
            "(a, /, b, *, c)",
            "(a, /, b, *, x)",
        ),
        ("(a, /, b, *, c)", "(a=0, b=0, c=0)"),
        ("(a=0, /, b=0, *, c=0)", "(a, b, c)"),  # TODO: See if this should pass
        ("(a, b, /, c, d, *, e, f)", "(b, a, /, d, c, *, f, e)"),
        (
            "(a, b, /, c, d, *, e, f)",
            "(a, c, /, b, e, *, d, f)",
        ),
        ("()", "(*args)"),
        ("()", "(**kwargs)"),
        ("()", "(*args, **kwargs)"),
        ("(*args)", "()"),
        ("(*args)", "(**kwargs)"),
        ("(*args)", "(*args, **kwargs)"),
        ("(**kwargs)", "()"),
        ("(**kwargs)", "(*args)"),
        ("(**kwargs)", "(*args, **kwargs)"),
        ("(*args, **kwargs)", "(*args, **kwargs)"),
    ],
)
def test_call_compatibility(sig_spec1, sig_spec2):
    sig1 = Sig(sig_spec1)
    sig2 = Sig(sig_spec2)
    is_compatible = sig1.is_call_compatible_with(sig2)

    exec_env = dict()
    f_def = f"def f{sig_spec2}: pass"
    exec(f_def, exec_env)
    foo = exec_env["f"]

    # @sig2
    # def foo(*args, **kwargs):
    #     pass

    pos1, pks1, vp1, kos1, vk1 = sig1.detail_names_by_kind()
    for args, kwargs in sig_to_inputs(sig1, variadics_source=((), {})):
        if vp1 is not None and len(args) == len(pos1) + len(pks1):
            args += ("extra_arg",)
        if vk1 is not None:
            kwargs["extra_kwarg_key"] = "extra_kwarg_value"
        try:
            foo(*args, **kwargs)
        except TypeError:
            if is_compatible:
                raise
            else:
                return
    assert (
        is_compatible
    ), f"sig1 is not compatible with sig2, when it should: {sig1} and {sig2}"


def test_bool():
    name = "bool"

    sig = Sig(sigs_for_sigless_builtin_name[name])

    assert function_is_compatible_with_signature(bool, sig)


def visualize_errors_for_function_call(func, sig):
    """
    Calls func on sig_to_inputs and prints error if any
    """

    print(f"============================================================{str(func)}")
    for args, kwargs in sig_to_inputs(sig):
        result = call_and_return_error(func, *args, **kwargs)
        if result is not None:
            print(f"{args=} , {kwargs=}")
            print(result)


def test_sigless_builtins():
    from operator import itemgetter, attrgetter, methodcaller

    mapping_methods = {
        "__eq__",
        "__ne__",
        "__iter__",
        "__getitem__",
        "__len__",
        "__contains__",
        "__setitem__",
        "__delitem__",
    }
    special_cases = {"breakpoint"} | mapping_methods

    for name in sigs_for_sigless_builtin_name:
        # removed breakpoint as it triggers a pdb session
        if name in special_cases:
            continue
        sig = Sig(sigs_for_sigless_builtin_name[name])
        assert function_is_compatible_with_signature(eval(name), sig)

    d = {"a": 1, "b": 2}
    for name in mapping_methods:
        method = getattr(d, name)
        assert function_is_compatible_with_signature(method, Sig(method))


@pytest.mark.parametrize(
    "sig_spec, args, kwargs, map_arguments_kwargs, expected_output",
    [
        # ------------------------------------------------------------------------------
        # PO
        # ------------------------------------------------------------------------------
        ("(a, /)", (1,), None, None, {"a": 1}),
        ("(a, /)", None, None, None, (TypeError, "missing.*required.*argument: 'a'")),
        ("(a, /)", None, None, dict(allow_partial=True), {}),
        ("(a=0, /)", None, None, dict(apply_defaults=True), {"a": 0}),
        (
            "(a=0, /)",
            None,
            None,
            dict(allow_partial=True, apply_defaults=True),
            {"a": 0},
        ),
        ("(a, /)", (1, 2), None, None, (TypeError, "too many positional arguments")),
        ("(a, /)", (1, 2), None, dict(allow_excess=True), {"a": 1}),
        (
            "(a, /)",
            (1,),
            {"b": 2},
            None,
            (TypeError, "got an unexpected keyword argument 'b'"),
        ),
        ("(a, /)", (1,), {"b": 2}, dict(allow_excess=True), {"a": 1}),
        # COMMENTED OUT DUE TO PYTHON 3.12 ERROR MESSAGE CHANGE
        # (
        #     "(a, /)",
        #     None,
        #     {"a": 1},
        #     None,
        #     (
        #         TypeError,
        #         "'a' parameter is positional only, but was passed as a keyword",
        #     ),
        # ),
        ("(a, /)", None, {"a": 1}, dict(ignore_kind=True), {"a": 1}),
        (
            "(a, /)",
            None,
            {"a": 1, "b": 2},
            dict(allow_excess=True, ignore_kind=True),
            {"a": 1},
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PK
        # ------------------------------------------------------------------------------
        ("(a)", (1,), None, None, {"a": 1}),
        ("(a)", None, {"a": 1}, None, {"a": 1}),
        ("(a)", None, None, None, (TypeError, "missing.*required.*argument: 'a'")),
        ("(a)", None, None, dict(allow_partial=True), {}),
        ("(a=0)", None, None, dict(apply_defaults=True), {"a": 0}),
        ("(a=0)", None, None, dict(apply_defaults=True, allow_partial=True), {"a": 0}),
        ("(a)", (1, 2), None, None, (TypeError, "too many positional arguments")),
        ("(a)", (1, 2), None, dict(allow_excess=True), {"a": 1}),
        (
            "(a)",
            (1,),
            {"b": 2},
            None,
            (TypeError, "got an unexpected keyword argument 'b'"),
        ),
        ("(a)", (1,), {"b": 2}, dict(allow_excess=True), {"a": 1}),
        (
            "(a)",
            None,
            {"a": 1, "b": 2},
            None,
            (TypeError, "got an unexpected keyword argument 'b'"),
        ),
        ("(a)", None, {"a": 1, "b": 2}, dict(allow_excess=True), {"a": 1}),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # VP
        # ------------------------------------------------------------------------------
        ("(*args)", (1, 2), None, None, {"args": (1, 2)}),
        ("(*args)", None, None, None, {}),
        ("(*args)", None, None, dict(allow_partial=True), {}),
        ("(*args)", None, None, dict(apply_defaults=True), {"args": ()}),
        (
            "(*args)",
            None,
            None,
            dict(allow_partial=True, apply_defaults=True),
            {"args": ()},
        ),
        ("(*args)", (1, 2), None, dict(allow_excess=True), {"args": (1, 2)}),
        (
            "(*args)",
            (1, 2),
            {"a": 1, "b": 2},
            None,
            (TypeError, "got an unexpected keyword argument 'a'"),
        ),
        (
            "(*args)",
            (1, 2),
            {"a": 1, "b": 2},
            dict(allow_excess=True),
            {"args": (1, 2)},
        ),
        ("(*args)", ((1, 2),), None, None, {"args": ((1, 2),)}),
        ("(*args)", ((1, 2),), None, dict(ignore_kind=True), {"args": (1, 2)}),
        (
            "(*args)",
            None,
            {"args": (1, 2)},
            None,
            (TypeError, "got an unexpected keyword argument 'args'"),
        ),
        ("(*args)", None, {"args": (1, 2)}, dict(ignore_kind=True), {"args": (1, 2)}),
        (
            "(*args)",
            ((1, 2), 3),
            {"a": 4, "b": 5},
            dict(allow_excess=True, ignore_kind=True),
            {"args": (1, 2)},
        ),
        (
            "(*args)",
            (1, 2),
            {"args": (1, 2)},
            dict(allow_excess=True, ignore_kind=True),
            (TypeError, "multiple values for argument 'args'"),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # KO
        # def foo(*, a): ...
        # ------------------------------------------------------------------------------
        ("(*, a)", None, {"a": 1}, None, {"a": 1}),
        ("(*, a)", None, None, None, (TypeError, "missing a required.*argument: 'a'")),
        ("(*, a)", None, None, dict(allow_partial=True), {}),
        ("(*, a=0)", None, None, dict(apply_defaults=True), {"a": 0}),
        (
            "(*, a=0)",
            None,
            None,
            dict(allow_partial=True, apply_defaults=True),
            {"a": 0},
        ),
        ("(*, a)", (2,), {"a": 1}, None, (TypeError, "too many positional arguments")),
        ("(*, a)", (2,), {"a": 1}, dict(allow_excess=True), {"a": 1}),
        (
            "(*, a)",
            None,
            {"a": 1, "b": 2},
            None,
            (TypeError, "got an unexpected keyword argument 'b'"),
        ),
        ("(*, a)", None, {"a": 1, "b": 2}, dict(allow_excess=True), {"a": 1}),
        ("(*, a)", (1,), None, None, (TypeError, "too many positional arguments")),
        ("(*, a)", (1,), None, dict(ignore_kind=True), {"a": 1}),
        ("(*, a)", (1, 2), None, dict(allow_excess=True, ignore_kind=True), {"a": 1}),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # VK
        # def foo(**kwargs): ...
        # ------------------------------------------------------------------------------
        ("(**kwargs)", None, {"a": 1, "b": 2}, None, {"kwargs": {"a": 1, "b": 2}}),
        ("(**kwargs)", None, None, None, {}),
        ("(**kwargs)", None, None, dict(allow_partial=True), {}),
        ("(**kwargs)", None, None, dict(apply_defaults=True), {"kwargs": {}}),
        (
            "(**kwargs)",
            None,
            None,
            dict(allow_partial=True, apply_defaults=True),
            {"kwargs": {}},
        ),
        (
            "(**kwargs)",
            None,
            {"a": 1, "b": 2},
            dict(allow_excess=True),
            {"kwargs": {"a": 1, "b": 2}},
        ),
        (
            "(**kwargs)",
            (1, 2),
            {"a": 1, "b": 2},
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(**kwargs)",
            (1, 2),
            {"a": 1, "b": 2},
            dict(allow_excess=True),
            {"kwargs": {"a": 1, "b": 2}},
        ),
        (
            "(**kwargs)",
            None,
            {"kwargs": {"a": 1, "b": 2}},
            None,
            {"kwargs": {"kwargs": {"a": 1, "b": 2}}},
        ),
        (
            "(**kwargs)",
            None,
            {"kwargs": {"a": 1, "b": 2}},
            dict(ignore_kind=True),
            {"kwargs": {"a": 1, "b": 2}},
        ),
        (
            "(**kwargs)",
            ({"a": 1, "b": 2},),
            None,
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(**kwargs)",
            ({"a": 1, "b": 2},),
            None,
            dict(ignore_kind=True),
            {"kwargs": {"a": 1, "b": 2}},
        ),
        (
            "(**kwargs)",
            ({"a": 1, "b": 2},),
            {"kwargs": {"a": 1, "b": 2}},
            dict(allow_excess=True, ignore_kind=True),
            (TypeError, "multiple values for argument 'kwargs'"),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + KO
        # def foo(a, /, b, *, c): ...
        # ------------------------------------------------------------------------------
        ("(a, /, b, *, c)", (1, 2), {"c": 3}, None, {"a": 1, "b": 2, "c": 3}),
        ("(a, /, b, *, c)", (1,), {"b": 2, "c": 3}, None, {"a": 1, "b": 2, "c": 3}),
        (
            "(a, /, b, *, c)",
            (1, 2),
            {"b": 2, "c": 3},
            None,
            (TypeError, "multiple values for argument 'b'"),
        ),
        (
            "(a, /, b, *, c)",
            None,
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        (
            "(a, /, b, *, c)",
            (1,),
            None,
            None,
            (TypeError, "missing.*required.*argument: 'b'"),
        ),
        (
            "(a, /, b, *, c)",
            (1, 2),
            None,
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        (
            "(a, /, b, *, c)",
            (1,),
            {"b": 2},
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        ("(a, /, b, *, c)", None, None, dict(allow_partial=True), {}),
        ("(a, /, b, *, c)", (1,), None, dict(allow_partial=True), {"a": 1}),
        ("(a, /, b, *, c)", (1, 2), None, dict(allow_partial=True), {"a": 1, "b": 2}),
        ("(a, /, b, *, c)", (1,), {"b": 2}, dict(allow_partial=True), {"a": 1, "b": 2}),
        (
            "(a=0, /, b=0, *, c=0)",
            None,
            None,
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "c": 0},
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            (1,),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 0, "c": 0},
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            (1, 2),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "c": 0},
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            (1,),
            {"b": 2},
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "c": 0},
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            None,
            {"b": 2},
            dict(apply_defaults=True),
            {"a": 0, "b": 2, "c": 0},
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            None,
            {"c": 3},
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "c": 3},
        ),
        (
            "(a, /, b, *, c)",
            (1, 2, 3),
            {"c": 3},
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c)",
            (1, 2, 3),
            {"c": 3},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        (
            "(a, /, b, *, c)",
            (1, 2),
            {"c": 3, "d": 4},
            None,
            (TypeError, "got an unexpected keyword argument 'd'"),
        ),
        (
            "(a, /, b, *, c)",
            (1, 2),
            {"c": 3, "d": 4},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        (
            "(a, /, b, *, c)",
            (1, 2, 3),
            {"c": 3, "d": 4},
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c)",
            (1, 2, 3),
            {"c": 3, "d": 4},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        (
            "(a, /, b, *, c)",
            (1, 2),
            {"b": 2, "c": 3},
            dict(allow_excess=True),
            (TypeError, "multiple values for argument 'b'"),
        ),
        (
            "(a, /, b, *, c)",
            (1, 2, 3),
            None,
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c)",
            (1, 2, 3),
            None,
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        # COMMENTED OUT DUE TO PYTHON 3.12 ERROR MESSAGE CHANGE
        # (
        #     "(a, /, b, *, c)",
        #     None,
        #     {"a": 1, "b": 2, "c": 3},
        #     None,
        #     (
        #         TypeError,
        #         "'a' parameter is positional only, but was passed as a keyword",
        #     ),
        # ),
        (
            "(a, /, b, *, c)",
            None,
            {"a": 1, "b": 2, "c": 3},
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        (
            "(a, /, b, *, c)",
            None,
            {"a": 1, "b": 2, "c": 3},
            dict(allow_excess=True, ignore_kind=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # VP + VK
        # def foo(*args, **kwargs): ...
        # ------------------------------------------------------------------------------
        (
            "(*args, **kwargs)",
            (1, 2, 3),
            {"a": 1, "b": 2, "c": 3},
            None,
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        ("(*args, **kwargs)", (1, 2, 3), None, None, {"args": (1, 2, 3)}),
        (
            "(*args, **kwargs)",
            None,
            {"a": 1, "b": 2, "c": 3},
            None,
            {"kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        ("(*args, **kwargs)", None, None, None, {}),
        ("(*args, **kwargs)", None, None, dict(allow_partial=True), {}),
        (
            "(*args, **kwargs)",
            None,
            None,
            dict(apply_defaults=True),
            {"args": (), "kwargs": {}},
        ),
        (
            "(*args, **kwargs)",
            None,
            None,
            dict(allow_partial=True, apply_defaults=True),
            {"args": (), "kwargs": {}},
        ),
        (
            "(*args, **kwargs)",
            (1, 2, 3),
            {"a": 1, "b": 2, "c": 3},
            dict(allow_excess=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        (
            "(*args, **kwargs)",
            ((1, 2, 3), {"a": 1, "b": 2, "c": 3}),
            None,
            dict(ignore_kind=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        (
            "(*args, **kwargs)",
            ((1, 2, 3),),
            {"kwargs": {"a": 1, "b": 2, "c": 3}},
            dict(ignore_kind=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        (
            "(*args, **kwargs)",
            None,
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
            dict(ignore_kind=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        (
            "(*args, **kwargs)",
            ((1, 2, 3), {"a": 1, "b": 2, "c": 3}),
            None,
            dict(allow_excess=True, ignore_kind=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        (
            "(*args, **kwargs)",
            ((1, 2, 3),),
            {"kwargs": {"a": 1, "b": 2, "c": 3}},
            dict(allow_excess=True, ignore_kind=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        (
            "(*args, **kwargs)",
            None,
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
            dict(allow_excess=True, ignore_kind=True),
            {"args": (1, 2, 3), "kwargs": {"a": 1, "b": 2, "c": 3}},
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + VP + KO
        # def foo(a, /, b, *args, c): ...
        # ------------------------------------------------------------------------------
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4),
            {"c": 5},
            None,
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4),
            {"b": 2, "c": 5},
            None,
            (TypeError, "multiple values for argument 'b'"),
        ),
        (
            "(a, /, b, *args, c)",
            None,
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        (
            "(a, /, b, *args, c)",
            (1,),
            None,
            None,
            (TypeError, "missing.*required.*argument: 'b'"),
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2),
            None,
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4),
            None,
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        (
            "(a, /, b, *args, c)",
            (1,),
            {"b": 2},
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        ("(a, /, b, *args, c)", None, None, dict(allow_partial=True), {}),
        ("(a, /, b, *args, c)", (1,), None, dict(allow_partial=True), {"a": 1}),
        (
            "(a, /, b, *args, c)",
            (1, 2),
            None,
            dict(allow_partial=True),
            {"a": 1, "b": 2},
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4),
            None,
            dict(allow_partial=True),
            {"a": 1, "b": 2, "args": (3, 4)},
        ),
        (
            "(a, /, b, *args, c)",
            (1,),
            {"b": 2},
            dict(allow_partial=True),
            {"a": 1, "b": 2},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            None,
            None,
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "args": (), "c": 0},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            (1,),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 0, "args": (), "c": 0},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            (1, 2),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "args": (), "c": 0},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            (1, 2, 3, 4),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 0},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            (1,),
            {"b": 2},
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "args": (), "c": 0},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            None,
            {"b": 2},
            dict(apply_defaults=True),
            {"a": 0, "b": 2, "args": (), "c": 0},
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            None,
            {"c": 3},
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "args": (), "c": 3},
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4, 5),
            {"c": 5},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "args": (3, 4, 5), "c": 5},
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4),
            {"c": 5, "d": 6},
            None,
            (TypeError, "got an unexpected keyword argument 'd'"),
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, 3, 4),
            {"c": 5, "d": 6},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
        ),
        (
            "(a, /, b, *args, c)",
            (1, 2, (3, 4), 5),
            None,
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
        ),
        # COMMENTED OUT DUE TO PYTHON 3.12 ERROR MESSAGE CHANGE
        # (
        #     "(a, /, b, *args, c)",
        #     (),
        #     {"a": 1, "b": 2, "args": (3, 4), "c": 5},
        #     None,
        #     (
        #         TypeError,
        #         "'a' parameter is positional only, but was passed as a keyword",
        #     ),
        # ),
        (
            "(a, /, b, *args, c)",
            (),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
        ),
        (
            "(a, /, b, *args, c)",
            (),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "d": 6},
            dict(allow_excess=True, ignore_kind=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
        ),
        (
            "(a, /, b, *args, c)",
            (3, 4),
            {"a": 1, "b": 2, "c": 5},
            None,
            (TypeError, "multiple values for argument 'b'"),
        ),
        (
            "(a, /, b, *args, c)",
            (3, 4),
            {"a": 1, "b": 2, "c": 5},
            dict(ignore_kind=True),
            (TypeError, "multiple values for argument 'a'"),
        ),
        (
            "(a, /, b, *args, c)",
            (3, 4),
            {"a": 1, "b": 2, "c": 5},
            dict(allow_excess=True, ignore_kind=True),
            (TypeError, "multiple values for argument 'a'"),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + KO + VK
        # def foo(a, /, b, *, c, **kwargs): ...
        # ------------------------------------------------------------------------------
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            {"c": 3, "d": 4, "e": 5},
            None,
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"b": 2, "c": 3, "d": 4, "e": 5},
            None,
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            None,
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            None,
            None,
            (TypeError, "missing.*required.*argument: 'b'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            None,
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"b": 2},
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            None,
            {"d": 4, "e": 5},
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"d": 4, "e": 5},
            None,
            (TypeError, "missing.*required.*argument: 'b'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            {"d": 4, "e": 5},
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"b": 2, "d": 4, "e": 5},
            None,
            (TypeError, "missing a required.*argument: 'c'"),
        ),
        ("(a, /, b, *, c, **kwargs)", None, None, dict(allow_partial=True), {}),
        ("(a, /, b, *, c, **kwargs)", (1,), None, dict(allow_partial=True), {"a": 1}),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            None,
            dict(allow_partial=True),
            {"a": 1, "b": 2},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"b": 2},
            dict(allow_partial=True),
            {"a": 1, "b": 2},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            None,
            {"d": 4, "e": 5},
            dict(allow_partial=True),
            {"kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"d": 4, "e": 5},
            dict(allow_partial=True),
            {"a": 1, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            {"d": 4, "e": 5},
            dict(allow_partial=True),
            {"a": 1, "b": 2, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1,),
            {"b": 2, "d": 4, "e": 5},
            dict(allow_partial=True),
            {"a": 1, "b": 2, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            None,
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "c": 0, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            (1,),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 0, "c": 0, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            (1, 2),
            None,
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "c": 0, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            (1,),
            {"b": 2},
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "c": 0, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            {"b": 2},
            dict(apply_defaults=True),
            {"a": 0, "b": 2, "c": 0, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            {"c": 3},
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "c": 3, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            {"d": 4, "e": 5},
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "c": 0, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            (1,),
            {"d": 4, "e": 5},
            dict(apply_defaults=True),
            {"a": 1, "b": 0, "c": 0, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            (1, 2),
            {"d": 4, "e": 5},
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "c": 0, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            (1,),
            {"b": 2, "d": 4, "e": 5},
            dict(apply_defaults=True),
            {"a": 1, "b": 2, "c": 0, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            {"b": 2, "d": 4, "e": 5},
            dict(apply_defaults=True),
            {"a": 0, "b": 2, "c": 0, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            {"c": 3, "d": 4, "e": 5},
            dict(apply_defaults=True),
            {"a": 0, "b": 0, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3),
            {"c": 3},
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3),
            {"c": 3},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "c": 3},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            {"c": 3, "d": 4},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3),
            {"c": 3, "d": 4},
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3),
            {"c": 3, "d": 4},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            {"b": 2, "c": 3},
            dict(allow_excess=True),
            (TypeError, "multiple values for argument 'b'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2),
            {"b": 2, "c": 3, "d": 4, "e": 5},
            dict(allow_excess=True),
            (TypeError, "multiple values for argument 'b'"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3, {"d": 4, "e": 5}),
            None,
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3, {"d": 4, "e": 5}),
            None,
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        # COMMENTED OUT DUE TO PYTHON 3.12 ERROR MESSAGE CHANGE
        # (
        #     "(a, /, b, *, c, **kwargs)",
        #     None,
        #     {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        #     None,
        #     (
        #         TypeError,
        #         "'a' parameter is positional only, but was passed as a keyword",
        #     ),
        # ),
        (
            "(a, /, b, *, c, **kwargs)",
            None,
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            None,
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}, "f": 6},
            dict(allow_excess=True, ignore_kind=True),
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3),
            {"kwargs": {"d": 4, "e": 5}},
            None,
            (TypeError, "too many positional arguments"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            (1, 2, 3),
            {"kwargs": {"d": 4, "e": 5}},
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + VP + KO + VK
        # def foo(a, /, b, *args, c, **kwargs): ...
        # ------------------------------------------------------------------------------
        (
            "(a, /, b, *args, c, **kwargs)",
            (1, 2, 3, 4),
            {"c": 5, "d": 6, "e": 7},
            None,
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            (1,),
            {"b": 2, "c": 5, "d": 6, "e": 7},
            None,
            {"a": 1, "b": 2, "c": 5, "kwargs": {"d": 6, "e": 7}},
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            (1, 2),
            {"c": 5},
            None,
            {"a": 1, "b": 2, "c": 5},
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            None,
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        ("(a, /, b, *args, c, **kwargs)", None, None, dict(allow_partial=True), {}),
        (
            "(a=0, /, b=0, *args, c=0, **kwargs)",
            None,
            None,
            dict(apply_defaults=True),
            {"a": 0, "args": (), "b": 0, "c": 0, "kwargs": {}},
        ),
        (
            "(a=0, /, b=0, *args, c=0, **kwargs)",
            None,
            None,
            dict(apply_defaults=True, allow_partial=True),
            {"a": 0, "args": (), "b": 0, "c": 0, "kwargs": {}},
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            (1, 2, 3, 4),
            {"c": 5, "d": 6, "e": 7},
            dict(allow_excess=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
        ),
        # COMMENTED OUT DUE TO PYTHON 3.12 ERROR MESSAGE CHANGE
        # (
        #     "(a, /, b, *args, c, **kwargs)",
        #     None,
        #     {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
        #     None,
        #     (
        #         TypeError,
        #         "'a' parameter is positional only, but was passed as a keyword",
        #     ),
        # ),
        (
            "(a, /, b, *args, c, **kwargs)",
            None,
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
            dict(ignore_kind=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            None,
            {
                "a": 1,
                "b": 2,
                "args": (3, 4),
                "c": 5,
                "kwargs": {"d": 6, "e": 7},
                "f": 8,
            },
            dict(allow_excess=True, ignore_kind=True),
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
        ),
        # ------------------------------------------------------------------------------
    ],
)
def test_map_arguments(sig_spec, args, kwargs, map_arguments_kwargs, expected_output):
    sig = Sig(sig_spec)
    map_arguments_kwargs = map_arguments_kwargs or {}
    call = lambda: sig.map_arguments(args, kwargs, **map_arguments_kwargs)
    _test_call(call, expected_output)


@pytest.mark.parametrize(
    "sig_spec, arguments, mk_args_and_kwargs_kw, expected_output",
    [
        # ------------------------------------------------------------------------------
        # PO
        # ------------------------------------------------------------------------------
        ("(a, /)", {"a": 1}, None, ((1,), {})),
        ("(a, /)", None, None, (TypeError, "missing.*required.*argument: 'a'")),
        ("(a, /)", None, dict(allow_partial=True), ((), {})),
        ("(a=0, /)", None, dict(apply_defaults=True), ((0,), {})),
        ("(a=0, /)", None, dict(allow_partial=True, apply_defaults=True), ((0,), {})),
        ("(a=0, /)", None, dict(ignore_kind=True, apply_defaults=True), ((), {"a": 0})),
        (
            "(a, /)",
            {"a": 1, "b": 2},
            None,
            (TypeError, "Got unexpected keyword arguments: b"),
        ),
        ("(a, /)", {"a": 1, "b": 2}, dict(allow_excess=True), ((1,), {})),
        ("(a, /)", {"a": 1}, dict(ignore_kind=True), ((), {"a": 1})),
        (
            "(a, /)",
            {"a": 1, "b": 2},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PK
        # ------------------------------------------------------------------------------
        ("(a)", {"a": 1}, None, ((), {"a": 1})),
        ("(a)", None, None, (TypeError, "missing.*required.*argument: 'a'")),
        ("(a)", None, dict(allow_partial=True), ((), {})),
        ("(a=0)", None, dict(apply_defaults=True), ((), {"a": 0})),
        ("(a=0)", None, dict(allow_partial=True, apply_defaults=True), ((), {"a": 0})),
        ("(a=0)", None, dict(ignore_kind=True, apply_defaults=True), ((), {"a": 0})),
        (
            "(a)",
            {"a": 1, "b": 2},
            None,
            (TypeError, "Got unexpected keyword arguments: b"),
        ),
        ("(a)", {"a": 1, "b": 2}, dict(allow_excess=True), ((), {"a": 1})),
        ("(a)", {"a": 1}, dict(ignore_kind=True), ((), {"a": 1})),
        (
            "(a)",
            {"a": 1, "b": 2},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1}),
        ),
        ("(a)", {"a": 1}, dict(args_limit=None), ((1,), {})),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # VP
        # ------------------------------------------------------------------------------
        ("(*args)", {"args": (1, 2)}, None, ((1, 2), {})),
        ("(*args)", None, None, ((), {})),
        ("(*args)", None, dict(allow_partial=True), ((), {})),
        ("(*args)", None, dict(apply_defaults=True), ((), {})),
        ("(*args)", None, dict(allow_partial=True, apply_defaults=True), ((), {})),
        (
            "(*args)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"args": ()}),
        ),
        (
            "(*args)",
            {"args": (1, 2), "a": 3},
            None,
            (TypeError, "Got unexpected keyword arguments: a"),
        ),
        ("(*args)", {"args": (1, 2), "a": 3}, dict(allow_excess=True), ((1, 2), {})),
        ("(*args)", {"args": (1, 2)}, dict(ignore_kind=True), ((), {"args": (1, 2)})),
        (
            "(*args)",
            {"args": (1, 2), "a": 3},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"args": (1, 2)}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # KO
        # def foo(*, a): ...
        # ------------------------------------------------------------------------------
        ("(*, a)", {"a": 1}, None, ((), {"a": 1})),
        ("(*, a)", None, None, (TypeError, "missing a required.*argument: 'a'")),
        ("(*, a)", None, dict(allow_partial=True), ((), {})),
        ("(*, a=0)", None, dict(apply_defaults=True), ((), {"a": 0})),
        (
            "(*, a=0)",
            None,
            dict(allow_partial=True, apply_defaults=True),
            ((), {"a": 0}),
        ),
        ("(*, a=0)", None, dict(ignore_kind=True, apply_defaults=True), ((), {"a": 0})),
        (
            "(*, a)",
            {"a": 1, "b": 2},
            None,
            (TypeError, "Got unexpected keyword arguments: b"),
        ),
        ("(*, a)", {"a": 1, "b": 2}, dict(allow_excess=True), ((), {"a": 1})),
        ("(*, a)", {"a": 1}, dict(ignore_kind=True), ((), {"a": 1})),
        (
            "(*, a)",
            {"a": 1, "b": 2},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # VK
        # def foo(**kwargs): ...
        # ------------------------------------------------------------------------------
        ("(**kwargs)", {"kwargs": {"a": 1, "b": 2}}, None, ((), {"a": 1, "b": 2})),
        ("(**kwargs)", None, None, ((), {})),
        ("(**kwargs)", None, dict(allow_partial=True), ((), {})),
        ("(**kwargs)", None, dict(apply_defaults=True), ((), {})),
        ("(**kwargs)", None, dict(allow_partial=True, apply_defaults=True), ((), {})),
        (
            "(**kwargs)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"kwargs": {}}),
        ),
        (
            "(**kwargs)",
            {"kwargs": {"a": 1, "b": 2}, "c": 3},
            None,
            (TypeError, "Got unexpected keyword arguments: c"),
        ),
        (
            "(**kwargs)",
            {"kwargs": {"a": 1, "b": 2}, "c": 3},
            dict(allow_excess=True),
            ((), {"a": 1, "b": 2}),
        ),
        (
            "(**kwargs)",
            {"kwargs": {"a": 1, "b": 2}},
            dict(ignore_kind=True),
            ((), {"kwargs": {"a": 1, "b": 2}}),
        ),
        (
            "(**kwargs)",
            {"kwargs": {"a": 1, "b": 2}, "c": 3},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"kwargs": {"a": 1, "b": 2}}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + KO
        # def foo(a, /, b, *, c): ...
        # ------------------------------------------------------------------------------
        ("(a, /, b, *, c)", {"a": 1, "b": 2, "c": 3}, None, ((1,), {"b": 2, "c": 3})),
        (
            "(a, /, b, *, c)",
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        ("(a, /, b, *, c)", None, dict(allow_partial=True), ((), {})),
        (
            "(a=0, /, b=0, *, c=0)",
            None,
            dict(apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            None,
            dict(allow_partial=True, apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *, c=0)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"a": 0, "b": 0, "c": 0}),
        ),
        (
            "(a, /, b, *, c)",
            {"a": 1, "b": 2, "c": 3, "d": 4},
            None,
            (TypeError, "Got unexpected keyword arguments: d"),
        ),
        (
            "(a, /, b, *, c)",
            {"a": 1, "b": 2, "c": 3, "d": 4},
            dict(allow_excess=True),
            ((1,), {"b": 2, "c": 3}),
        ),
        (
            "(a, /, b, *, c)",
            {"a": 1, "b": 2, "c": 3},
            dict(ignore_kind=True),
            ((), {"a": 1, "b": 2, "c": 3}),
        ),
        (
            "(a, /, b, *, c)",
            {"a": 1, "b": 2, "c": 3, "d": 4},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1, "b": 2, "c": 3}),
        ),
        (
            "(a, /, b, *, c)",
            {"a": 1, "b": 2, "c": 3},
            dict(args_limit=None),
            ((1, 2), {"c": 3}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # VP + VK
        # def foo(*args, **kwargs): ...
        # ------------------------------------------------------------------------------
        (
            "(*args, **kwargs)",
            {"args": (1, 2), "kwargs": {"a": 1, "b": 2}},
            None,
            ((1, 2), {"a": 1, "b": 2}),
        ),
        ("(*args, **kwargs)", None, None, ((), {})),
        ("(*args, **kwargs)", None, dict(allow_partial=True), ((), {})),
        ("(*args, **kwargs)", None, dict(apply_defaults=True), ((), {})),
        (
            "(*args, **kwargs)",
            None,
            dict(allow_partial=True, apply_defaults=True),
            ((), {}),
        ),
        (
            "(*args, **kwargs)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"args": (), "kwargs": {}}),
        ),
        (
            "(*args, **kwargs)",
            {"args": (1, 2), "kwargs": {"a": 1, "b": 2}, "c": 3},
            None,
            (TypeError, "Got unexpected keyword arguments: c"),
        ),
        (
            "(*args, **kwargs)",
            {"args": (1, 2), "kwargs": {"a": 1, "b": 2}, "c": 3},
            dict(allow_excess=True),
            ((1, 2), {"a": 1, "b": 2}),
        ),
        (
            "(*args, **kwargs)",
            {"args": (1, 2), "kwargs": {"a": 1, "b": 2}},
            dict(ignore_kind=True),
            ((), {"args": (1, 2), "kwargs": {"a": 1, "b": 2}}),
        ),
        (
            "(*args, **kwargs)",
            {"args": (1, 2), "kwargs": {"a": 1, "b": 2}, "c": 3},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"args": (1, 2), "kwargs": {"a": 1, "b": 2}}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + VP + KO
        # def foo(a, /, b, *args, c): ...
        # ------------------------------------------------------------------------------
        (
            "(a, /, b, *args, c)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
            None,
            ((1, 2, 3, 4), {"c": 5}),
        ),
        (
            "(a, /, b, *args, c)",
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        ("(a, /, b, *args, c)", None, dict(allow_partial=True), ((), {})),
        (
            "(a=0, /, b=0, *args, c=0)",
            None,
            dict(apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            None,
            dict(allow_partial=True, apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *args, c=0)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"a": 0, "b": 0, "args": (), "c": 0}),
        ),
        (
            "(a, /, b, *args, c)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "d": 6},
            None,
            (TypeError, "Got unexpected keyword arguments: d"),
        ),
        (
            "(a, /, b, *args, c)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "d": 6},
            dict(allow_excess=True),
            ((1, 2, 3, 4), {"c": 5}),
        ),
        (
            "(a, /, b, *args, c)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5},
            dict(ignore_kind=True),
            ((), {"a": 1, "b": 2, "args": (3, 4), "c": 5}),
        ),
        (
            "(a, /, b, *args, c)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "d": 6},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1, "b": 2, "args": (3, 4), "c": 5}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + KO + VK
        # def foo(a, /, b, *, c, **kwargs): ...
        # ------------------------------------------------------------------------------
        (
            "(a, /, b, *, c, **kwargs)",
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
            None,
            ((1,), {"b": 2, "c": 3, "d": 4, "e": 5}),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        ("(a, /, b, *, c, **kwargs)", None, dict(allow_partial=True), ((), {})),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            dict(apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            dict(allow_partial=True, apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *, c=0, **kwargs)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"a": 0, "b": 0, "c": 0, "kwargs": {}}),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}, "f": 6},
            None,
            (TypeError, "Got unexpected keyword arguments: f"),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}, "f": 6},
            dict(allow_excess=True),
            ((1,), {"b": 2, "c": 3, "d": 4, "e": 5}),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
            dict(ignore_kind=True),
            ((), {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}}),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}, "f": 6},
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}}),
        ),
        (
            "(a, /, b, *, c, **kwargs)",
            {"a": 1, "b": 2, "c": 3, "kwargs": {"d": 4, "e": 5}},
            dict(args_limit=None),
            ((1, 2), {"c": 3, "d": 4, "e": 5}),
        ),
        # ------------------------------------------------------------------------------
        # ------------------------------------------------------------------------------
        # PO + PK + VP + KO + VK
        # def foo(a, /, b, *args, c, **kwargs): ...
        # ------------------------------------------------------------------------------
        (
            "(a, /, b, *args, c, **kwargs)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
            None,
            ((1, 2, 3, 4), {"c": 5, "d": 6, "e": 7}),
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            None,
            None,
            (TypeError, "missing.*required.*argument: 'a'"),
        ),
        ("(a, /, b, *args, c, **kwargs)", None, dict(allow_partial=True), ((), {})),
        (
            "(a=0, /, b=0, *args, c=0, **kwargs)",
            None,
            dict(apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *args, c=0, **kwargs)",
            None,
            dict(allow_partial=True, apply_defaults=True),
            ((0,), {"b": 0, "c": 0}),
        ),
        (
            "(a=0, /, b=0, *args, c=0, **kwargs)",
            None,
            dict(ignore_kind=True, apply_defaults=True),
            ((), {"a": 0, "b": 0, "args": (), "c": 0, "kwargs": {}}),
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            {
                "a": 1,
                "b": 2,
                "args": (3, 4),
                "c": 5,
                "kwargs": {"d": 6, "e": 7},
                "f": 8,
            },
            None,
            (TypeError, "Got unexpected keyword arguments: f"),
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            {
                "a": 1,
                "b": 2,
                "args": (3, 4),
                "c": 5,
                "kwargs": {"d": 6, "e": 7},
                "f": 8,
            },
            dict(allow_excess=True),
            ((1, 2, 3, 4), {"c": 5, "d": 6, "e": 7}),
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}},
            dict(ignore_kind=True),
            ((), {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}}),
        ),
        (
            "(a, /, b, *args, c, **kwargs)",
            {
                "a": 1,
                "b": 2,
                "args": (3, 4),
                "c": 5,
                "kwargs": {"d": 6, "e": 7},
                "f": 8,
            },
            dict(allow_excess=True, ignore_kind=True),
            ((), {"a": 1, "b": 2, "args": (3, 4), "c": 5, "kwargs": {"d": 6, "e": 7}}),
        ),
        # ------------------------------------------------------------------------------
    ],
)
def test_mk_args_and_kwargs(
    sig_spec, arguments, mk_args_and_kwargs_kw, expected_output
):
    sig = Sig(sig_spec)
    mk_args_and_kwargs_kw = mk_args_and_kwargs_kw or {}
    call = lambda: sig.mk_args_and_kwargs(arguments, **mk_args_and_kwargs_kw)
    _test_call(call, expected_output)


def _test_call(call, expected_output):
    if (
        isinstance(expected_output, tuple)
        and isinstance(expected_output[0], type)
        and issubclass(expected_output[0], Exception)
    ):
        err, err_msg = expected_output
        with pytest.raises(err, match=err_msg):
            call()
    else:
        assert call() == expected_output
```

## tests/test_util.py

```python
"""Test utils"""

import pickle
from i2.signatures import Sig
from i2.wrapper import wrap
from i2.util import copy_func
import tempfile
import os

from i2.util import FileLikeObject


def test_file_pointer():
    # Create a temporary file and write some content to it
    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        temp_file.write(b"Test content")
        temp_file_path = temp_file.name

    try:
        # Case 1: file is a string (file path)
        with FileLikeObject(temp_file_path) as f:
            assert f.read() == b"Test content"

        # Case 2: file is bytes
        with FileLikeObject(b"Test content") as f:
            assert f.read() == b"Test content"

        # Case 3: file is an open file pointer
        with open(temp_file_path, "rb") as file:
            with FileLikeObject(file) as f:
                assert f.read() == b"Test content"
    finally:
        # Clean up the temporary file
        os.remove(temp_file_path)


def test_copy_func_with_wrap(copy_func=copy_func):
    f = lambda x, *, y=2: x * y
    wrapped_f = wrap(f)  # empty wrap
    assert Sig(copy_func(wrapped_f)) == Sig(f)
    assert wrapped_f(3) == f(3) == 6


def is_pickable(obj):
    try:
        _ = pickle.loads(pickle.dumps(obj))
        return True
    except Exception:
        return False


def unpickled_func_still_works(func, *args, **kwargs):
    expected = func(*args, **kwargs)
    unpickled_func = pickle.loads(pickle.dumps(func))
    result = unpickled_func(*args, **kwargs)
    return expected == result
```

## tests/test_wrapper.py

```python
"""Testing wrapper"""

from collections.abc import Iterable
from i2.wrapper import wrap, mk_ingress_from_name_mapper, rm_params
from i2.deco import FuncFactory
from i2.signatures import Sig


def _test_ingress(a, b: str, c="hi"):
    return (a + len(b) % 2,), dict(string=f"{c} {b}")


def _test_func(times, string):
    return times * string


def test_wrap():
    import pickle
    from inspect import signature

    func = _test_func

    # Just wrapping the func gives you a sort of copy of the func.
    wrapped_func = wrap(func)  # no transformations
    assert wrapped_func(2, "co") == "coco" == func(2, "co")

    # If you give the wrap an ingress function
    ingress = _test_ingress
    wrapped_func = wrap(func, ingress=ingress)
    # it will use it to (1) determine the signature of the wrapped_func
    assert (
        str(signature(wrapped_func)) == "(a, b: str, c='hi')"
    )  # "(a, b: str, c='hi')"
    # and (2) to map inputs
    assert wrapped_func(2, "world! ", "Hi") == "Hi world! Hi world! Hi world! "

    # An egress function can be used to transform outputs
    wrapped_func = wrap(func, egress=len)
    assert wrapped_func(2, "co") == 4 == len("coco") == len(func(2, "co"))

    # Both ingress and egress can be used in combination
    wrapped_func = wrap(func, ingress=ingress, egress=len)
    assert (
        wrapped_func(2, "world! ", "Hi") == 30 == len("Hi world! Hi world! Hi world! ")
    )

    # A wrapped function is pickle-able (unlike the usual way decorators are written)

    unpickled_wrapped_func = pickle.loads(pickle.dumps(wrapped_func))
    assert (
        unpickled_wrapped_func(2, "world! ", "Hi")
        == 30
        == len("Hi world! Hi world! Hi world! ")
    )


def _test_foo(a, b: int, c=7):
    return a + b * c


def _test_bar(a, /, b: int, *, c=7):
    return a + b * c


def test_mk_ingress_from_name_mapper():
    import pickle
    from inspect import signature

    foo = _test_foo
    # Define the mapping (keys are inner and values are outer names)
    name_mapper = dict(a="aa", c="cc")
    # Make an ingress function with that mapping
    ingress = mk_ingress_from_name_mapper(foo, name_mapper)
    # Use the ingress function to wrap a function
    wrapped_foo = wrap(foo, ingress=ingress)
    # See that the signature of the wrapped func uses the mapped arg names
    assert (
        str(signature(wrapped_foo)) == str(signature(ingress)) == "(aa, b: int, cc=7)"
    )
    # And that wrapped function does compute correctly
    assert (
        foo(1, 2, c=4)
        == foo(a=1, b=2, c=4)
        == wrapped_foo(aa=1, b=2, cc=4)
        == wrapped_foo(1, 2, cc=4)
    )
    # The ingress function returns args and kwargs for wrapped function
    assert ingress("i was called aa", b="i am b", cc=42) == (
        (),
        {"a": "i was called aa", "b": "i am b", "c": 42},
    )
    # See above that the args is empty. That will be the case most of the time.
    # Keyword arguments will be favored when there's a choice. If wrapped
    # function uses position-only arguments though, ingress will have to use them
    bar = _test_bar
    assert str(signature(bar)) == "(a, /, b: int, *, c=7)"
    ingress_for_bar = mk_ingress_from_name_mapper(bar, name_mapper)
    assert ingress_for_bar("i was called aa", b="i am b", cc=42) == (
        ("i was called aa",),
        {"b": "i am b", "c": 42},
    )
    wrapped_bar = wrap(bar, ingress=ingress_for_bar)
    assert (
        bar(1, 2, c=4)
        == bar(1, b=2, c=4)
        == wrapped_bar(1, b=2, cc=4)
        == wrapped_bar(1, 2, cc=4)
    )

    # Note that though bar had a positional only and a keyword only argument,
    # we are (by default) free of argument kind constraints in the wrapped function:
    # We can can use a positional args on `cc` and keyword args on `aa`
    assert str(signature(wrapped_bar)) == "(aa, b: int, cc=7)"
    assert wrapped_bar(1, 2, 4) == wrapped_bar(aa=1, b=2, cc=4)

    # If you want to conserve the argument kinds of the wrapped function, you can
    # specify this with `conserve_kind=True`:
    ingress_for_bar = mk_ingress_from_name_mapper(bar, name_mapper, conserve_kind=True)
    wrapped_bar = wrap(bar, ingress=ingress_for_bar)
    assert str(signature(wrapped_bar)) == "(aa, /, b: int, *, cc=7)"

    # A wrapped function is pickle-able (unlike the usual way decorators are written)
    unpickled_wrapped_foo = pickle.loads(pickle.dumps(wrapped_foo))
    assert (
        str(signature(unpickled_wrapped_foo))
        == str(signature(ingress))
        == "(aa, b: int, cc=7)"
    )
    assert (
        foo(1, 2, c=4)
        == foo(a=1, b=2, c=4)
        == unpickled_wrapped_foo(aa=1, b=2, cc=4)
        == unpickled_wrapped_foo(1, 2, cc=4)
    )


def test_arg_val_converter():
    from i2.wrapper import arg_val_converter
    from i2.tests.objects_for_testing import formula1, times_2, plus_1

    assert formula1(4, 3, 2, z=1) == 14
    assert formula1(4, 3 * 2, 2, z=1 + 1) == 400

    # See that "transparently" converting the function doesn't change anything

    formula2 = arg_val_converter(formula1)
    assert formula2(4, 3, 2, z=1) == 14 == formula1(4, 3, 2, z=1)

    # But now if we ask to convert x and z...

    formula2 = arg_val_converter(formula1, x=times_2, z=plus_1)
    assert formula2(4, 3, 2, z=1) == 400 == formula1(4, 3 * 2, 2, z=1 + 1)

    from i2.tests.test_util import unpickled_func_still_works

    assert unpickled_func_still_works(formula1, 4, 3, 2, z=1)
    assert unpickled_func_still_works(formula2, 4, 3, 2, z=1)


from i2.wrapper import Wrapx


def test_wrapx():
    from inspect import signature

    # Test that an trivial Wrapx instance (i.e. no ingress, caller, or egress
    # modifications) gives us an object behaving like the wrapped function.

    # TODO: Make it work with param kinds: e.g. func(x: int, *, y=1) -> int:
    def func(x: int, y=1) -> int:
        return x + y

    wrapped_func = Wrapx(func)
    assert (
        str(signature(wrapped_func)) == "(x: int, y=1) -> int" == str(signature(func))
    )

    # Test egress that has a single param z

    def func(x, y):
        return x + y

    def egress(v, *, z):
        return v * z

    wrapped_func = Wrapx(func, egress=egress)

    assert func(1, 2) == 3

    # TODO: should be '(x, y=1, *, z)' --> Need to work on the merge for this.
    assert str(signature(wrapped_func)) == "(x, y, z)"
    assert wrapped_func(1, 2, z=3) == 9 == func(1, 2) * 3

    # A more realistic application: Saving outputs to a specific location on output

    def func(x, y):
        return x + y

    def save_on_output_egress(v, *, k, s):
        s[k] = v
        return v

    save_on_output = Wrapx(func, egress=save_on_output_egress)
    # TODO: should be `(x, y, *, k, s)` --> Need to work on the merge for this.
    assert str(signature(save_on_output)) == "(x, y, k, s)"

    store = dict()
    save_on_output(1, 2, k="save_here", s=store)
    assert save_on_output(1, 2, k="save_here", s=store) == 3 == func(1, 2)
    assert store == {"save_here": 3}

    # Trying out a caller: Here, we want to wrap the function so it will apply to an
    # iterable of inputs, returning a list of results

    def func(x, y=2):
        return x + y

    def iterize(func, args, kwargs):
        first_arg_val = next(iter(kwargs.values()))
        return list(map(func, first_arg_val))

    iterized_func = Wrapx(func, caller=iterize)

    assert iterized_func([1, 2, 3, 4]) == [3, 4, 5, 6]

    # The same as above, except generalized to allow other variables (here ``y``) to
    # be input as well

    from functools import partial

    def func(x, y):
        return x + y

    def iterize_first_arg(func, args, kwargs):
        first_arg_name = next(iter(kwargs))
        remaining_kwargs = {k: v for k, v in kwargs.items() if k != first_arg_name}
        return list(map(partial(func, **remaining_kwargs), kwargs[first_arg_name]))

    iterized_func = Wrapx(func, caller=iterize_first_arg)

    assert iterized_func([1, 2, 3, 4], 10) == [11, 12, 13, 14]


def simple_chunker(a: Iterable, chk_size: int):
    """Generate fixed sized non-overlapping chunks of an iterable ``a``.

    >>> list(simple_chunker(range(7), 3))
    [(0, 1, 2), (3, 4, 5)]
    """
    return zip(*([iter(a)] * chk_size))


def test_rm_params():
    # ----------------------------------------------------------------------------
    # Edge case of rm_params with FuncFactory: https://github.com/i2mint/i2/issues/44
    wf = range(7)
    mk_chunker = FuncFactory(simple_chunker, exclude="a")
    chunker = mk_chunker(chk_size=3)
    assert str(Sig(chunker)) == "(a: collections.abc.Iterable, *, chk_size: int = 3)"
    assert list(chunker(wf)) == [(0, 1, 2), (3, 4, 5)]

    mk_chunker = rm_params(
        FuncFactory(simple_chunker),
        params_to_remove=["a"],
        allow_removal_of_non_defaulted_params=True,
        allow_partial=True,  # wouldn't work without this
    )

    assert str(Sig(mk_chunker)) == "(chk_size: int)"

    wf = range(7)
    chunker = mk_chunker(chk_size=3)
    assert str(Sig(chunker)) == "(a: collections.abc.Iterable, *, chk_size: int = 3)"
    assert list(chunker(wf)) == [(0, 1, 2), (3, 4, 5)]
    # ----------------------------------------------------------------------------
```

## tests/util.py

```python
"""Utils for testing"""

from inspect import Parameter, Signature
from itertools import product
from typing import (
    List,
    Any,
    Union,
    Tuple,
    Optional,
)
from collections.abc import Callable, Iterator, Iterable, Mapping
from inspect import signature

from i2.signatures import KO, PK, PO, VP, VK, var_param_kinds
from i2.signatures import _empty
from i2.signatures import ParamsAble, Sig, ensure_param, SignatureAble

ParameterAble = Union[int, Parameter, str]
ParamsAble_ = Union[ParamsAble, str, list[int]]


def _is_valid_arg_for_sig(x):
    return (
        isinstance(x, (Callable, Signature))
        or isinstance(x, str)
        and x.startswith("(")
        and x.endswith(")")
    )


def generate_params(params: ParamsAble_):
    """Generate inspect.Parameter instances quickly.

    Example: Generate params solely from a list of their kinds

    >>> str(Sig(generate_params([0, 0, 1, 1, 1, 2, 3, 4])))
    '(a00, a01, /, a12, a13, a14, *a25, a36, **a47)'

    >>> str(Sig(generate_params("00111234")))
    '(a00, a01, /, a12, a13, a14, *a25, a36, **a47)'
    """
    if _is_valid_arg_for_sig(params):
        # generate params from a callable or signature
        yield from Sig(params).params
    else:
        for i, spec in enumerate(params):
            if isinstance(spec, int):
                kind = spec
                yield Parameter(f"a{kind}{i}", kind=kind)
            elif isinstance(spec, Parameter):
                param = spec
                yield param
            elif isinstance(spec, str) and spec.isnumeric():
                kind = int(spec)
                yield Parameter(f"a{kind}{i}", kind=kind)
            else:
                try:
                    yield ensure_param(spec)
                except Exception:
                    raise TypeError(
                        f"Don't know how to handle this type of obj: {spec}"
                    )


def params_to_arg_name_and_val(params: ParamsAble_):
    """Generate a `{argname: argval, ...}` dictionary from an iterable of params.

    >>> assert dict(params_to_arg_name_and_val(generate_params("00111234"))) == {
    ...     "a00": 0,
    ...     "a01": 1,
    ...     "a12": 2,
    ...     "a13": 3,
    ...     "a14": 4,
    ...     "a25": (5, -5),
    ...     "a36": 6,
    ...     "a47": {"a47": 7, "a47_": -7},
    ... }
    """
    params = generate_params(params)
    for i, param in enumerate(params):
        if param.kind == Parameter.VAR_POSITIONAL:
            val = (i, -i)
        elif param.kind == Parameter.VAR_KEYWORD:
            val = {param.name: i, param.name + "_": -i}
        else:
            val = i
        yield (param.name, val)


def inject_defaults(params: ParamsAble_, defaults: dict):
    """Yields params with defaults ({argname: default_val,...}) edited.

    >>> assert (
    ...     str(
    ...         Sig(
    ...             inject_defaults(
    ...                 generate_params("00111234"), defaults={"a14": 40, "a36": 60}
    ...             )
    ...         )
    ...     )
    ...     == "(a00, a01, /, a12, a13, a14=40, *a25, a36=60, **a47)"
    ... )
    """
    for param in generate_params(params):
        if param.name in defaults:
            yield param.replace(default=defaults[param.name])
        else:
            yield param


def _str_of_call_args(_call_kwargs: dict):
    return ", ".join(f"{k}={v}" for k, v in _call_kwargs.items())


def _params_to_name(params):
    return "f" + "".join(str(int(p.kind)) for p in params)


def mk_func_from_params(
    params: ParamsAble = "00111234",
    *,
    defaults=None,
    name=None,
    callback: Callable[[dict], Any] = _str_of_call_args,
):
    """Make a function (that actually returns something based on args) from params.

    See Also: ``sig_to_func``

    :param params: params (arguments) of the function (can be expressed in many ways!)
    :param defaults: Optional {argname: default,...} dict to inject defaults
    :param name: Optional name to give the function
    :param callback: The function defining what the function actually does.
        Must be a function taking a single dict input encapsulating the all arguments.
        The default will return a string representation of this dict.
    :return: A function with the specified params, returning a string of it's (call) args

    There's many ways you can express the `params` input.
    Any of the ways understood by the `signatures.ensure_params` function, for one;
    plus a few more.

    One nice way to express the params is through an actual function.
    Note that the code of the function isn't even looked out.
    Only it's signature is taken into consideration.
    The returned function will have the same signature.
    Instead, the callback function will be acalled on the infered _call_kwargs
    dict of {argname: argval} pairs.
    The default callaback is a string exhibiting these argname/argval pairs.

    >>> f = mk_func_from_params(lambda x, /, y, *, z: None)
    >>> print(f"{f.__name__}{Sig(f)}")
    f(x, /, y, *, z)
    >>> f(1, 2, z=3)
    'x=1, y=2, z=3'
    >>> f(1, y=2, z=3)
    'x=1, y=2, z=3'
    >>> f = mk_func_from_params(lambda x, /, y=42, *, z='ZZZ': None)
    >>> print(f"{f.__name__}{Sig(f)}")
    f(x, /, y=42, *, z='ZZZ')
    >>> f(3.14)
    'x=3.14, y=42, z=ZZZ'

    If you're not interested in having that level of control, but are just
    interested in the number and kinds of the arguments, you can specify only that;
    a sequence of kinds.
    These must be a non-decreasing sequence of integers between
    0 and 4 inclusive. These integers represent kinds of parameters.
    See https://docs.python.org/3/library/inspect.html#inspect.Parameter.kind
    to see what each integer value means.
    You can also specify this integer sequence as a single string, as shown below.

    >>> f = mk_func_from_params(params="00111234")
    >>> str(Sig(f))
    '(a00, a01, /, a12, a13, a14, *a25, a36, **a47)'

    The next one is convoluted on purpose, to try to break the tools with name clashing,
    using the name a47 several times.
    The ``a47=...`` in the input could have been any name, but is bound to be caught in
    the ``**a47`` variadic argument.
    Therefore, the first ``a47=...`` of the output is saying "I'm the variadic that caught
    that input". The ``{'a47': ...`` following it is saying "the user input an extra
    keyword argument that they called `a47`. Then the ``{"a47": 7, "a47_": -7}`` is just
    the value of that keyword argument.

    >>> f(0, 1, 2, 3, 4, 5, -5, a36=6, a47={"a47": 7, "a47_": -7})
    "a00=0, a01=1, a12=2, a13=3, a14=4, a25=(5, -5), a36=6, a47={'a47': {'a47': 7, 'a47_': -7}}"

    >>> f(0, 1, 2, a13=3, a14=4, a36=6)
    'a00=0, a01=1, a12=2, a13=3, a14=4, a25=(), a36=6, a47={}'

    What just happened?
    Well, `params="00111234"` was transformed to `params=[0, 0, 1, 1, 1, 2, 3, 4]`,
    which was transformed to a list of the same size, using

    Now, if you really want full control over those params, you can specify them
    completely using the `inspect.Parameter` class.
    You can also decide what level of control you want, and mix and match all kinds of
    specifications, as below.

    >>> from inspect import Parameter
    >>> f = mk_func_from_params([
    ...     0,
    ...     'blah',
    ...     Parameter(name='hello',
    ...               kind=Parameter.POSITIONAL_OR_KEYWORD,
    ...               default='world')
    ... ])
    >>> print(f"{f.__name__}{Sig(f)}")
    f(a00, /, blah, hello='world')
    >>> assert f(11, 22) == 'a00=11, blah=22, hello=world'

    """
    params = generate_params(params)
    params = inject_defaults(params, defaults=defaults or {})
    sig = Sig(params)

    @sig
    def arg_str_func(*args, **kwargs):
        _call_kwargs = sig.map_arguments(args, kwargs, apply_defaults=True)
        return callback(_call_kwargs)

    arg_str_func.__name__ = name or _params_to_name(params)

    return arg_str_func


def _sig_to_str_of_call_args_code_str(sig: Sig):
    return (
        "return " + 'f"' + _str_of_call_args({p: f"{{{p}}}" for p in sig.names}) + '"'
    )


def _is_simple_expression(code_lines):
    if len(code_lines) == 1:
        line = code_lines[0].strip()
        if not (
            line in {"pass", "..."}
            or line.startswith("return")
            or (line.startswith('"') and line.endswith("'"))
        ):
            return True
    else:
        return False


def sig_to_func(
    sig: ParamsAble = "00111234",
    code_lines: (
        str | Iterable | Callable[[Sig], str]
    ) = _sig_to_str_of_call_args_code_str,
    *,
    name: str | None = None,
    globals: dict | None = None,
    locals: Mapping | None = None,
):
    """
    Make a function from a signature

    See Also: ``mk_func_from_params``

    More information: https://github.com/i2mint/i2/issues/34

    :param sig: Signature (or something that can be made into a signature) the func
    should have
    :param name: Name the function should have
    :param code_lines: The code lines

    The ``globals`` must be a dictionary and ``locals`` can be any mapping, defaulting
    to the current globals and locals. If only ``globals`` is given, ``locals``
    defaults to it.

    >>> sig = Sig('(a, /, b=2)')
    >>> f = sig_to_func(sig)
    >>> str(Sig(f))
    '(a, /, b=2)'
    >>> f(2, 3)
    'a=2, b=3'
    >>> f.__name__
    'f01'

    See how the function was given a name automatically? This name was created by
    appending the kind number
    (see https://docs.python.org/3/library/inspect.html#inspect.Parameter.kind)
    to 'f'.

    Let's demo how we can

    - use kind numbers (see `inspect` module) to specify the signature

    - give an explicit name to the function

    - specify ``locals()`` so that ``sig_to_func`` can insert the new function there
    (which makes it picklable, for instances)

    >>> _ = sig_to_func('012', name='foo', locals=locals())
    >>> # and now `foo` is in local name space, and has signature:
    >>> str(Sig(foo))
    '(a00, /, a11, *a22)'
    >>> foo(1,2,3,4,5)
    'a00=1, a11=2, a22=(3, 4, 5)'

    In the examples above, the function body was was created from the input ``sig``
    through the ``_sig_to_str_of_call_args_code_str`` function which outputs a string
    formed from the input argument names and values.

    But you can specify your own function. This function should take a ``Sig`` object
    and return the string or lines (iterable of strings) of the function's body.
    You can also specify a string or lines directly.

    >>> g = sig_to_func('(x, y=2)', 'pass')
    >>> assert g(2, y=3) is None
    >>> h = sig_to_func('(x, y=2)', 'return x * y')
    >>> h(3)
    6
    >>> h(x=10, y=3)
    30

    You can even omit the return instruction if, as in ``lambda`` functions, the body is
    a simple expression (not `pass`, `...`, or starting something with quotes).

    >>> h = sig_to_func('(x, y=2)', 'x / y')
    >>> h(10, 5)
    2.0

    """
    sig = Sig(generate_params(sig))
    name = name or _params_to_name(sig.params)
    if callable(code_lines):
        code_lines = code_lines(sig)  # call the function on sig to get lines
    if isinstance(code_lines, str):
        code_lines = code_lines.split("\n")
    if _is_simple_expression(code_lines):
        # If code_lines has only one line and it seems it's an expression, prepend return
        code_lines = [f"return {code_lines[0]}"]
    code_string = "\n\t".join(code_lines)
    func_def_string = f"def {name}{sig}:\n\t{code_string}"
    _locals = locals or {}
    exec(func_def_string, globals, _locals)
    return _locals[name]


def _all_prefixes(x: Iterable):
    """
    >>> list(_all_prefixes([1,2,3]))
    [(), (1,), (1, 2), (1, 2, 3)]
    >>> list(map(dict, _all_prefixes({'a': 1, 'b': 2}.items())))
    [{}, {'a': 1}, {'a': 1, 'b': 2}]
    """
    x = tuple(x)
    for i in range(len(x) + 1):
        yield x[:i]


def _args_kwargs_combinations(args, kwargs):
    """

    >>> assert list(_args_kwargs_combinations((1,2), {'a': 3, 'b': 4})) == [
    ...     ((), {}),
    ...     ((), {'a': 3}),
    ...     ((), {'a': 3, 'b': 4}),
    ...     ((1,), {}),
    ...     ((1,), {'a': 3}),
    ...     ((1,), {'a': 3, 'b': 4}),
    ...     ((1, 2), {}),
    ...     ((1, 2), {'a': 3}),
    ...     ((1, 2), {'a': 3, 'b': 4})
    ... ]

    """
    for args_prefix, kwargs_items_prefix in product(
        _all_prefixes(args), _all_prefixes(kwargs.items())
    ):
        yield args_prefix, dict(kwargs_items_prefix)


def variadic_type(sig, variadics):
    var_kinds = [sig.kinds[param] for param in variadics]
    if not variadics:
        return "no_var"
    if VP in var_kinds and VK not in var_kinds:
        return "vp_only"
    if VK in var_kinds and VP not in var_kinds:
        return "vk_only"
    else:
        return "vp_vk"


def create_variadic_source(sig, variadics, dflt_source):
    var_type = variadic_type(sig, variadics)
    if var_type == "no_var":
        result = ((), {})
    elif var_type == "vp_only":
        result = (dflt_source[0], {})
    elif var_type == "vk_only":
        result = ((), dflt_source[1])
    elif var_type == "vp_vk":
        result = dflt_source
    return result


# TODO: Get rid of ignore_variadics once using code is refactored
def sig_to_inputs(
    sig: SignatureAble,
    argument_vals: Iterable | None = None,
    *,
    variadics_source: tuple[tuple, dict] = (
        ("args1", "args2"),
        {"kwargs1": "kwargs1_val"},
    ),
) -> Iterator[tuple[tuple, dict]]:
    """Generate all kind-valid (arg, kwargs) input combinations for a function with a
    given signature ``sig``, with argument values taken from the ``argument_vals``

    :param sig: A signature or anything that ``i2.Sig`` can use to create one (e.g.
        function, string, list of dicts etc.)
    :param argument_vals: An interable from which the argument values will be drawn.
        Defaults to ``list(range(n_args))``.
    :param variadics_source: The ``(tuple, dict)`` pair that will be used to extract
        variadics' inputs.
    :return: A generator of ``(args: tuple, kwargs: dict)`` pairs

    >>> assert list(
    ...     sig_to_inputs(lambda a, b, /, c, d, *, e, f: None)
    ... ) == [
    ...     ((0, 1), {'c': 2, 'd': 3, 'e': 4, 'f': 5}),
    ...     ((0, 1, 2), {'d': 3, 'e': 4, 'f': 5}),
    ...     ((0, 1, 2, 3), {'e': 4, 'f': 5})
    ... ]

    >>> list(sig_to_inputs(Sig('(a, *args, b, **kwargs)')))  # doctest: +NORMALIZE_WHITESPACE
    [((), {'a': 0, 'b': 1}),
     ((), {'a': 0, 'b': 1, 'kwargs1': 'kwargs1_val'}),
     (('args1',), {'a': 0, 'b': 1}),
     (('args1',), {'a': 0, 'b': 1, 'kwargs1': 'kwargs1_val'}),
     (('args1', 'args2'), {'a': 0, 'b': 1}),
     (('args1', 'args2'), {'a': 0, 'b': 1, 'kwargs1': 'kwargs1_val'}),
     ((0,), {'b': 1}),
     ((0,), {'b': 1, 'kwargs1': 'kwargs1_val'}),
     ((0, 'args1'), {'b': 1}),
     ((0, 'args1'), {'b': 1, 'kwargs1': 'kwargs1_val'}),
     ((0, 'args1', 'args2'), {'b': 1}),
     ((0, 'args1', 'args2'), {'b': 1, 'kwargs1': 'kwargs1_val'})]

    Tip: To ignore variadics all together, you can specify ``variadics_source=((), {})``.
    """
    sig = Sig(sig)
    init_sig = sig
    already_yielded = []
    variadics = [param for param, kind in sig.kinds.items() if kind in var_param_kinds]
    if variadics:
        for v in variadics:
            sig -= v
        variadics_source = create_variadic_source(
            init_sig, variadics, dflt_source=variadics_source
        )
        var_args, var_kwargs = variadics_source
        for args, kwargs in sig_to_inputs(sig, argument_vals):
            for _args, _kwargs in _args_kwargs_combinations(var_args, var_kwargs):
                yield args + _args, dict(kwargs, **_kwargs)
    else:
        for sub_sig in _get_sub_sigs_from_default_values(sig):
            po, pk, ko = _get_non_variadic_kind_counts(sub_sig)
            for args, kwargs_vals in _sig_to_inputs(
                po, pk, ko, argument_vals=argument_vals
            ):
                input_ = (
                    tuple(args),
                    {k: v for k, v in zip(sub_sig.names[len(args) :], kwargs_vals)},
                )
                if input_ not in already_yielded:
                    yield input_
                    already_yielded.append(input_)


def _sig_to_inputs(po=0, pk=0, ko=0, argument_vals: Iterable | None = None):
    """

    >>> list(_sig_to_inputs(2,2,2))
    [([0, 1], [2, 3, 4, 5]), ([0, 1, 2], [3, 4, 5]), ([0, 1, 2, 3], [4, 5])]

    :param po: Number of POSITION_ONLY args in signature.
    :param pk: Number of POSITION_OR_KEYWORD args in signature.
    :param ko: Number of KEYWORD_ONLY args in signature.
    :param argument_vals: An interable from which the argument values will be drawn.
        Defaults to ``list(range(n_args))``.
    :return: A generator of ``(vals_for_args: tuple, vals_for_kwargs)`` pairs
    """
    if argument_vals is None:
        argument_vals = list(range(po + pk + ko))
    else:
        argument_vals = list(argument_vals)
    for n_args_from_pk in range(pk + 1):
        yield argument_vals[: (po + n_args_from_pk)], argument_vals[
            (po + n_args_from_pk) :
        ]


def _get_sub_sigs_from_default_values(sig: Sig) -> Iterator[Sig]:
    """Generate all the signatures compatible with the given signature
    by ignoring the arguments that have default values.
    >>> sig = Sig('(a=0, /, b=0, *args, c=0, **kwargs)')
    >>> assert [str(s) for s in _get_sub_sigs_from_default_values(sig)] == [
    ...     '(*args, **kwargs)',
    ...     '(a=0, /, *args, **kwargs)',
    ...     '(*args, b=0, **kwargs)',
    ...     '(a=0, /, b=0, *args, **kwargs)',
    ...     '(*args, c=0, **kwargs)',
    ...     '(a=0, /, *args, c=0, **kwargs)',
    ...     '(*args, b=0, c=0, **kwargs)',
    ...     '(a=0, /, b=0, *args, c=0, **kwargs)'
    ... ]
    """

    def internal_get_sub_sigs(sig):
        kos = [n for n in sig.names_of_kind[KO] if n in sig.defaults]
        for ko in reversed(kos):
            _sig = sig - ko
            yield from internal_get_sub_sigs(_sig)

        pks = [n for n in sig.names_of_kind[PK] if n in sig.defaults]
        for i, pk in reversed(list(enumerate(pks))):
            _sig = sig - pk
            pks_to_transform_to_ko = pks[i + 1 :]
            params = [
                p.replace(kind=KO) if p.name in pks_to_transform_to_ko else p
                for p in _sig.params
            ]
            params.sort(key=lambda p: p.kind)
            _sig = Sig(params)
            yield from internal_get_sub_sigs(_sig)

        pos = [n for n in sig.names_of_kind[PO] if n in sig.defaults]
        if pos:
            _sig = sig - pos[-1]
            params = [p.replace(kind=KO) if p.kind == PK else p for p in _sig.params]
            params.sort(key=lambda p: p.kind)
            _sig = Sig(params)
            yield from internal_get_sub_sigs(_sig)

        if sig not in already_yielded:
            yield sig
            already_yielded.add(sig)

    already_yielded = set()
    yield from internal_get_sub_sigs(sig)


def _get_non_variadic_kind_counts(sig: Sig):
    po = pk = ko = 0
    for kind in sig.kinds.values():
        po += kind == sig.POSITIONAL_ONLY
        pk += kind == sig.POSITIONAL_OR_KEYWORD
        ko += kind == sig.KEYWORD_ONLY
    return po, pk, ko


def mk_func_inputs_for_params(params: ParamsAble_, param_to_input):
    pass


# ---------------------------------------------------------------------------------------
# Tools to analyze compatibility between signature and function call

from i2 import Pipe
import re

_signature_msg_patterns = [
    "keyword arguments$",
    "invalid keyword argument",
    "expected at most",
    "keyword argument",
    r"got some positional\-only arguments passed as keyword arguments",
    "no signature found",
]

_signature_msg_regex = re.compile("|".join(map("({})".format, _signature_msg_patterns)))
is_signature_msg = Pipe(_signature_msg_regex.search, bool)


def _is_signature_error(
    error_obj,
    signature_error_types=(TypeError, ValueError),
    is_signature_msg=is_signature_msg,
):
    if isinstance(error_obj, signature_error_types):
        error_msg = str(error_obj)
        return is_signature_msg(error_msg)
    return False


def call_and_return_error(func, /, *args, **kwargs):
    try:
        func(*args, **kwargs)
        return None
    except BaseException as error_obj:
        return error_obj


def on_error_return_none(func, /, *args, **kwargs):
    try:
        return func(*args, **kwargs)
    except BaseException as error_obj:
        return None


call_raises_signature_error = Pipe(call_and_return_error, _is_signature_error)

call_raises_signature_error.__doc__ = """
>>> call_raises_signature_error(lambda x, /, y: None, 1, y=2)
False
>>> call_raises_signature_error(lambda x, /, y: None, x=1, y=2)
True
"""


# Yes, I too see that this can be made into yet another Pipe!
def function_is_compatible_with_signature(func, sig):
    """
    Runs through all combinations of positional and keyword arguments,

    >>> function_is_compatible_with_signature(hasattr, Sig(lambda obj, name: ...))
    False
    >>> function_is_compatible_with_signature(hasattr, Sig(lambda obj, name, /: ...))
    True
    """

    def _call_raises_sig_error():
        for args, kwargs in sig_to_inputs(sig):
            yield call_raises_signature_error(func, *args, **kwargs)

    return not any(_call_raises_sig_error())


def builtin_objects():
    for name in dir(__builtins__):
        yield getattr(__builtins__, name)


def builtin_signatureless_callables():
    """
    A generator of builtin callables that don't have signatures.
    """
    for obj in builtin_objects():
        if callable(obj) and call_raises_signature_error(signature, obj):
            yield obj


# ---------------------------------------------------------------------------------------

#
# @mk_func_from_params.register
# def mk_func_from_params(params: Iterable[int], defaults=None, name=None):
#     """
#
#     :param kinds:
#     :param defaults:
#     :return:
#
#     Make a sequence of kinds (must be a non-decreasing sequence of integers between
#     0 and 4 inclusive. These integers represent kinds of parameters.
#     See https://docs.python.org/3/library/inspect.html#inspect.Parameter.kind
#     to see what each integer value means.
#
#     >>> kinds = list(map(int, "00111234"))
#     >>> kinds
#     [0, 0, 1, 1, 1, 2, 3, 4]
#
#     Note: `kinds_to_arg_str_func` also works directly with strings such as "00111234".
#
#     Now tell `kinds_to_arg_str_func` to make a function with those kinds.
#
#     >>> f = kinds_to_arg_str_func(kinds)
#     >>> str(Sig(f))
#     '(a00, a01, /, a12, a13, a14, *a25, a36, **a47)'
#     >>> f(0, 1, 2, 3, 4, 5, -5, a36=6, a47={"a47": 7, "a47_": -7})
#     "a00=0, a01=1, a12=2, a13=3, a14=4, a25=(5, -5), a36=6, a47={'a47': {'a47': 7, 'a47_': -7}}"
#     >>> f(0, 1, 2, a13=3, a14=4, a36=6)
#     'a00=0, a01=1, a12=2, a13=3, a14=4, a25=(), a36=6, a47={}'
#     """
#     kinds = params
#     params = inject_defaults(generate_params(kinds), defaults=defaults or {})
#     name = name or "f" + "".join(map(str, kinds))
#
#     return mk_func_from_params(params, defaults, name)

#
#
# @mk_func_from_params.register
# def _(kinds: str):
#     """
#     >>> f = kinds_to_arg_str_func("00111234")
#     >>> str(Sig(f))
#     '(a00, a01, /, a12, a13, a14, *a25, a36, **a47)'
#     >>> f(0, 1, 2, 3, 4, 5, -5, a36=6, a47={"a47": 7, "a47_": -7})
#     "a00=0, a01=1, a12=2, a13=3, a14=4, a25=(5, -5), a36=6, a47={'a47': {'a47': 7, 'a47_': -7}}"
#     >>> f(0, 1, 2, a13=3, a14=4, a36=6)
#     'a00=0, a01=1, a12=2, a13=3, a14=4, a25=(), a36=6, a47={}'
#
#     """
#     return mk_func_from_params(_kinds_str_to_int_list(kinds))
#
#
# f = mk_func_from_params("00111234")
# assert str(Sig(f)) == "(a00, a01, /, a12, a13, a14, *a25, a36, **a47)"
# assert (
#     f(0, 1, 2, 3, 4, 5, -5, a36=6, a47={"a47": 7, "a47_": -7})
#     == "a00=0, a01=1, a12=2, a13=3, a14=4, a25=(5, -5), a36=6, a47={'a47': {'a47': 7, 'a47_': -7}}"
# )


empty = _empty

mappingproxy = type(Signature().parameters)


def trace_call(func, local_vars, name=None):
    if name is None:
        name = func.__name__
    return (
        f"{name}("
        + ", ".join(f"{argname}={local_vars[argname]}" for argname in Sig(func).names)
        + ")"
    )


# class KeywordArg(dict):
#     """Just to mark a dict as a keyword argument"""
#
#
# def _separate_pk_arguments_into_positional_and_keyword(pka):
#     args = []
#     kwargs = {}
#     pka_iter = iter(pka)
#     for a in pka_iter:
#         if not isinstance(a, KeywordArg):
#             args.append(a)
#         else:
#             kwargs.update(dict(a))
#     for a in pka_iter:
#         kwargs.update(dict(a))
#
#     return args, kwargs
```

## util.py

```python
"""Misc util objects"""

from operator import attrgetter, itemgetter
import inspect
import re
import os
import itertools
import sys
from functools import partial, wraps, cached_property, partialmethod
import types
from types import SimpleNamespace
from typing import (
    Tuple,
    Any,
    Union,
    Optional,
    TypeVar,
    KT,
    Literal,
)
from collections.abc import Mapping, Callable, MutableMapping, Iterable, Iterator
from collections import namedtuple
import contextlib
import io

T = TypeVar("T")


def asis(x: T) -> T:
    """the identity function: f(x) := x (takes only one argument, and returns it)"""
    return x


def return_false(*args, **kwargs):
    """a function that returns True (takes any number of arguments)"""
    return False


def return_true(*args, **kwargs):
    """a function that returns False (takes any number of arguments)"""
    return False


def return_none(*args, **kwargs) -> None:
    return None


from typing import Any, Optional
from collections.abc import MutableMapping


def name_of_obj(
    o: object,
    *,
    base_name_of_obj: Callable = attrgetter("__name__"),
    caught_exceptions: tuple = (AttributeError,),
    default_factory: Callable = return_none,
) -> str | None:
    """
    Tries to find the (or "a") name for an object, even if `__name__` doesn't exist.

    >>> name_of_obj(map)
    'map'
    >>> name_of_obj([1, 2, 3])
    'list'
    >>> name_of_obj(print)
    'print'
    >>> name_of_obj(lambda x: x)
    '<lambda>'
    >>> from functools import partial
    >>> name_of_obj(partial(print, sep=","))
    'print'
    >>> from functools import cached_property
    >>> class A:
    ...     @property
    ...     def prop(self):
    ...         return 1.0
    ...     @cached_property
    ...     def cached_prop(self):
    ...         return 2.0
    >>> name_of_obj(A.prop)
    'prop'
    >>> name_of_obj(A.cached_prop)
    'cached_prop'

    Note that ``name_of_obj`` uses the ``__name__`` attribute as its base way to get
    a name. You can customize this behavior though.
    For example, see that:

    >>> from inspect import Signature
    >>> name_of_obj(Signature.replace)
    'replace'

    If you want to get the fully qualified name of an object, you can do:

    >>> alt = partial(name_of_obj, base_name_of_obj=attrgetter('__qualname__'))
    >>> alt(Signature.replace)
    'Signature.replace'

    """
    try:
        return base_name_of_obj(o)
    except caught_exceptions:
        kwargs = dict(
            base_name_of_obj=base_name_of_obj,
            caught_exceptions=caught_exceptions,
            default_factory=default_factory,
        )
        if isinstance(o, (cached_property, partial, partialmethod)) and hasattr(
            o, "func"
        ):
            return name_of_obj(o.func, **kwargs)
        elif isinstance(o, property) and hasattr(o, "fget"):
            return name_of_obj(o.fget, **kwargs)
        elif hasattr(o, "__class__"):
            return name_of_obj(type(o), **kwargs)
        elif hasattr(o, "fset"):
            return name_of_obj(o.fset, **kwargs)
        return default_factory(o)


def register_object(
    obj: Any = None,
    name: str | None = None,
    *,
    registry: MutableMapping,
):
    """
    Register an object (e.g. function, class) in the global registry.

    The raw use is to define a registry Mapping and then call this function with the registry and the object to register.

    >>> registry = {}
    >>> def wet():
    ...     pass
    >>> register_object(wet, registry=registry)  # doctest: +ELLIPSIS
    <function wet at 0x...>
    >>> registry  # doctest: +ELLIPSIS
    {'wet': <function wet at 0x...>}

    >>> register_object(wet, name='custom_name', registry=registry)  # doctest: +ELLIPSIS
    <function wet at 0x...>
    >>> registry  # doctest: +ELLIPSIS
    {'wet': <function wet at 0x...>, 'custom_name': <function wet at 0x...>}

    The most common use of this function is to use it as a decorator with a fixed (but mutable!) registry:

    >>> another_registry = {}
    >>> register_to_another = register_object(registry=another_registry)
    >>> @register_to_another
    ... def dry():
    ...     pass
    >>> another_registry  # doctest: +ELLIPSIS
    {'dry': <function dry at 0x...>}

    >>> @register_to_another('DRY')
    ... def foo():
    ...     pass
    >>> another_registry  # doctest: +ELLIPSIS
    {'dry': <function dry at 0x...>, 'DRY': <function foo at 0x...>}
    """
    if obj is None:
        # if obj is None, it means we are using the decorator form
        # (this is the register_object() case)
        return partial(register_object, name=name, registry=registry)
    if isinstance(obj, str):
        # if the obj is a string, it is the name of the object to register
        # (this is the register_object(name) case)
        name = obj
        return partial(register_object, name=name, registry=registry)

    if name is None:
        name = name_of_obj(obj)

    registry[name] = obj
    return obj


ExceptionTypes = Union[BaseException, tuple[BaseException]]
Handler = Callable[[BaseException], None]
Handlers = Union[Handler, Mapping[KT, Handler]]

ignore_exception = asis  # an alias of asis, to make it clear in context where used


# Note: Similar functionality in meshed.slabs
#       (https://github.com/i2mint/meshed/blob/61a5633cc8e1d4b4b26f31d3bf70d744aab327c4/meshed/slabs.py#L145)
class ConditionalExceptionCatcher:
    """
    Context manager to catch exceptions of a certain type and instance condition.

    :param exception_types: The type of exception to catch. Can be a single exception
        type or a tuple of exception types.
    :param exception_condition: A function that takes an exception instance and returns
        a "key" value indicating whether the exception should be caught.
        If the bool(key) is True, the exception is caught.
        If the bool(key) is False, the exception is not caught.
        The key can further be used to determine the handler to use, when the handlers
        argument is a mapping.
        The default is to catch all exceptions of the specified type(s).
    :param handlers: Specification of how to handle the exceptions. Can be a single
        function to run on the exception object when an exception of the specified
        type is caught, or a mapping (e.g. dict) of handler functions, keyed by the
        key returned by the exception_condition function.
    :param prevent_propagation: Whether to prevent the exception from propagating. Defaults
        to ``True``.

    Example:

    >>> exception_catcher = ConditionalExceptionCatcher(
    ...     ValueError, lambda e: e.args[0] == 'foo', handlers=print
    ... )
    >>> with exception_catcher:
    ...     raise ValueError('foo')
    foo
    >>> with exception_catcher:
    ...     raise TypeError('foo')
    Traceback (most recent call last):
        ...
    TypeError: foo
    >>> with exception_catcher:
    ...     raise ValueError('bar')
    Traceback (most recent call last):
        ...
    ValueError: bar

    """

    def __init__(
        self,
        exception_types: ExceptionTypes,
        exception_condition: Callable[[BaseException], bool] = return_true,
        handlers: Callable[[BaseException], None] = ignore_exception,
        *,
        prevent_propagation=True,
    ):
        self.exception_types = exception_types
        self.exception_condition = exception_condition
        self.handlers = handlers
        self.prevent_propagation = prevent_propagation

        assert callable(
            self.exception_condition
        ), f"Expected a callable for {self.exception_condition=}"
        assert callable(self.handlers) or isinstance(
            self.handlers, Mapping
        ), f"Expected a callable or a mapping for {self.handlers=}"

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        # TODO: Could extend exception_condition and/or handler to use traceback
        if exc_type is self.exception_types and (
            key := self.exception_condition(exc_value)
        ):
            if isinstance(self.handlers, Mapping):
                handler = self.handlers[key]
            else:
                handler = self.handlers
            handler(exc_value)
            return self.prevent_propagation
        return False  # Allows other exceptions to propagate


# -------------------------------------------------------------------------------------
# Attribute Mapping Classes
# (Vendored in dol)


class AttributeMapping(SimpleNamespace, Mapping[str, Any]):
    """
    A read-only mapping with attribute access.

    Useful when you want mapping interface but don't need mutation.

    Examples:

    >>> ns = AttributeMapping(x=10, y=20)
    >>> ns.x
    10
    >>> ns['y']
    20
    >>> list(ns)
    ['x', 'y']
    """

    @classmethod
    def from_mapping(self, mapping: Mapping[str, Any]) -> "AttributeMapping":
        """
        Create an AttributeMapping from a regular mapping.

        This is useful when you want to convert a dictionary or other mapping
        into an AttributeMapping for attribute-style access.
        """
        return self(**mapping)

    def __getitem__(self, key: str) -> Any:
        """Get item with proper KeyError on missing keys."""
        return _get_attr_or_key_error(self, key)

    def __iter__(self) -> Iterator[str]:
        """Iterate over attribute names."""
        return iter(self.__dict__)

    def __len__(self) -> int:
        """Return number of attributes."""
        return len(self.__dict__)


class AttributeMutableMapping(AttributeMapping, MutableMapping[str, Any]):
    """
    A mutable mapping that provides both attribute and dictionary-style access.

    Extends AttributeMapping with mutation capabilities,
    ensuring proper error handling and protocol compliance.

    Examples:

    >>> ns = AttributeMutableMapping(apple=1, banana=2)
    >>> ns.apple
    1
    >>> ns['banana']
    2
    >>> ns['cherry'] = 3
    >>> ns.cherry
    3
    >>> list(ns)
    ['apple', 'banana', 'cherry']
    >>> len(ns)
    3
    >>> 'apple' in ns
    True
    >>> del ns['banana']
    >>> 'banana' in ns
    False
    """

    def __setitem__(self, key: str, value: Any) -> None:
        """Set item via attribute assignment."""
        setattr(self, key, value)

    def __delitem__(self, key: str) -> None:
        """Delete item with proper KeyError on missing keys."""
        try:
            delattr(self, key)
        except AttributeError:
            raise KeyError(key)


def _get_attr_or_key_error(obj: object, key: str) -> Any:
    """
    Get attribute or raise KeyError if not found.

    Helper function to maintain consistent error handling across
    mapping implementations.
    """
    try:
        return getattr(obj, key)
    except AttributeError:
        raise KeyError(key)


# -------------------------------------------------------------------------------------


@contextlib.contextmanager
def FileLikeObject(file, *, io_cls=io.BytesIO, open_mode="rb"):
    """Context manager for file-like objects.

    The purpose of this context manager is to be able to ensure we have a file-like
    object interface to work with, regardless of whether we are given a file path,
    bytes of a file, or an open file pointer.

    Args:
        file (str, bytes, io.IOBase): The file path, bytes of a file, or an open file pointer.

    Yields:
        io.IOBase: A file-like object.
    """
    if isinstance(file, str):
        # If file is a string, open the file and yield the file pointer
        with open(file, open_mode) as f:
            yield f  # TODO: Is f protected by the open context?
    elif isinstance(file, bytes):
        # If file is bytes, create a BytesIO object and yield it
        with io.BytesIO(file) as f:
            yield f
    else:
        # Otherwise, assume file is already a file-like object and yield it
        yield file


def copy_func(
    func: Callable, *, copy_dict: bool = True, code=None, globals_: dict = None
):
    """Make a (shallow) copy of a function.

    >>> f = lambda x, *, y=2: x * y
    >>> f.an_attr = 42
    >>> f_copy = copy_func(f)
    >>> f_copy(3) == f(3) == 6
    True
    >>> f_copy.an_attr == f.an_attr == 42
    True

    Verify that making an attribute in one won't create an attribute in the other:

    >>> f.another_attr = 42
    >>> hasattr(f_copy, 'another_attr')
    False
    >>> f_copy.yet_another_attr = 84
    >>> hasattr(f, 'yet_another_attr')
    False

    :param func: The function to be copied.
    :param copy_dict: Indicates whether to copy the ``__dict__`` attribute of the
        function. Defaults to ``True``.
    :param code: The value to be use as the ``__code__`` attribute of the copy.
    :param globals_: The value to be use as the ``__globals__`` attribute of the copy.
    :return: A shallow copy of the function.

    Note that it should always work with proper functions and attempts to do the
    best job it can with other callables, but there are no guarantees on how
    ``copy_function`` will behave with custom callables.

    If these custom callables don't have a `__code__` attribute, the copy will fail.
    Furthermore, if the custom callable  doesn't have ``__globals__``, the empty
    dictionary will be used as the globals.
    We provide a ``code`` and ``globals`` argument to allow the user to provide
    the ``__code__`` and ``__globals__`` attributes of the function to be copied.

    :param func: The function to be copied. Must be a function, not just any method or callable.
    :param copy_dict: Also copy any attributes set on the function instance. Defaults to ``True``.
    :return: A shallow copy of the function.
    """
    from types import FunctionType

    code = code or getattr(func, "__code__", None)
    if not isinstance(code, types.CodeType):
        raise TypeError(f"Expected a types.CodeType object, but got {type(code)=}")
    globals_ = globals_ or getattr(func, "__globals__", {})
    new_func = FunctionType(
        code,  # if your func doesn't have a __code__ attr, can't make a copy!
        globals_,
        name=getattr(func, "__name__", None),
        argdefs=getattr(func, "__defaults__", None),
        closure=getattr(func, "__closure__", None),
    )
    if hasattr(func, "__kwdefaults__"):
        new_func.__kwdefaults__ = func.__kwdefaults__
    if copy_dict:
        new_func.__dict__.update(func.__dict__)
    return new_func


class OverwritesForbidden(ValueError):
    """Raise when a user is not allowed to overwrite a mapping's key"""


def is_lambda(func):
    return getattr(func, "__name__", None) == "<lambda>"


# TODO: Fragile. Make more robust.
def lambda_code(lambda_func) -> str:
    """Extract code of expression from lambda function.
    For lambda code-extraction see:
    https://stackoverflow.com/questions/73980648/how-to-transform-a-lambda-function-into-a-pickle-able-function

    """
    func_str = str(inspect.getsourcelines(lambda_func)[0])
    return func_str.strip("['\\n']").split(" = ")[1]


# TODO: Only works with lambdas so either assert function is a lambda on construction
#  or make it work with functions more generally.
class PicklableLambda:
    """
    Wraps a lambda function to make it picklable (through extracting its code)
    Also, provide it with a name, optionally.

    >>> f = lambda x, y=0: x + y
    >>> ff = PicklableLambda(f)
    >>> import pickle
    >>> fff = pickle.loads(pickle.dumps(ff))
    >>> assert fff(2, 3) == ff(2, 3) == f(2, 3)

    For lambda code-extraction see:
    https://stackoverflow.com/questions/73980648/how-to-transform-a-lambda-function-into-a-pickle-able-function

    """

    def __init__(self, func, name=None):
        self.func = func
        self.__signature__ = inspect.signature(self.func)
        self.__name__ = name or getattr(func, "__name__", type(self).__name__)

    def __getstate__(self):
        return lambda_code(self.func), self.__name__

    def __setstate__(self, state):
        func_code, name = state
        func = eval(func_code)  # scary?
        self.__init__(func, name)

    def __call__(self, *args, **kwargs):
        return self.func(*args, **kwargs)

    def __repr__(self):
        return f"<{type(self).__name__}({self.__name__})>"


def ensure_identifiers(
    *objs: Iterable[T],
    get_identfiers: Callable[[T], Iterable[str]] = str.split,
    is_identifier: Callable[[str], bool] = str.isidentifier,
):
    """Ensure an iterable of identifiers

    >>> list(ensure_identifiers('these', 'are', 'valid', 'identifiers'))
    ['these', 'are', 'valid', 'identifiers']

    By default, ``ensure_identifiers`` will apply ``str.split`` to each ``obj`` of
    ``objs`` (assumed to be strings!) so that it can extract identifiers from
    space-separated strings of identifiers:

    >>> list(ensure_identifiers('these are valid identifiers'))
    ['these', 'are', 'valid', 'identifiers']

    You can control this functionality through the ``get_identfiers`` argument, for
    example, disallowing such splitting, or enabling the extraction of identifiers
    from other objects than strings.

    >>> list(ensure_identifiers(
    ...     {'this': 0, 'works': 1}, {'too': 2},
    ...     get_identfiers=list
    ... ))
    ['this', 'works', 'too']

    You can also control the ``is_identifier`` validatation function:

    >>> def less_than_6_chars(s): return len(s) < 6
    >>> list(ensure_identifiers('okay', 'too_long', is_identifier=less_than_6_chars))
    Traceback (most recent call last):
      ...
    ValueError: too_long isn't an identifier according toless_than_6_chars

    """
    for obj in objs:
        for identifier in get_identfiers(obj):
            if is_identifier(identifier):
                yield identifier
            else:
                raise ValueError(
                    f"{identifier} isn't an identifier according to"
                    f"{getattr(is_identifier, '__name__', str(is_identifier))}"
                )


def insert_name_based_objects_in_scope(
    *names,
    factory: Callable[[str], Any],
    scope: MutableMapping,
    allow_overwrites: bool = False,
):
    """
    Make several string-parametrized objects and insert them in a scope (e.g. locals()).

    This is useful when to avoid (error-prone) situations where we want the name we
    assign an object to, to be aligned with it's internal name, such as::

        foo = Factory('foo', ...)
        bar = Factory('bar', ...)
        baz = Factory('baz', ...)

    :param names: Identifier (valid python variable name) strings.
        These are used both as arguments of the ``factory`` and as keys for the
        ``scope`` the object the factory makes will be inserted under.
    :param factory: A function that takes a (valid python identifier) string and
        returns an object parametrized by that string.
    :param scope: The ``MutableMapping`` we want to insert the objects in.
    :param allow_overwrites: Whether the objects we create can overwrite existing
        objects the ``scope`` may already have. If we don't allow overwrites and we
        try to write under an existing key, a ``OverwritesForbidden`` error will be
        raised. This also includes the situation where we have some duplicates in
        ``names``.
    :return: None (this function has the side effect of inserting items in ``scope``.

    One of the (controversal) uses of ``insert_name_based_objects_in_scope`` is to be
    able to make several string-parametrized

    >>> from collections import namedtuple
    >>> from functools import partial
    >>>
    >>> factory = partial(namedtuple, field_names='apple banana')
    >>> insert_namedtuples_in_locals = partial(insert_name_based_objects_in_scope,
    ...     factory=factory, scope=locals(), allow_overwrites=True
    ... )
    >>> insert_namedtuples_in_locals('foo bar', 'baz')

    And now ``foo` exists!

    >>> 'foo' in locals()
    True
    >>> foo(1,2)
    foo(apple=1, banana=2)

    And so does ``bar`` and ``baz``:

    >>> bar(3, banana=4)
    bar(apple=3, banana=4)
    >>> baz(apple=3, banana=4)
    baz(apple=3, banana=4)
    """
    for name in ensure_identifiers(*names):
        if name not in scope or allow_overwrites:
            scope[name] = factory(name)
        else:
            raise OverwritesForbidden(
                f"This key already exisited and is not allowed to be overwritten"
            )


class LiteralVal:
    """An object to indicate that the value should be considered literally.

    >>> t = LiteralVal(42)
    >>> t.get_val()
    42
    >>> t()
    42

    """

    def __init__(self, val):
        self.val = val

    def get_val(self):
        """Get the value wrapped by Literal instance.

        One might want to use ``literal.get_val()`` instead ``literal()`` to get the
        value a ``Literal`` is wrapping because ``.get_val`` is more explicit.

        That said, with a bit of hesitation, we allow the ``literal()`` form as well
        since it is useful in situations where we need to use a callback function to
        get a value.
        """
        return self.val

    __call__ = get_val


def dflt_idx_preprocessor(obj, idx):
    if isinstance(idx, str) and str.isdigit(idx):
        idx = int(idx)
    if isinstance(idx, int) or isinstance(obj, Mapping):
        return obj[idx]
    elif hasattr(obj, idx):
        return getattr(obj, idx)
    else:
        raise KeyError(f"Couldn't extract a {idx} from object {obj}")


def path_extractor(tree, path, getter=dflt_idx_preprocessor, *, path_sep="."):
    """Get items from a tree-structured object from a sequence of tree-traversal indices.

    :param tree: The object you want to extract values from:
        Can be any object you want, as long as the indices listed by path and how to get
        the items indexed are well specified by ``path`` and ``getter``.
    :param path: An iterable of indices that define how to traverse the tree to get
        to desired item(s). If this iterable is a string, the ``path_sep`` argument
        will be used to transform it into a tuple of string indices.
    :param getter: A ``(tree, idx)`` function that specifies how to extract item ``idx``
        from the ``tree`` object.
    :param path_sep: The string separator to use if ``path`` is a string
    :return: The ``tree`` item(s) referenced by ``path``


    >>> tree = {'a': {'b': [0, {'c': [1, 2, 3]}]}}
    >>> path_extractor(tree, path=['a'])
    {'b': [0, {'c': [1, 2, 3]}]}
    >>> path_extractor(tree, path=['a', 'b'])
    [0, {'c': [1, 2, 3]}]
    >>> path_extractor(tree, path=['a', 'b', 1])
    {'c': [1, 2, 3]}
    >>> path_extractor(tree, path=['a', 'b', 1, 'c'])
    [1, 2, 3]
    >>> path_extractor(tree, path=('a', 'b', 1, 'c', 2))
    3

    You could do the same by specifying the path as a dot-separated string.

    >>> path_extractor(tree, 'a.b.1.c.2')
    3

    You can use any separation you want.

    >>> path_extractor(tree, 'a/b/1/c/2', path_sep='/')
    3

    You can also use `*` to indicate that you want to keep all the nodes of a given
    level.

    >>> tree = {'a': [{'b': [1, 10]}, {'b': [2, 20]}, {'b': [3, 30]}]}
    >>> path_extractor(tree, 'a.*.b.1')
    [10, 20, 30]

    A generalization of `*` is to specify a callable which will be intepreted as
    a filter function.

    >>> tree = {'a': [{'b': 1}, {'c': 2}, {'b': 3}, {'b': 4}]}
    >>> path_extractor(tree, ['a', lambda x: 'b' in x])
    [{'b': 1}, {'b': 3}, {'b': 4}]
    >>> path_extractor(tree, ['a', lambda x: 'b' in x, 'b'])
    [1, 3, 4]
    """
    if isinstance(path, str):
        path = path.split(path_sep)
    if len(path) == 0:
        return tree
    else:
        idx, *path = path  # extract path[0] as idx & update path to path[1:]
        if isinstance(idx, str) and idx == "*":
            idx = lambda x: True  # use a filter function (but filter everything in)
        if callable(idx) and not isinstance(idx, LiteralVal):
            # If idx is a non-literal callable, consider it as a filter to be applied
            # to iter(tree)
            # TODO: https://github.com/i2mint/i2/issues/27
            return [
                path_extractor(sub_tree, path, getter) for sub_tree in filter(idx, tree)
            ]
        else:
            if isinstance(idx, LiteralVal):
                # Use of Literal is meant get out of trouble if we want to use a
                # callable as an actual index, not as a filter.
                idx = idx.get_val()
            tree = getter(tree, idx)
            return path_extractor(tree, path, getter)


# Note: Specialization of path_extractor for Mappings
def dp_get(d, dot_path):
    """Get stuff from a dict (or any Mapping), using dot_paths (i.e. 'foo.bar' instead of
     ['foo']['bar'])

    >>> d = {'foo': {'bar': 2, 'alice': 'bob'}, 3: {'pi': 3.14}}
    >>> assert dp_get(d, 'foo') == {'bar': 2, 'alice': 'bob'}
    >>> assert dp_get(d, 'foo.bar') == 2
    >>> assert dp_get(d, 'foo.alice') == 'bob'
    """
    return path_extractor(d, dot_path, lambda d, k: d[k])


class lazyprop:
    """
    A descriptor implementation of lazyprop (cached property) from David Beazley's "Python Cookbook" book.
    It's

    >>> class Test:
    ...     def __init__(self, a):
    ...         self.a = a
    ...     @lazyprop
    ...     def len(self):
    ...         print('generating "len"')
    ...         return len(self.a)
    >>> t = Test([0, 1, 2, 3, 4])
    >>> t.__dict__
    {'a': [0, 1, 2, 3, 4]}
    >>> t.len
    generating "len"
    5
    >>> t.__dict__
    {'a': [0, 1, 2, 3, 4], 'len': 5}
    >>> t.len
    5
    >>> # But careful when using lazyprop that no one will change the value of a without deleting the property first
    >>> t.a = [0, 1, 2]  # if we change a...
    >>> t.len  # ... we still get the old cached value of len
    5
    >>> del t.len  # if we delete the len prop
    >>> t.len  # ... then len being recomputed again
    generating "len"
    3
    """

    def __init__(self, func):
        self.func = func

    def __get__(self, instance, cls):
        if instance is None:
            return self
        else:
            value = self.func(instance)
            setattr(instance, self.func.__name__, value)
            return value


class FrozenHashError(TypeError):
    pass


class FrozenDict(dict):
    """An immutable dict subtype that is hashable and can itself be used
    as a :class:`dict` key or :class:`set` entry. What
    :class:`frozenset` is to :class:`set`, FrozenDict is to
    :class:`dict`.

    There was once an attempt to introduce such a type to the standard
    library, but it was rejected: `PEP 416 <https://www.python.org/dev/peps/pep-0416/>`_.

    Because FrozenDict is a :class:`dict` subtype, it automatically
    works everywhere a dict would, including JSON serialization.

    """

    __slots__ = ("_hash",)

    def updated(self, *a, **kw):
        """Make a copy and add items from a dictionary or iterable (and/or
        keyword arguments), overwriting values under an existing
        key. See :meth:`dict.update` for more details.
        """
        data = dict(self)
        data.update(*a, **kw)
        return type(self)(data)

    @classmethod
    def fromkeys(cls, keys, value=None):
        # one of the lesser known and used/useful dict methods
        return cls(dict.fromkeys(keys, value))

    def __repr__(self):
        cn = self.__class__.__name__
        return "{}({})".format(cn, dict.__repr__(self))

    def __reduce_ex__(self, protocol):
        return type(self), (dict(self),)

    def __hash__(self):
        try:
            ret = self._hash
        except AttributeError:
            try:
                ret = self._hash = hash(frozenset(self.items()))
            except Exception as e:
                ret = self._hash = FrozenHashError(e)

        if ret.__class__ is FrozenHashError:
            raise ret

        return ret

    def __copy__(self):
        return self  # immutable types don't copy, see tuple's behavior

    # block everything else
    def _raise_frozen_typeerror(self, *a, **kw):
        "raises a TypeError, because FrozenDicts are immutable"
        raise TypeError("%s object is immutable" % self.__class__.__name__)

    __setitem__ = __delitem__ = update = _raise_frozen_typeerror
    setdefault = pop = popitem = clear = _raise_frozen_typeerror

    del _raise_frozen_typeerror


frozendict = FrozenDict  # alias to align with frozenset

########################################################################################################################


function_type = type(
    lambda x: x
)  # using this instead of callable() because classes are callable, for instance


class NoDefault:
    def __repr__(self):
        return "no_default"


no_default = NoDefault()


class imdict(dict):
    def __hash__(self):
        return id(self)

    def _immutable(self, *args, **kws):
        raise TypeError("object is immutable")

    __setitem__ = _immutable
    __delitem__ = _immutable
    clear = _immutable
    update = _immutable
    setdefault = _immutable
    pop = _immutable
    popitem = _immutable


def inject_method(self, method_function, method_name=None):
    """
    Inject a method into an object instance.

    method_function could be:
        * a function
        * a {method_name: function, ...} dict (for multiple injections)
        * a list of functions or (function, method_name) pairs
    """
    if isinstance(method_function, function_type):
        if method_name is None:
            method_name = method_function.__name__
        setattr(self, method_name, types.MethodType(method_function, self))
    else:
        if isinstance(method_function, dict):
            method_function = [
                (func, func_name) for func_name, func in method_function.items()
            ]
        for method in method_function:
            if isinstance(method, tuple) and len(method) == 2:
                self = inject_method(self, method[0], method[1])
            else:
                self = inject_method(self, method)

    return self


########################################################################################################################


def get_function_body(func):
    """
    Get the body of a function as a string.

    :param func: The function to get the body of.

    :return: The body of the function as a string.

    """
    source_lines = inspect.getsourcelines(func)[0]
    source_lines = itertools.dropwhile(lambda x: x.startswith("@"), source_lines)
    line = next(source_lines).strip()
    if not line.startswith("def ") and not line.startswith("class"):
        return line.rsplit(":")[-1].strip()
    elif not line.endswith(":"):
        for line in source_lines:
            line = line.strip()
            if line.endswith(":"):
                break
    # Handle functions that are not one-liners
    first_line = next(source_lines)
    # Find the indentation of the first line
    indentation = len(first_line) - len(first_line.lstrip())
    return "".join(
        [first_line[indentation:]] + [line[indentation:] for line in source_lines]
    )


class ExistingArgument(ValueError):
    pass


class MissingArgument(ValueError):
    pass


def _default_sentinel_repr_method(self):
    return "{}({!r})".format(self.__class__.__name__, self.__name__)


def mk_sentinel(
    name,
    boolean_value: bool = False,
    repr_: str | Callable = _default_sentinel_repr_method,
    *,
    module: str | None = None,
):
    """Creates and returns a new **instance** of a new class, suitable for usage as a
    "sentinel" since it is a kind of singleton (there can be only one instance of it.)

    A frequent use case for sentinels are where we want to indicate that something is
    missing. Often, we use ``None`` for this, but sometimes ``None`` is a valid value in
    our context (see for example the ``inspect.Parameter.empty`` sentinel to indicate
    that an argument doesn't have a default or annotation).
    Other times, we may want to distinguish different kinds of "nothing".

    ``mk_sentinel`` can help you create such sentinels, takes care of annoying details
    like pickability and allows you to control how to resolve your sentinel to a boolean.

    :param name: The name of your sentinel. Will be used for ``__name__`` attribute.
    :param boolean_value: The boolean value that the sentinel instance should resolve to.
    :param repr_: The method or string that should be used for the repr.
    :param module:
    :return: A sentinel instance

    >>> Empty = mk_sentinel('Empty')
    >>> Empty
    Sentinel('Empty')

    By default, the boolean resolution of a sentinel is ``False``. Meaning:

    >>> Nothing = mk_sentinel('Nothing')
    >>> bool(Nothing)
    False

    This is consistent with ``None``, so that you can check that an object ``x`` is not
    ``Nothing`` by doing ``if x: ...`` or idioms like:

    >>> x = Nothing
    >>> x = x or 'default'
    >>> x
    'default'

    (Though note that in situations where other elements that cast to ``False`` are
    valid values for ``x`` (like ``0``, ``None``, or ``False`` itself), it's safer to use
    ``if x is not Nothing: ...``.)

    Anyway, I digress.
    Point is that in some situations, the semantics  or usage of your sentinel is better
    align with True. You can control what the boolean resolution of your
    sentinel should be through the ``boolean_value`` argument:

    >>> Empty = mk_sentinel('Empty', boolean_value=True)
    >>> bool(Empty)
    True

    You can also control what you see in the repr, specifying a string value;

    >>> Empty = mk_sentinel('undefined', repr_='undefined')
    >>> Empty
    undefined

    or a method;

    >>> Empty = mk_sentinel('Empty', repr_=lambda self: f"<{self.__name__}>")
    >>> Empty
    <Empty>

    And yes, even though we used a lambda here, it's still picklable:

    >>> import pickle

    >>> Empty = mk_sentinel('Empty', repr_='Empty', module=__name__)
    >>> pickle.loads(pickle.dumps(Empty))  # doctest: +SKIP
    Empty

    Talking about pickle, here's some more info on that:

    >>> unpickled_Empty = pickle.loads(pickle.dumps(Empty))  # doctest: +SKIP
    >>> # The unpickled version is "equal" to the original:
    >>> unpickled_Empty == Empty  # doctest: +SKIP
    True
    >>> # the types are the same too:
    >>> type(unpickled_Empty) == type(Empty)    # doctest: +SKIP
    True
    >>>
    >>>

    Note that though two sentinels might have the same name, they're not equal:

    >>> Empty = mk_sentinel('Empty')
    >>> AnotherEmptyWithSameName = mk_sentinel('Empty')
    >>> Empty
    Sentinel('Empty')
    >>> AnotherEmptyWithSameName
    Sentinel('Empty')
    >>> # but...
    >>> AnotherEmptyWithSameName == Empty
    False
    >>> # Note even the types are the same!
    >>> type(AnotherEmptyWithSameName) == type(Empty)
    False

    One thing that makes the pickle work is that we took care of sticking in a
    ``__module__`` for you. ``mk_sentinel`` figures this out by some dark magic
    involving looking into the system's "frames" etc. This may not always work since
    some systems (e.g. ``pypy``) may use different "under-the-hood" methods.

    But if you want to control the value of ``__module__`` yourself, you can, simply
    but indicating what the module of the sentinel is.
    Usually, you'll just specify it as ``module=__name__``, which will stick the
    name of the module you're defining the sentinel in for you!

    >>> MySentinel = mk_sentinel('MySentinel', module=__name__)

    Thanks: Inspired greately from the ``make_sentinel`` function of ``boltons``:
    See https://boltons.readthedocs.io/.

    """

    class Sentinel:
        def __init__(self):
            self.__name__ = name

        if callable(repr_):
            __repr__ = repr_
        else:

            def __repr__(self):
                return repr_

        def __reduce__(self):
            return self.__name__

        def __bool__(self):
            return boolean_value

    if module is None:
        # TODO: Try to use something else than hidden _getframe
        # TODO: extract this module resolver so can be reused (_getframe(2)?)
        frame = sys._getframe(1)
        module = frame.f_globals.get("__name__")

    if not module or module not in sys.modules:
        raise ValueError(
            "Pickleable sentinel objects can only be made from top-level module scopes"
        )
    Sentinel.__module__ = module

    return Sentinel()


def _indent(text, margin, newline="\n", key=bool):
    "based on boltons.strutils.indent"
    indented_lines = [
        (margin + line if key(line) else line) for line in text.splitlines()
    ]
    return newline.join(indented_lines)


NO_DEFAULT = mk_sentinel("NO_DEFAULT", boolean_value=False)

# --------------------------------------------------------------------------------------
# FunctionBuilder, vendored and adapted from boltons.funcutils

from inspect import formatannotation


def inspect_formatargspec(
    args,
    varargs=None,
    varkw=None,
    defaults=None,
    kwonlyargs=(),
    kwonlydefaults={},
    annotations={},
    formatarg=str,
    formatvarargs=lambda name: "*" + name,
    formatvarkw=lambda name: "**" + name,
    formatvalue=lambda value: "=" + repr(value),
    formatreturns=lambda text: " -> " + text,
    formatannotation=formatannotation,
):
    """Copy formatargspec from python 3.7 standard library.
    Python 3 has deprecated formatargspec and requested that Signature
    be used instead, however this requires a full reimplementation
    of formatargspec() in terms of creating Parameter objects and such.
    Instead of introducing all the object-creation overhead and having
    to reinvent from scratch, just copy their compatibility routine.
    """

    def formatargandannotation(arg):
        result = formatarg(arg)
        if arg in annotations:
            result += ": " + formatannotation(annotations[arg])
        return result

    specs = []
    if defaults:
        firstdefault = len(args) - len(defaults)
    for i, arg in enumerate(args):
        spec = formatargandannotation(arg)
        if defaults and i >= firstdefault:
            spec = spec + formatvalue(defaults[i - firstdefault])
        specs.append(spec)
    if varargs is not None:
        specs.append(formatvarargs(formatargandannotation(varargs)))
    else:
        if kwonlyargs:
            specs.append("*")
    if kwonlyargs:
        for kwonlyarg in kwonlyargs:
            spec = formatargandannotation(kwonlyarg)
            if kwonlydefaults and kwonlyarg in kwonlydefaults:
                spec += formatvalue(kwonlydefaults[kwonlyarg])
            specs.append(spec)
    if varkw is not None:
        specs.append(formatvarkw(formatargandannotation(varkw)))
    result = "(" + ", ".join(specs) + ")"
    if "return" in annotations:
        result += formatreturns(formatannotation(annotations["return"]))
    return result


class FunctionBuilder:
    """The FunctionBuilder type provides an interface for programmatically
    creating new functions, either based on existing functions or from
    scratch.

    Note: Based on https://boltons.readthedocs.io

    Values are passed in at construction or set as attributes on the
    instance. For creating a new function based of an existing one,
    see the :meth:`~FunctionBuilder.from_func` classmethod. At any
    point, :meth:`~FunctionBuilder.get_func` can be called to get a
    newly compiled function, based on the values configured.

    >>> fb = FunctionBuilder('return_five', doc='returns the integer 5',
    ...                      body='return 5')
    >>> f = fb.get_func()
    >>> f()
    5
    >>> fb.varkw = 'kw'
    >>> f_kw = fb.get_func()
    >>> f_kw(ignored_arg='ignored_val')
    5

    Note that function signatures themselves changed quite a bit in
    Python 3, so several arguments are only applicable to
    FunctionBuilder in Python 3. Except for *name*, all arguments to
    the constructor are keyword arguments.

    Args:
        name (str): Name of the function.
        doc (str): `Docstring`_ for the function, defaults to empty.
        module (str): Name of the module from which this function was
            imported. Defaults to None.
        body (str): String version of the code representing the body
            of the function. Defaults to ``'pass'``, which will result
            in a function which does nothing and returns ``None``.
        args (list): List of argument names, defaults to empty list,
            denoting no arguments.
        varargs (str): Name of the catch-all variable for positional
            arguments. E.g., "args" if the resultant function is to have
            ``*args`` in the signature. Defaults to None.
        varkw (str): Name of the catch-all variable for keyword
            arguments. E.g., "kwargs" if the resultant function is to have
            ``**kwargs`` in the signature. Defaults to None.
        defaults (tuple): A tuple containing default argument values for
            those arguments that have defaults.
        kwonlyargs (list): Argument names which are only valid as
            keyword arguments. **Python 3 only.**
        kwonlydefaults (dict): A mapping, same as normal *defaults*,
            but only for the *kwonlyargs*. **Python 3 only.**
        annotations (dict): Mapping of type hints and so
            forth. **Python 3 only.**
        filename (str): The filename that will appear in
            tracebacks. Defaults to "boltons.funcutils.FunctionBuilder".
        indent (int): Number of spaces with which to indent the
            function *body*. Values less than 1 will result in an error.
        dict (dict): Any other attributes which should be added to the
            functions compiled with this FunctionBuilder.

    All of these arguments are also made available as attributes which
    can be mutated as necessary.

    .. _Docstring: https://en.wikipedia.org/wiki/Docstring#Python

    """

    _argspec_defaults = {
        "args": list,
        "varargs": lambda: None,
        "varkw": lambda: None,
        "defaults": lambda: None,
        "kwonlyargs": list,
        "kwonlydefaults": dict,
        "annotations": dict,
    }

    @classmethod
    def _argspec_to_dict(cls, f):
        argspec = inspect.getfullargspec(f)
        return {attr: getattr(argspec, attr) for attr in cls._argspec_defaults}

    _defaults = {
        "doc": str,
        "dict": dict,
        "is_async": lambda: False,
        "module": lambda: None,
        "body": lambda: "pass",
        "indent": lambda: 4,
        "annotations": dict,
        "filename": lambda: "boltons.funcutils.FunctionBuilder",
    }

    _defaults.update(_argspec_defaults)

    _compile_count = itertools.count()

    def __init__(self, name, **kw):
        self.name = name
        for a, default_factory in self._defaults.items():
            val = kw.pop(a, None)
            if val is None:
                val = default_factory()
            setattr(self, a, val)

        if kw:
            raise TypeError("unexpected kwargs: %r" % kw.keys())
        return

    # def get_argspec(self):  # TODO

    def get_sig_str(self, with_annotations=True):
        """Return function signature as a string.

        with_annotations is ignored on Python 2.  On Python 3 signature
        will omit annotations if it is set to False.
        """
        if with_annotations:
            annotations = self.annotations
        else:
            annotations = {}

        return inspect_formatargspec(
            self.args, self.varargs, self.varkw, [], self.kwonlyargs, {}, annotations
        )

    _KWONLY_MARKER = re.compile(
        r"""
    \*     # a star
    \s*    # followed by any amount of whitespace
    ,      # followed by a comma
    \s*    # followed by any amount of whitespace
    """,
        re.VERBOSE,
    )

    def get_invocation_str(self):
        kwonly_pairs = None
        formatters = {}
        if self.kwonlyargs:
            kwonly_pairs = {arg: arg for arg in self.kwonlyargs}
            formatters["formatvalue"] = lambda value: "=" + value

        sig = inspect_formatargspec(
            self.args,
            self.varargs,
            self.varkw,
            [],
            kwonly_pairs,
            kwonly_pairs,
            {},
            **formatters,
        )
        sig = self._KWONLY_MARKER.sub("", sig)
        return sig[1:-1]

    @classmethod
    def from_func(cls, func):
        """Create a new FunctionBuilder instance based on an existing
        function. The original function will not be stored or
        modified.
        """
        # TODO: copy_body? gonna need a good signature regex.
        # TODO: might worry about __closure__?
        if not callable(func):
            raise TypeError("expected callable object, not {!r}".format(func))

        if isinstance(func, partial):
            kwargs = {
                "name": func.__name__,
                "doc": func.__doc__,
                "module": getattr(func, "__module__", None),  # e.g., method_descriptor
                "annotations": getattr(func, "__annotations__", {}),
                "dict": getattr(func, "__dict__", {}),
            }

        kwargs.update(cls._argspec_to_dict(func))

        if inspect.iscoroutinefunction(func):
            kwargs["is_async"] = True

        return cls(**kwargs)

    def get_func(self, execdict=None, add_source=True, with_dict=True):
        """Compile and return a new function based on the current values of
        the FunctionBuilder.

        Args:
            execdict (dict): The dictionary representing the scope in
                which the compilation should take place. Defaults to an empty
                dict.
            add_source (bool): Whether to add the source used to a
                special ``__source__`` attribute on the resulting
                function. Defaults to True.
            with_dict (bool): Add any custom attributes, if
                applicable. Defaults to True.

        To see an example of usage, see the implementation of
        :func:`~boltons.funcutils.wraps`.
        """
        execdict = execdict or {}
        body = self.body or self._default_body

        tmpl = "def {name}{sig_str}:"
        tmpl += "\n{body}"

        if self.is_async:
            tmpl = "async " + tmpl

        body = _indent(self.body, " " * self.indent)

        name = self.name.replace("<", "_").replace(">", "_")  # lambdas
        src = tmpl.format(
            name=name,
            sig_str=self.get_sig_str(with_annotations=False),
            doc=self.doc,
            body=body,
        )
        self._compile(src, execdict)
        func = execdict[name]

        func.__name__ = self.name
        func.__doc__ = self.doc
        func.__defaults__ = self.defaults
        func.__kwdefaults__ = self.kwonlydefaults
        func.__annotations__ = self.annotations

        if with_dict:
            func.__dict__.update(self.dict)
        func.__module__ = self.module
        # TODO: caller module fallback?

        if add_source:
            func.__source__ = src

        return func

    def get_defaults_dict(self):
        """Get a dictionary of function arguments with defaults and the
        respective values.
        """
        ret = dict(
            reversed(list(zip(reversed(self.args), reversed(self.defaults or []))))
        )
        kwonlydefaults = getattr(self, "kwonlydefaults", None)
        if kwonlydefaults:
            ret.update(kwonlydefaults)
        return ret

    def get_arg_names(self, only_required=False):
        arg_names = tuple(self.args) + tuple(getattr(self, "kwonlyargs", ()))
        if only_required:
            defaults_dict = self.get_defaults_dict()
            arg_names = tuple([an for an in arg_names if an not in defaults_dict])
        return arg_names

    def add_arg(self, arg_name, default=NO_DEFAULT, kwonly=False):
        """Add an argument with optional *default* (defaults to
        ``funcutils.NO_DEFAULT``). Pass *kwonly=True* to add a
        keyword-only argument
        """
        if arg_name in self.args:
            raise ExistingArgument(
                "arg {!r} already in func {} arg list".format(arg_name, self.name)
            )
        if arg_name in self.kwonlyargs:
            raise ExistingArgument(
                "arg {!r} already in func {} kwonly arg list".format(
                    arg_name, self.name
                )
            )
        if not kwonly:
            self.args.append(arg_name)
            if default is not NO_DEFAULT:
                self.defaults = (self.defaults or ()) + (default,)
        else:
            self.kwonlyargs.append(arg_name)
            if default is not NO_DEFAULT:
                self.kwonlydefaults[arg_name] = default
        return

    def remove_arg(self, arg_name):
        """Remove an argument from this FunctionBuilder's argument list. The
        resulting function will have one less argument per call to
        this function.

        Args:
            arg_name (str): The name of the argument to remove.

        Raises a :exc:`ValueError` if the argument is not present.

        """
        args = self.args
        d_dict = self.get_defaults_dict()
        try:
            args.remove(arg_name)
        except ValueError:
            try:
                self.kwonlyargs.remove(arg_name)
            except (AttributeError, ValueError):
                # py2, or py3 and missing from both
                exc = MissingArgument(
                    "arg %r not found in %s argument list:"
                    " %r" % (arg_name, self.name, args)
                )
                exc.arg_name = arg_name
                raise exc
            else:
                self.kwonlydefaults.pop(arg_name, None)
        else:
            d_dict.pop(arg_name, None)
            self.defaults = tuple([d_dict[a] for a in args if a in d_dict])
        return

    def _compile(self, src, execdict):
        filename = "<%s-%d>" % (
            self.filename,
            next(self._compile_count),
        )
        try:
            code = compile(src, filename, "single")
            exec(code, execdict)
        except Exception:
            raise
        return execdict


def deprecation_of(func, old_name):
    @wraps(func)
    def wrapper(*args, **kwargs):
        from warnings import warn

        warn(
            f"`{old_name}` is deprecated. Use `{func.__module__}.{func.__qualname__}` instead.",
            DeprecationWarning,
        )
        return func(*args, **kwargs)

    return wrapper


# ----------------------------------------------------------------------------------------------------------------------
# More or less vendored from config2py

FolderSpec = namedtuple("FolderSpec", ["env_var", "default_path"])

if os.name == "nt":
    APP_FOLDER_STANDARDS = dict(
        config=FolderSpec("APPDATA", os.getenv("APPDATA", "")),
        data=FolderSpec("LOCALAPPDATA", os.getenv("LOCALAPPDATA", "")),
        cache=FolderSpec(
            "LOCALAPPDATA", os.path.join(os.getenv("LOCALAPPDATA", ""), "Temp")
        ),
        state=FolderSpec("LOCALAPPDATA", os.getenv("LOCALAPPDATA", "")),
        runtime=FolderSpec("TEMP", os.getenv("TEMP", "")),
    )
else:
    APP_FOLDER_STANDARDS = dict(
        config=FolderSpec("XDG_CONFIG_HOME", "~/.config"),
        data=FolderSpec("XDG_DATA_HOME", "~/.local/share"),
        cache=FolderSpec("XDG_CACHE_HOME", "~/.cache"),
        state=FolderSpec("XDG_STATE_HOME", "~/.local/state"),
        runtime=FolderSpec("XDG_RUNTIME_DIR", "/tmp"),
    )

AppFolderKind = Literal["config", "data", "cache", "state", "runtime"]
DFLT_APP_FOLDER_KIND = "config"


def get_app_folder(folder_kind: AppFolderKind = DFLT_APP_FOLDER_KIND):
    """
    Get the full path of a directory suitable for storing application-specific configs,
    (or data, or cache, or state or runtime)

    On Windows, this is typically %APPDATA%.
    On macOS, this is typically ~/.config.
    On Linux, this is typically ~/.config.

    Parameters:
        folder_kind (str): The kind of folder to get. One of 'config', 'data', 'cache', 'state', 'runtime'.
            Defaults to 'config'.
            Here are concise explanations for each folder kind:
            **config**: User preferences and settings files (e.g., API keys, theme preferences, editor settings). Files users might edit manually or that define how the app behaves.
            **data**: Essential user-created content and application state (e.g., databases, saved games, user documents, session files). Data that should be backed up and persists across updates.
            **cache**: Temporary, regeneratable files (e.g., downloaded images, compiled assets, web cache). Can be safely deleted to free space without losing user work.
            **state**: Application state and logs that persist between sessions but aren't critical user data (e.g., command history, undo history, recently opened files, log files). Unlike cache, shouldn't be auto-deleted.
            **runtime**: Temporary runtime files that only exist while the app runs (e.g., PID files, Unix sockets, lock files, named pipes). Typically cleared on logout/reboot.
            **TL;DR**: config = settings, data = user files, cache = disposable, state = logs/history, runtime = process files.

    Returns:
        str: The full path of the app data folder.

    See https://github.com/i2mint/i2mint/issues/1.
    """
    env_var, default = APP_FOLDER_STANDARDS[folder_kind]
    return os.path.expanduser(os.getenv(env_var, default))


get_app_config_folder = partial(get_app_folder, folder_kind="config")
get_app_data_folder = partial(get_app_folder, folder_kind="data")
```

## wrapper.py

```python
"""A wrapper object and tools to work with it

How the ``Wrap`` class works:

.. code-block::

          *outer_args, **outer_kwargs
                     │
                     ▼
    ┌───────────────────────────────────┐
    │              ingress              │
    └───────────────────────────────────┘
                     │
                     ▼
          *inner_args, **inner_kwargs
                     │
                     ▼
    ┌───────────────────────────────────┐
    │               func                │
    └───────────────────────────────────┘
                     │
                     ▼
                 func_output
                     │
                     ▼
    ┌───────────────────────────────────┐
    │              egress               │
    └───────────────────────────────────┘
                     │
                     ▼
                final_output


How the ``Ingress`` class (ingress templated function maker) works:

.. code-block::

          *outer_args, **outer_kwargs
                     │
                     ▼
    ┌───────────────────────────────────┐
    │          outer_sig_bind           │
    └───────────────────────────────────┘
                     │
                     ▼
              outer_all_kwargs
                     │
                     ▼
    ┌───────────────────────────────────┐
    │            kwargs_trans           │
    └───────────────────────────────────┘
                     │
                     ▼
              inner_all_kwargs
                     │
                     ▼
    ┌───────────────────────────────────┐
    │          inner_sig_bind           │
    └───────────────────────────────────┘
                     │
                     ▼
          *inner_args, **inner_kwargs

"""

from functools import wraps, partial
from inspect import Parameter, signature
from typing import (
    Any,
    Optional,
    Union,
    NewType,
    Dict,
    Tuple,
)
from collections.abc import Mapping, Callable, Iterable, Sequence

from types import MethodType
from inspect import Parameter
from dataclasses import make_dataclass, dataclass

from i2.signatures import (
    Sig,
    name_of_obj,
    KO,
    PK,
    VK,
    parameter_to_dict,
    call_forgivingly,
    _call_forgivingly,
    kind_forgiving_func,
)
from i2.errors import OverwritesNotAllowed
from i2.multi_object import Pipe
from i2.deco import double_up_as_factory
from i2.util import deprecation_of

# ---------------------------------------------------------------------------------------
# Wrap

empty = Parameter.empty
OuterKwargs = dict
InnerKwargs = dict
KwargsTrans = Callable[[OuterKwargs], InnerKwargs]


def identity(x):
    """Transparent function, returning what's been input"""
    return x


def transparent_ingress(*args, **kwargs):
    """
    >>> transparent_ingress(1, 2, test=1)
    ((1, 2), {'test': 1})
    """

    return args, kwargs


def transparent_egress(output):
    """
    >>> transparent_egress('unnecessary_doctest')
    'unnecessary_doctest'
    """

    return output


class MakeFromFunc:
    """Used to indicate that an object should be made as a function of an input func"""

    def __init__(self, func_to_obj):
        self.func_to_obj = func_to_obj

    def __call__(self, func):
        return self.func_to_obj(func)


# TODO: Continue factoring out Wrap and Wrapx code
class _Wrap:
    """To be used as the base of actual Wrap objects."""

    _init_args = ()
    _init_kwargs = ()

    def __init__(self, func, *args, **kwargs):
        self._init_args = (func, *args)
        self._init_kwargs = kwargs
        # wraps(func)(self) is there to copy over to self anything that func may
        # have had. It should be before anything else so it doesn't overwrite stuff
        # that we may add to self in init (like .func for example!)
        wraps(func)(self)  # TODO: should we really copy everything by default?
        if (name := kwargs.get("name", None)) is not None:
            self.__name__ = name
        self.func = func  # Note: overwrites self.func that wraps MAY have inserted
        self.__wrapped__ = func
        # TODO: Pros and cons analysis of pointing __wrapped__ to func. partial uses
        #  .func, but wraps looks for __wrapped__

    def __call__(self, *args, **kwargs):
        """Just forward the call to the wrapped function."""
        return self.func(*args, **kwargs)

    def __reduce__(self):
        """reduce is meant to control how things are pickled/unpickled"""
        return type(self), self._init_args, dict(self._init_kwargs)

    def __get__(self, instance, owner):
        """This method allows things to work well when we use Wrap object as method"""
        if instance is None:
            return self
        return MethodType(self, instance)

    def __repr__(self):
        # TODO: Replace i2.Wrap with dynamic (Wrap or Wrapx)
        name = getattr(self, "__name__", None) or "Wrap"
        return f"<i2.Wrap {name}{signature(self)}>"

    # TODO: Don't know exactly what I'm doing below. Review with someone!
    def __set_name__(self, owner, name):
        """So that name of function is passed on to method when assigning to attribute
        That is, doing ``method = Wrap(func)`` in a class definition"""
        # TODO: Look into sanity of mutating the name and other ways to achieve same
        self.__name__ = name

    def __set__(self, instance, value):
        instance.__dict__[self.__name__] = value

    # To get help(instance.method) to work!
    # TODO: Does this have undesirable side effects?
    # TODO: Perhaps self.__call__.__code__ is better (since it's actually what runs)?
    @property
    def __code__(self):
        return self.func.__code__


def _defaults_and_kwdefaults_of_func(func: Callable):
    try:
        return func.__defaults__, func.__kwdefaults__
    except AttributeError:
        # if python can't do it on it's own, you may have a special kind of callable
        # here, so try doing it through Sig, which is more flexible
        sig = Sig(func)
        return sig._defaults_, sig._kwdefaults_


class Wrap(_Wrap):
    """A function wrapper with interface modifiers.

    :param func: The wrapped function
    :param ingress: The incoming data transformer. It determines the argument properties
        (name, kind, default and annotation) as well as the actual input of the
        wrapped function.
    :param egress: The outgoing data transformer. It also takes precedence over the
        wrapped function to determine the return annotation of the ``Wrap`` instance
    :param name: Name to give the wrapper (will use wrapped func name by default)
    :return: A callable instance wrapping ``func``

    Some examples:

    >>> from inspect import signature
    >>> from i2 import Sig

    >>> def func(a, b):
    ...     return a * b

    >>> wrapped_func = wrap(func)  # no transformations: wrapped_func is the same as func
    >>> assert wrapped_func(2, 'Hi') == func(2, 'Hi') == 'HiHi'

    Modifying the first argument

    >>> def ingress(a, b):
    ...   return (2 * a, b), dict()
    >>> wrapped_func = wrap(func, ingress=ingress)  # first variable is now multiplied by 2
    >>> wrapped_func(2, 'Hi')
    'HiHiHiHi'

    Same using keyword args, we need to use tuple to represent an empty tuple

    >>> def ingress(a, b):
    ...   return tuple(), dict(a=2 * a, b=b) # Note that b MUST be present as well, or an error will be raised
    >>> wrapped_func = wrap(func, ingress=ingress)  # first variable is now multiplied by 2
    >>> wrapped_func(2, 'Hi')
    'HiHiHiHi'

    Using both args and kwargs

    >>> def ingress(a, b):
    ...   return (2 * a, ), dict(b=b)
    >>> wrapped_func = wrap(func, ingress=ingress)  # first variable is now multiplied by 2
    >>> wrapped_func(2, 'Hi')
    'HiHiHiHi'

    We can use ingress to ADD parameters to func

    >>> def ingress(a, b, c):
    ...   return (a, b + c), dict()
    >>> wrapped_func = wrap(func, ingress=ingress)
    >>> # now wrapped_func takes three arguments
    >>> wrapped_func(2, 'Hi', 'world!')
    'Hiworld!Hiworld!'

    Egress is a bit more straightforward, it simply applies to the output of the
    wrapped function. We can use ingress to ADD parameters to func

    >>> def egress(output):
    ...   return output + ' ITSME!!!'
    >>> wrapped_func = wrap(func, ingress=ingress, egress=egress)
    >>> # now wrapped_func takes three arguments
    >>> wrapped_func(2, 'Hi', 'world!')
    'Hiworld!Hiworld! ITSME!!!'

    A more involved example:

    >>> def ingress(a, b: str, c="hi"):
    ...     return (a + len(b) % 2,), dict(string=f"{c} {b}")
    ...
    >>> def func(times, string):
    ...     return times * string
    ...
    >>> wrapped_func = wrap(func, ingress=ingress)
    >>> assert wrapped_func(2, "world! ", "Hi") == "Hi world! Hi world! Hi world! "
    >>>
    >>> wrapped_func = wrap(func, egress=len)
    >>> assert wrapped_func(2, "co") == 4 == len("coco") == len(func(2, "co"))
    >>>
    >>> wrapped_func = wrap(func, ingress=ingress, egress=len)
    >>> assert (
    ...     wrapped_func(2, "world! ", "Hi")
    ...     == 30
    ...     == len("Hi world! Hi world! Hi world! ")
    ... )

    An ``ingress`` function links the interface of the wrapper to the interface of the
    wrapped func; therefore it's definition often depends on information of both,
    and for that reason, we provide the ability to specify the ingress not only
    explicitly (as in the examples above), but through a factory -- a function that
    will be called on ``func`` to produce the ingress that should be used to wrap it.

    .. seealso::

        ``wrap`` function.

    """

    def __init__(self, func, ingress=None, egress=None, *, name=None):
        super().__init__(func, ingress, egress, name=name)
        ingress_sig = Sig(func)

        if ingress is None:
            self.ingress = transparent_ingress
            self.__defaults__, self.__kwdefaults__ = _defaults_and_kwdefaults_of_func(
                func
            )
        else:

            if isinstance(ingress, MakeFromFunc):
                func_to_ingress = ingress  # it's not the ingress function itself
                # ... but an ingress factory: Should make the ingress in function of func
                self.ingress = func_to_ingress(func)
            else:
                assert callable(ingress), f"Should be callable: {ingress}"
                self.ingress = ingress

            ingress_sig = Sig(self.ingress)
            self.__defaults__, self.__kwdefaults__ = _defaults_and_kwdefaults_of_func(
                self.ingress
            )

        return_annotation = empty

        if egress is None:
            self.egress = transparent_egress
        else:
            self.egress = egress
            egress_return_annotation = Sig(egress).return_annotation
            if egress_return_annotation is not Parameter.empty:
                return_annotation = egress_return_annotation

        self.__signature__ = Sig(ingress_sig, return_annotation=return_annotation)

    def __call__(self, *ingress_args, **ingress_kwargs):
        func_args, func_kwargs = self.ingress(*ingress_args, **ingress_kwargs)
        return self.egress(self.func(*func_args, **func_kwargs))


@double_up_as_factory
def wrap(
    func=None, *, ingress=None, egress=None, caller=None, name=None, dflt_wrap=Wrap
):
    """Wrap a function, optionally transforming interface, input and output.

    :param func: The wrapped function
    :param ingress: The incoming data transformer. It determines the argument properties
        (name, kind, default and annotation) as well as the actual input of the
        wrapped function.
    :param egress: The outgoing data transformer. It also takes precedence over the
        wrapped function to determine the return annotation of the ``Wrap`` instance
    :return: A callable instance wrapping ``func``

    Consider the following function.

    >>> def f(w, /, x: float = 1, y=2, *, z: int = 3):
    ...     return w + x * y ** z
    ...
    >>> assert f(0) == 8
    >>> assert f(1,2) == 17 == 1 + 2 * 2 ** 3

    See that ``f`` is restricted to use ``z`` as keyword only argument kind:

    >>> f(1, 2, 3, 4)
    Traceback (most recent call last):
      ...
    TypeError: f() takes from 1 to 3 positional arguments but 4 were given

    and ``w`` has position only argument kind:

    >>> f(w=1, x=2, y=3, z=4)
    Traceback (most recent call last):
      ...
    TypeError: f() got some positional-only arguments passed as keyword arguments: 'w'

    Say we wanted a version of this function that didn't have the argument kind
    restrinctions, where the annotation of ``x`` was ``int`` and where the default
    of ``z`` was ``10`` instead of ``3``, and doesn't have an annotation.
    We can do so using the following ingress function:

    >>> def ingress(w, x: int = 1, y: int=2, z = 10):
    ...     return (w,), dict(x=x, y=y, z=z)

    The ingress function serves two purposes:

    - Redefining the signature (i.e. the argument names, kinds, defaults,
    and annotations (not including the return annotation, which is taken care of by the
    egress argument).

    - Telling the wrapper how to get from that interface to the interface of the
    wrapped function.

    If we also wanted to add a return_annotation, we could do so via an ``egress``
    function argument:

    >>> def egress(wrapped_func_output) -> float:
    ...     return wrapped_func_output  # because here we don't want to do anything extra

    Now we can use these ingress and egress functions to get the version of ``f`` of
    our dreams:

    >>> h = wrap(f, ingress=ingress, egress=egress)

    Let's see what the signature of our new function looks like:

    >>> from inspect import signature
    >>> str(signature(h))
    '(w, x: int = 1, y: int = 2, z=10) -> float'

    Now let's see that we can actually use this new function ``h``, without the
    restrictions of argument kind, getting the same results as the wrapped ``f``,
    but with default ``z=10``.

    What we wanted (but couldn't) do with ``f``:

    >>> h(1, 2, 3, 4)  # == 1 + 2 * 3 ** 4
    163
    >>> h(w=1, x=2, y=3, z=4)
    163

    >>> assert h(0) == h(0, 1) == h(0, 1, 2) == 0 + 1 * 2 ** 10 == 2 ** 10 == 1024

    Note that ``wrap`` can also be used as a decorator "factory", for instance to
    wrap functions at definition time, and if we change ``caller`` it will automatically
    use ``Wrapx`` instead of ``Wrap`` to wrap the function.

    >>> def iterize(func, args, kwargs):
    ...     first_arg_val = next(iter(kwargs.values()))
    ...     return list(map(func, first_arg_val))
    >>>
    >>> @wrap(caller=iterize)
    ... def func(x, y=2):
    ...     return x + y
    ...
    >>> isinstance(func, Wrapx)
    True
    >>> func([1, 2, 3, 4])
    [3, 4, 5, 6]

    For more examples, see also the

    .. seealso::

        ``Wrap`` class.
        ``Wrapx`` class.

    """
    if _should_use_wrapx(func, ingress, egress, caller):
        return Wrapx(func, ingress, egress, caller=caller, name=name)
    else:
        return dflt_wrap(func, ingress, egress, name=name)


# TODO: Add conditions on egress to route to Wrapx when complex egress
def _should_use_wrapx(func, ingress, egress, caller):
    if caller is not None:
        return True
    else:
        return False


def append_empty_args(func):
    """To use to transform an ingress function that only returns kwargs to one that
    returns the normal form of ingress functions: ((), kwargs)"""

    @wraps(func)
    def _func(*args, **kwargs):
        return (), func(*args, **kwargs)

    return _func


class Ingress:
    r"""The Ingress class offers a template for creating ingress classes.

    Note that when writing a decorator with ``i2.wrapper``, you're usually better off
    writing an ingress function for the purpose. As a result, your code will usually
    be less complex, easier to read, and more efficient than using the Ingress class.

    So why use the ``Ingress`` class at all? For one, because it'll take care of some
    common mechanics for you, so once you understand how to use it, you'll probably
    create a correct wrapper faster.

    Further, if you're writing a general wrapping tool (e.g. your own currying machine,
    some rule-based input casting function, etc.) then you'll find that using
    Ingres will usually with on the complexity, readability and/or efficiency front.

    Consider the following function:

    >>> def f(w, /, x: float, y=2, *, z: int = 3):
    ...     return f"(w:={w}) + (x:={x}) * (y:={y}) ** (z:={z}) == {w + x * y ** z}"
    >>>
    >>> f(0, 1)
    '(w:=0) + (x:=1) * (y:=2) ** (z:=3) == 8'

    Let’s say you wanted to dispatch this function to a command line interface,
    or a webservice where all arguments are taken from the url.
    The problem here is that this means that all incoming values will be strings
    in that case.
    Say you wanted all input values to be cast to ints. In that case you could do:

    >>> from i2.wrapper import Ingress, wrap
    >>> from inspect import signature
    >>>
    >>> trans_all_vals_to_ints = lambda d: {k: int(v) for k, v in d.items()}
    >>>
    >>> cli_f = wrap(
    ...     f,
    ...     ingress=Ingress(signature(f), kwargs_trans=trans_all_vals_to_ints)
    ... )
    >>>
    >>> cli_f("2", "3", "4")
    '(w:=2) + (x:=3) * (y:=4) ** (z:=3) == 194'

    In a more realistic situation, you'd want to have more control over this value
    transformation.

    Say you wanted to convert to int if it's possible, try float if not,
    and just leave the string alone otherwise.

    >>> def _try_casting_to_numeric(x):
    ...     try:
    ...         return int(x)
    ...     except ValueError:
    ...         try:
    ...             return float(x)
    ...         except ValueError:
    ...             return x
    ...
    >>> def cast_numbers(d: dict):
    ...     return {k: _try_casting_to_numeric(v) for k, v in d.items()}
    >>>
    >>> cli_f = wrap(f, ingress=Ingress(signature(f), kwargs_trans=cast_numbers))
    >>>
    >>> cli_f("2", "3.14", "4")
    '(w:=2) + (x:=3.14) * (y:=4) ** (z:=3) == 202.96'

    Let's say that our values transformations are not all 1-to-1 as in the examples
    above.
    Instead, they can be

    - ``1-to-many`` (e.g. the outer ``w`` is used to compute the inner ``w`` and ``x``)

    - ``many-to-1`` (e.g. the outer ``x`` and ``y`` are used to compute inner ``y``)

    .. code-block::

          w   x   y   z
         / \   \ /    |
        w   x   y     z


    >>> def kwargs_trans(outer_kw):
    ...     return dict(
    ...         # e.g. 1-to-many: one outer arg (w) producing two inner args (w, and y)
    ...         w=outer_kw['w'] * 2,
    ...         x=outer_kw['w'] * 3,
    ...         # e.g. many-to-1: two outer args (x and y) producing one inner arg (y)
    ...         y=outer_kw['x'] + outer_kw['y'],
    ...         # Note that no z is mentioned: This means we're just leaving it alone
    ...     )
    ...
    >>>
    >>> ingress = Ingress(signature(f), kwargs_trans=kwargs_trans)
    >>> assert ingress(2, x=3, y=4) == ((4,), {'x': 6, 'y': 7, 'z': 3})
    >>>
    >>> wrapped_f = wrap(f, ingress=ingress)
    >>> assert wrapped_f(2, x=3, y=4) == '(w:=4) + (x:=6) * (y:=7) ** (z:=3) == 2062'


    The following is an example that involves several aspects of the ``Ingress`` class.

    >>> from i2 import Sig
    >>> def kwargs_trans(outer_kw):
    ...     return dict(
    ...         w=outer_kw['w'] * 2,
    ...         x=outer_kw['w'] * 3,
    ...         # need to pop you (inner func has no you argument)
    ...         y=outer_kw['x'] + outer_kw.pop('you'),
    ...         # Note that no z is mentioned: This means we're just leaving it alone
    ...     )
    >>>
    >>> ingress = Ingress(
    ...     inner_sig=signature(f),
    ...     kwargs_trans=kwargs_trans,
    ...     outer_sig=Sig(f).ch_names(y='you')  # need to give the outer sig a you
    ...     # You could also express it this way (though you'd lose the annotations)
    ...     # outer_sig=lambda w, /, x, you=2, *, z=3: None
    ... )
    >>> assert ingress(2, x=3, you=4) == ((4,), {'x': 6, 'y': 7, 'z': 3})
    >>>
    >>> wrapped_f = wrap(f, ingress=ingress)
    >>> assert wrapped_f(2, x=3, you=4) == '(w:=4) + (x:=6) * (y:=7) ** (z:=3) == 2062'

    A convenience method allows to do the same with the ingress instance itself:

    >>> wrapped_f = ingress.wrap(f)
    >>> assert wrapped_f(2, x=3, you=4) == '(w:=4) + (x:=6) * (y:=7) ** (z:=3) == 2062'
    """

    def __init__(
        self,
        inner_sig,
        kwargs_trans: KwargsTrans | None = None,
        outer_sig=None,
        *,
        allow_excess=True,
        apply_defaults=True,
        allow_partial=False,
    ):
        """Init of an Ingress instance.

        :param inner_sig: Signature of the inner function the ingress is for.
            The function itself can be given and the signature will be extracted.
        :param kwargs_trans: A dict-to-dict transformation of the outer kwargs to
            the kwargs that should be input to the inner function.
            That is ``kwargs_trans`` is ``outer_kwargs -> inner_kwargs``.
            Note that though both outer and inner signatures could have those annoying
            position-only kinds, you don't have to think of that.
            The parameter kind restrictions are taken care of automatically.
        :param outer_sig: The outer signature. The ingress function's signature will
            have (and therefore, the wrapped function's signature too.
            Also serves to convert input ``(args, kwargs)`` to the ``kwargs``
            to the kwargs that will be given to ``kwargs_trans``.
        :param allow_excess: Whether to allow the inner kwargs to have some excess
            variables in them. This enables more flexibility in the outer signature,
            but may want to set to ``False`` to be more explicit.
        :param apply_defaults: Whether to apply the defaults of the outer signature.
            Default is ``True`` but in some rare cases, you may not want to apply them.

        When making an Ingress function directly, one must take care that
        ``inner_sig``, ``kwargs_trans`` and ``outer_sig`` are aligned.

        Namely, ``kwargs_trans`` must be able to handle outputs of
        ``outer_sig.map_arguments`` and itself output kwargs that
        can be handled by ``inner_sig.mk_args_and_kwargs``.

        """
        self.inner_sig = Sig(inner_sig)

        # kwargs_trans should be callable and have one required arg: a dict
        # if it's None, we'll just make it be the identity function
        if kwargs_trans is None:
            kwargs_trans = identity
        self.kwargs_trans = kwargs_trans

        # default to inner_sig = outer_sig
        if outer_sig is None:
            outer_sig = inner_sig
        self.outer_sig = Sig(outer_sig)

        self.outer_sig(self)
        self.apply_defaults = apply_defaults
        self.allow_excess = allow_excess
        self.allow_partial = allow_partial

    def __call__(self, *ingress_args, **ingress_kwargs):
        # Get the all-keywords version of the arguments (args,kwargs->kwargs)
        func_kwargs = self.outer_sig.map_arguments(
            ingress_args, ingress_kwargs, apply_defaults=self.apply_defaults
        )

        func_kwargs = dict(
            func_kwargs,  # by default, keep the func_kwargs, but
            **self.kwargs_trans(func_kwargs),  # change those that kwargs_trans desires
        )

        # Return an (args, kwargs) pair that respects the inner function's
        # argument kind restrictions.
        # TODO: Reflect on pros/cons of allow_excess=True
        return self.inner_sig.mk_args_and_kwargs(
            func_kwargs,
            apply_defaults=True,
            allow_excess=self.allow_excess,
            allow_partial=self.allow_partial,
        )

    def __repr__(self):
        return f"Ingress signature: {signature(self)}"

    def wrap(self, func: Callable, egress=None, *, name=None) -> Wrap:
        """Convenience method to wrap a function with the instance ingress.
        ``ingress.wrap(func,...)`` equivalent to ``Wrap(func, ingress, ...)``
        """
        return Wrap(func, ingress=self, egress=egress, name=name)

    @classmethod
    def name_map(cls, wrapped, **old_to_new_name):
        """Change argument names.

        >>> def f(w, /, x: float, y=2, *, z: int = 3):
        ...     return f"(w:={w}) + (x:={x}) * (y:={y}) ** (z:={z}) == {w + x * y ** z}"
        >>> ingress = Ingress.name_map(f, w='DoubleYou', z='Zee')
        >>> ingress
        Ingress signature: (DoubleYou, /, x: float, y=2, *, Zee: int = 3)
        >>> wrapped_f = ingress.wrap(f)
        >>> wrapped_f(1, 2, y=3, Zee=4)
        '(w:=1) + (x:=2) * (y:=3) ** (z:=4) == 163'

        """
        new_to_old_name = {v: k for k, v in old_to_new_name.items()}
        assert len(new_to_old_name) == len(
            old_to_new_name
        ), f"Inversion is not possible since {old_to_new_name=} has duplicate values."
        return cls(
            wrapped,
            partial(Pipe(items_with_mapped_keys, dict), key_mapper=new_to_old_name),
            Sig(wrapped).ch_names(**old_to_new_name),
        )

    #     @classmethod
    #     def defaults(cls, wrapped, **defaults):
    #         """"""
    #
    #     @classmethod
    #     def order(cls, wrapped, arg_order):
    #         """"""
    #
    #     @classmethod
    #     def factory(cls, wrapped, **func_for_name):
    #         """"""


def items_with_mapped_keys(d: dict, key_mapper):
    """Transform dict keys. More precisely yield (new_k,v) pairs from a key mapper dict.

    :param d: src dict
    :param key_mapper: {old_name: new_name, ...} mapping
    :return: generator of (new_name, value) pairs

    Often used in conjunction with dict:

    >>> dict(items_with_mapped_keys(
    ...     {'a': 1, 'b': 2, 'c': 3, 'd': 4},
    ...     {'a': 'Ay', 'd': 'Dee'})
    ... )
    {'Ay': 1, 'b': 2, 'c': 3, 'Dee': 4}

    """
    for k, v in d.items():
        # key_mapper.get(k, k) will give the new key name if present,
        # else will use the old
        yield key_mapper.get(k, k), v


def invert_map(d: dict):
    new_d = {v: k for k, v in d.items()}
    if len(new_d) == len(d):
        return new_d
    else:
        raise ValueError(f"There are duplicate keys so I can invert map: {d}")


def parameters_to_dict(parameters):
    return {name: parameter_to_dict(param) for name, param in parameters.items()}


def _handle_ingress_class_inputs(
    inner_sig,
    kwargs_trans: KwargsTrans | None,
    outer_sig,
    *,
    _allow_reordering=False,
):
    inner_sig = Sig(inner_sig)

    # kwargs_trans should be callable and have one required arg: a dict
    # if it's None, we'll just make it be the identity function
    if kwargs_trans is None:
        kwargs_trans = identity

    # default to inner_sig = outer_sig
    if outer_sig is None:
        outer_sig = inner_sig  # if nothing specified, want same outer and inner sigs
    elif isinstance(outer_sig, Mapping):
        changes_for_name = outer_sig  # it's a dict of modifications of the inner sig
        outer_sig = inner_sig.modified(
            _allow_reordering=_allow_reordering, **changes_for_name
        )
    else:
        outer_sig = Sig(outer_sig)

    return inner_sig, kwargs_trans, outer_sig


# TODO: See what this adds over ``Ingress`` class. Consider merging or reusing.
class InnerMapIngress:
    """A class to help build ingresses systematically by mapping the inner signature.

    *Systematically*, i.e. "according to a fixed plan/system" is what it's about here.
    As we'll see below, if you need to write a particular adapter for a specific case,
    you probably should do by writing an actual ingress function directly.
    In cases where you might want to apply a same logic to wrap many functions,
    you may want to fix that wrapping logic: ``InnerMapIngress`` provides one
    way to do this.

    :param inner_sig: The signature of the wrapped function.
    :param kwargs_trans: A dict-to-dict transformation of the outer kwargs to
        the kwargs that should be input to the inner function.
        That is ``kwargs_trans`` is ``outer_kwargs -> inner_kwargs``.
        Note that though both outer and inner signatures could have those annoying
        position-only kinds, you don't have to think of that.
        The parameter kind restrictions are taken care of automatically.
    :param _allow_reordering: Whether we want to allow reordering of variables
    :param in_to_out_sig_changes: The ``inner_name=dict_of_changes_for_that_name``
    pairs, the ``dict_of_changes_for_that_name`` is a ``dict`` with keys being valid
    ``inspect.Parameter``

    Consider the following function that has a position only, a keyword only,
    two arguments with annotations, and three with a default.

    >>> def f(w, /, x: float = 1, y=2, *, z: int = 3):
    ...     return w + x * y ** z

    Say we wanted a version of this function

    - that didn't have the argument kind restrinctions (all POSITION_OR_KEYWORD),

    - where the annotation of ``x`` was changed ``int`` and the default removed

    - where ``y`` was named ``you`` instead, and has an annotation (``int``).

    - where the default of ``z`` was ``10`` instead of ``3``, and doesn't have an
        annotation.

    In order to get a version of this function we wanted (more lenient kinds,
    with some annotations and a default change), we can use the ingress function:

    >>> def directly_defined_ingress(w, x: int, you: int=2, z = 10):
    ...     return (w,), dict(x=x, y=you, z=z)


    When we need to wrap a specific function in a specific way, defining an
    ingress function  this way is usually the simplest way.
    But in some cases we need to build the ingress function using some predefined
    rule/protocol to make applying the rule/protocol systematic.

    For those cases, ``InnerMapIngress`` comes in handy.

    With ``InnerMapIngress`` we'd build our ingress function like this:

    >>> from inspect import Parameter, signature
    >>> PK = Parameter.POSITIONAL_OR_KEYWORD
    >>> empty = Parameter.empty
    >>> ingress = InnerMapIngress(
    ...     f,
    ...     # change kind to PK:
    ...     w=dict(kind=PK),
    ...     # change annotation of x from float to int and remove default
    ...     x=dict(annotation=int, default=empty),
    ...     # rename y to you and add annotation int:
    ...     y=dict(name='you', annotation=int),
    ...     # change kind to PK, default to 10, and remove annotation:
    ...     z=dict(kind=PK, default=10, annotation=empty),
    ... )

    Note:

    - Only the changes we wish to make to the parameters are mentioned.
        You could also define the parameters explicitly by simply listing all three
        of the dimensions (kind, annotation, and default)

    - Three? But a ``Parameter`` object has four; what about the name?
        Indeed, you can use name as well, more on that later.

    - Note that in order to specify that you want no default, or no annotation,
        you cannot use ``None`` since ``None`` is both a valid default and a valid
        annotation; So instead you need to use ``Parameter.empty`` (conveniently
        assigned to a constant named ``empty`` in the ``wrapping`` module.

    Now see that all arguments are ``POSITIONAL_OR_KEYWORD``, ``x`` and ``y`` are
    ``int``, and default of ``z`` is 10:

    >>> assert (
    ...     str(signature(ingress))
    ...     == str(signature(directly_defined_ingress))
    ...     == '(w, x: int, you: int = 2, z=10)'
    ... )

    Additionally, ``ingress`` function does it's job of dispatching the right args
    and kwargs to the target function:

    >>> assert (
    ...     ingress(0,1,2,3)
    ...     == directly_defined_ingress(0,1,2,3)
    ...     == ((0,), {'x': 1, 'y': 2, 'z': 3})
    ... )


    """

    def __init__(
        self,
        inner_sig,
        kwargs_trans: KwargsTrans | None = None,
        *,
        _allow_reordering=False,
        **changes_for_name,
    ):
        self.inner_sig = Sig(inner_sig)

        self.outer_sig = self.inner_sig.modified(
            _allow_reordering=_allow_reordering, **changes_for_name
        )

        # kwargs_trans should be callable and have one required arg: a dict
        # if it's None, we'll just make it be the identity function
        if kwargs_trans is None:
            kwargs_trans = identity
        self.kwargs_trans = kwargs_trans

        outer_name_for_inner_name = {
            inner_name: change["name"]
            for inner_name, change in changes_for_name.items()
            if "name" in change
        }
        self.inner_name_for_outer_name = invert_map(outer_name_for_inner_name)
        self.outer_sig(self)

    def __call__(self, *ingress_args, **ingress_kwargs):
        # Get the all-keywords version of the arguments (args,kwargs->kwargs)
        func_kwargs = self.outer_sig.map_arguments(
            ingress_args, ingress_kwargs, apply_defaults=True
        )

        # Modify the keys of func_kwargs so they reflect the inner signature's names
        # That is, map outer names to inner names.
        func_kwargs = dict(
            items_with_mapped_keys(func_kwargs, self.inner_name_for_outer_name)
        )
        func_kwargs = dict(
            func_kwargs,  # by default, keep the func_kwargs, but
            **self.kwargs_trans(func_kwargs),  # change those that kwargs_trans desires
        )

        # TODO: See Ingress: Has allow_partial etc.
        # Return an (args, kwargs) pair the respects the inner function's
        # argument kind restrictions.
        return self.inner_sig.mk_args_and_kwargs(func_kwargs)

    # TODO: Use/test annotations of outer_sig
    @classmethod
    def from_signature(cls, inner_sig, outer_sig, _allow_reordering=False):
        """
        A convienience ingress constructor to specify wrappings that affect arguments
        independently.

        :param inner_sig: The signature of wrapped, inner function (or the inner
        function itself)
        :param outer_sig: The desired outer signature. Can also use a function (will
        only take it's signature though).
        :param _allow_reordering: Whether to allow ``outer_sig`` to reorder arguments.
        :return: An ingress that will allow one to use a function having the
        ``inner_sig`` signature to

        Say we wanted to get a version of the function:

        >>> def f(w, /, x: float = 1, y=2, *, z: int = 3):
        ...     return w + x * y ** z

        That was equivalent to (note the kind, default and annotation differences):

        >>> def g(w, x=1, y: float = 2.0, z=10):
        ...     return w + x * y ** z

        >>> h = wrap(f, ingress=InnerMapIngress.from_signature(f, g))
        >>> Sig(h)
        <Sig (w, x=1, y: float = 2.0, z=10)>

        Note we could have used ``...from_signature(Sig(f), Sig(g))`` as well,
        since the method doesn't use the actual functions, just their signatures.

        So we've seen that ``h`` takes on the signature (kind, defaults,
        and annotations) of ``g``.
        Let's see now that ``h`` actually computes, uses the defaults of ``g`` and
        can doesn't have the position only restriction on ``w``.

        >>> assert h(0) == g(0) == 1024 == 0 + 1 * 2 ** 10
        >>> assert h(1,2) == g(1,2) == 2049 == 1 + 2 * 2 ** 10
        >>> assert h(1,2,3,4) == g(1,2,3,4) == 1 + 2 * 3 ** 4
        >>>
        >>> assert h(w=1,x=2,y=3,z=4) == g(1,2,3,4) == 1 + 2 * 3 ** 4  # w keyword arg!

        """
        outer_sig = Sig(outer_sig)
        return cls(
            inner_sig,
            _allow_reordering=_allow_reordering,
            **parameters_to_dict(outer_sig.parameters),
        )


# TODO: Fits global pattern -- merge
class ArgNameMappingIngress:
    def __init__(self, inner_sig, *, conserve_kind=False, **outer_name_for_inner_name):
        self.inner_sig = Sig(inner_sig)
        self.outer_sig = self.inner_sig.ch_names(**outer_name_for_inner_name)
        if conserve_kind is not True:
            self.outer_sig = self.outer_sig.ch_kinds_to_position_or_keyword()
        self.inner_name_for_outer_name = invert_map(outer_name_for_inner_name)
        self.outer_sig(self)

    def __call__(self, *ingress_args, **ingress_kwargs):
        # Get the all-keywords version of the arguments (args,kwargs->kwargs)
        func_kwargs = self.outer_sig.map_arguments(ingress_args, ingress_kwargs)
        # Modify the keys of func_kwargs so they reflect the inner signature's names
        # That is, map outer names to inner names.
        func_kwargs = dict(
            items_with_mapped_keys(func_kwargs, self.inner_name_for_outer_name)
        )

        # Return an (args,kwargs) pair the respects the inner function's
        # argument kind restrictions.
        return self.inner_sig.mk_args_and_kwargs(func_kwargs)


def mk_ingress_from_name_mapper(func, name_mapper: Mapping, *, conserve_kind=False):
    return ArgNameMappingIngress(func, conserve_kind=conserve_kind, **name_mapper)


# TODO: Replace with i2.wrapper instead
# TODO: Make sure VARIADICs are handled properly, or error raised if present


def apply_func_on_cond(func, cond, k, v):
    if cond(k, v):
        return func(v)
    else:
        return v


def modify_dict_on_cond(d, cond, func):
    return {k: apply_func_on_cond(func, cond, k, v) for k, v in d.items()}


def convert_VK_to_KO(kinds):
    cond = lambda k, v: v == VK
    func = lambda v: KO

    return modify_dict_on_cond(kinds, cond, func)


nice_kinds = deprecation_of(kind_forgiving_func, "nice_kinds")


def wrap_from_sig(func, new_sig):
    from i2 import call_somewhat_forgivingly

    @wraps(func)
    def _func(*args, **kwargs):
        return call_somewhat_forgivingly(func, args, kwargs, enforce_sig=new_sig)

    _func.__signature__ = new_sig

    return _func


# ---------------------------------------------------------------------------------------
# wrap tools


def _only_keep_non_none_values(d: dict):
    return {k: v for k, v in d.items() if v is not None}


@double_up_as_factory
def ch_names(func=None, **old_to_new_name):
    """Change the argument names of a function.

    >>> def f(w, /, x: float, y=2, *, z: int = 3):
    ...     return f"(w:={w}) + (x:={x}) * (y:={y}) ** (z:={z}) == {w + x * y ** z}"
    >>> wrapped_f = ch_names(f, w='DoubleYou', z='Zee')
    >>> wrapped_f
    <i2.Wrap f(DoubleYou, /, x: float, y=2, *, Zee: int = 3)>
    >>> wrapped_f(1, 2, y=3, Zee=4)
    '(w:=1) + (x:=2) * (y:=3) ** (z:=4) == 163'

    Can also be used as a factory:
    >>> @ch_names(a='alpha', g='gamma')
    ... def foo(a, b, g=1):
    ...     return a + b * g
    >>> foo(alpha=1, b=2, gamma=3)
    7
    """
    old_to_new_name = _only_keep_non_none_values(old_to_new_name)
    if old_to_new_name:
        return Ingress.name_map(func, **old_to_new_name).wrap(func)
    else:
        return func


map_names = ch_names  # back-compatibility alias

from i2.deco import _resolve_inclusion


# TODO: A more general version would allow include and exclude to be expressed as
#     functions that apply to one or several properties of the params
#     (name, kind, default, annotation).
def include_exclude_ingress_factory(
    func, include=None, exclude=None, *, allow_partial=False
):
    """
    A pattern underlying any ingress that takes a subset of parameters (possibly
    reordering them).

    For example: Keep only required arguments, or reorder params to be able to
    partialize #3 (without having to partialize #1 and #2)
    """
    sig = Sig(func)
    include = _resolve_inclusion(include, exclude, sig.names)
    return Ingress(inner_sig=sig, outer_sig=sig[include], allow_partial=allow_partial)


@double_up_as_factory
def include_exclude(func=None, *, include=None, exclude=None):
    """Reorder and/or remove parameters.

    >>> def foo(a, b, c='C', d='D'):
    ...     print(f"{a=},{b=},{c=},{d=}")
    >>> bar = include_exclude(foo, include='b a', exclude='c d')

    The signature of ``bar`` has only ``b`` and ``a``, in that order:

    >>> from inspect import signature
    >>> str(signature(bar))
    '(b, a)'

    But the function still works and does the same thing:

    >>> bar('B', 'A')
    a='A',b='B',c='C',d='D'

    """
    return wrap(func, ingress=include_exclude_ingress_factory(func, include, exclude))


# TODO: Not working completely with allow_removal_of_non_defaulted_params=True
#  See https://github.com/i2mint/i2/issues/44
@double_up_as_factory
def rm_params(
    func=None,
    *,
    params_to_remove=(),
    allow_removal_of_non_defaulted_params=False,
    allow_partial=False,
):
    """Get a function with some parameters removed.

    >>> from inspect import signature
    >>> def func(x, y=1, z=2):
    ...     return x + y * z
    >>>
    >>> f = rm_params(func, params_to_remove='z')
    >>> assert f(3) == func(3) == 5
    >>> assert f(3, 4) == func(3, 4) == 11
    >>> str(signature(f))
    '(x, y=1)'
    >>>
    >>> f = rm_params(func, params_to_remove='y z')
    >>> assert f(3) == func(3) == 5
    >>> str(signature(f))
    '(x)'

    But ``rm_params`` won't let you remove params that don't have defaults.

    >>> f = rm_params(func, params_to_remove='x z')
    Traceback (most recent call last):
    ...
    AssertionError: Some of the params you want to remove don't have defaults: {'x'}

    """
    if isinstance(params_to_remove, str):
        params_to_remove = params_to_remove.split()
    sig = Sig(func)
    params_to_remove_that_do_not_have_defaults = set(params_to_remove) & set(
        sig.without_defaults.names
    )
    if not allow_removal_of_non_defaulted_params:
        assert not params_to_remove_that_do_not_have_defaults, (
            f"Some of the params you want to remove don't have defaults: "
            f"{params_to_remove_that_do_not_have_defaults}"
        )

    return wrap(
        func,
        ingress=include_exclude_ingress_factory(
            func, exclude=params_to_remove, allow_partial=allow_partial
        ),
    )


#     new_sig = sig - params_to_remove
#     return new_sig(func)


def arg_val_converter(func, **conversion_for_arg):
    return Wrap(func, ingress=ArgValConverterIngress(func, **conversion_for_arg))


def arg_val_converter_ingress(func, __strict=True, **conversion_for_arg):
    sig = Sig(func)
    if __strict:
        conversion_names_that_are_not_func_args = conversion_for_arg.keys() - sig.names
        assert not conversion_names_that_are_not_func_args, (
            "Some of the arguments you want to convert are not argument names "
            f"for the function: {conversion_names_that_are_not_func_args}"
        )

    @sig
    def ingress(*args, **kwargs):
        # TODO: Make a helper function for this ak -> k -> proc -> ak pattern
        kwargs = sig.map_arguments(args, kwargs)
        kwargs = dict(convert_dict_values(kwargs, conversion_for_arg))
        args, kwargs = sig.mk_args_and_kwargs(kwargs)
        return args, kwargs

    return ingress


# TODO: Fits global pattern -- merge
class ArgValConverterIngress:
    def __init__(self, func, __strict=True, **conversion_for_arg):
        sig = Sig(func)
        if __strict:
            conversion_names_that_are_not_func_args = (
                conversion_for_arg.keys() - sig.names
            )
            assert not conversion_names_that_are_not_func_args, (
                "Some of the arguments you want to convert are not argument names "
                f"for the function: {conversion_names_that_are_not_func_args}"
            )
        self.sig = sig
        self.conversion_for_arg = conversion_for_arg
        wraps(func)(self)

    def __call__(self, *args, **kwargs):
        # TODO: Make a helper function for this ak -> k -> proc -> ak pattern
        kwargs = self.sig.map_arguments(args, kwargs)
        kwargs = dict(convert_dict_values(kwargs, self.conversion_for_arg))
        args, kwargs = self.sig.mk_args_and_kwargs(kwargs)
        return args, kwargs


# ---------------------------------------------------------------------------------------
# Utils to help define value conversions in ingress functions


def convert_dict_values(to_convert: dict, key_to_conversion_function: dict):
    for k, v in to_convert.items():
        if k in key_to_conversion_function:
            conversion_func = key_to_conversion_function[k]
            yield k, conversion_func(v)  # converted kv pair
        else:
            yield k, v  # unconverted kv pair


# TODO: Test for performance and ask about readability
def _alt_convert_dict_values(to_convert: dict, key_to_conversion_function: dict):
    for k, v in to_convert.items():
        conversion_func = key_to_conversion_function.get(k, lambda x: x)
        yield k, conversion_func(v)


def params_used_in_funcs(funcs):
    return {name for func in funcs for name in Sig(func).names}


def required_params_used_in_funcs(funcs):
    return {name for func in funcs for name in Sig(func).required_names}


# TODO: Should we add some explicit/validation/strict options?
def kwargs_trans(
    kwargs: dict = None, /, _recursive=False, _inplace=False, **key_and_val_func
):
    """Transform a kwargs dict or build a transformer.

    :param kwargs: The dict containing the input kwargs that we will transform
    :param _recursive: Whether the transformations listed in ``key_and_val_func`` should
        be applied "recursively". When set to ``False`` (default), each transformation
        function applies to the original ``kwargs``, not the one that was transformed,
        so far.
    :param _inplace: If set to ``False`` will make a shallow copy of the ``kwargs``
        before transforming it (only relevant if ``_recursive=True``
    :param key_and_val_func: The ``key=val_func`` pairs that indicate that a
        ``val_func`` should be applied to the ``kwargs``, maching the argument names of
        the ``val_func`` to the keys of ``kwargs`` and extracting the values found
        therein to use for the corresponding inputs of that ``val_func``.
    :return: The transformed kwargs.

    >>> d = dict(a=1, b=2, c=3)
    >>> kwargs_trans(
    ...     d,
    ...     a=lambda a: a * 10,
    ...     b=lambda a, b: a + b
    ... )
    {'a': 10, 'b': 3, 'c': 3}

    See that ``d`` is unchanged here (transformation is not in place).

    >>> d
    {'a': 1, 'b': 2, 'c': 3}

    Typically you'll use ``kwargs_trans`` as a factory:

    >>> trans = kwargs_trans(a=lambda a: a * 10, b=lambda a, b: a + b)
    >>> trans(d)
    {'a': 10, 'b': 3, 'c': 3}

    Here we'll demo what the ``_recursive`` and ``_inplace`` arguments do.

    >>> from functools import partial
    >>> re_kwargs_trans = partial(kwargs_trans, _recursive=True, _inplace=True)
    >>> d = dict(a=1, b=2, c=3)
    >>>
    >>> re_kwargs_trans(
    ...     d,
    ...     a=lambda a: a * 10,
    ...     b=lambda a, b: a + b
    ...     # since _recursive=True, the a that is used is the new a = 10, not a = 1:
    ... )
    {'a': 10, 'b': 12, 'c': 3}

    Since ``_inplace=True``, ``d`` itself has changed:

    >>> d
    {'a': 10, 'b': 12, 'c': 3}

    Sometimes you'll pipe several transformers together:

    >>> from i2 import Pipe
    >>> trans = Pipe(
    ...     re_kwargs_trans(a=lambda a: a / 10),
    ...     re_kwargs_trans(c=lambda a,b,c: a * b * c),
    ...     # and then compute a new value of a using a and c:
    ...     re_kwargs_trans(a=lambda a, c: c - 1),
    ... )
    >>> trans(d)
    {'a': 35.0, 'b': 12, 'c': 36.0}
    """
    if kwargs is None:
        return partial(
            kwargs_trans, _recursive=_recursive, _inplace=_inplace, **key_and_val_func
        )
    if not _recursive:
        new_kwargs = dict()
        for key, val_func in key_and_val_func.items():
            new_kwargs[key] = _call_forgivingly(val_func, (), kwargs)
        return dict(kwargs, **new_kwargs)
    else:
        if _inplace is False:
            kwargs = kwargs.copy()
        for key, val_func in key_and_val_func.items():
            kwargs[key] = _call_forgivingly(val_func, (), kwargs)
        return kwargs


# ---------------------------------------------------------------------------------------

empty = Parameter.empty


def camelize(s):
    """
    >>> camelize('camel_case')
    'CamelCase'
    """
    return "".join(ele.title() for ele in s.split("_"))


def kwargs_trans_to_extract_args_from_attrs(
    outer_kwargs: dict, attr_names=(), obj_param="self"
):
    self = outer_kwargs.pop(obj_param)
    arguments_extracted_from_obj = {name: getattr(self, name) for name in attr_names}
    # The kwargs we need are the union of the extracted arguments with the remaining outer_kwargs
    return dict(arguments_extracted_from_obj, **outer_kwargs)


# TODO: kind lost here, only 3.10 offers dataclasses with some control over kind:
#   See: https://stackoverflow.com/questions/49908182/how-to-make-keyword-only-fields-with-dataclasses
def param_to_dataclass_field_tuple(param: Parameter):
    return param.name, param.annotation, param.default


MethodFunc = Callable


def func_to_method_func(
    func,
    instance_params=(),
    *,
    method_name=None,
    method_params=None,
    instance_arg_name="self",
) -> MethodFunc:
    """Get a 'method function' from a 'normal function'. Also known as "methodize".

    That is, get a function that gives the same outputs as the 'normal function',
    except that some of the arguments are sourced from the attributes of the first
    argument.

    The intended use case is when you want to inject one or several methods in a class
    or instance, sourcing some of the arguments of the underlying function from a
    common pool: The attributes of the instance.

    Consider the following function involving four parameters: ``a, b, c`` and ``d``.

    >>> def func(a, b: int, c=2, *, d='bar'):
    ...     return f"{d}: {(a + b) * c}"
    >>> func(1, 2, c=3, d='hello')
    'hello: 9'

    If we wanted to make an equivalent "method function" that would source it's ``a``
    and it's ``c`` from the first argument's (in practice this first argument will be
    and instance of the class the method will be bound to), we can do so like so:

    >>> method_func = func_to_method_func(func, 'a c')
    >>> from inspect import signature
    >>> str(signature(method_func))
    "(self, b: int, *, d='bar')"

    Note that the first argument is ``self`` (default name for an "instance"),
    that ``a`` and ``c`` are not there, but that the two remaining parameters,
    ``b`` and ``d`` are present, in the same order, and with the same annotations and
    parameter kind (the ``d`` is still keyword-only).

    Now let's make a dummy object that has attributes ``a`` and a ``c``, and use it to
    call ``method_func``:

    >>> from collections import namedtuple
    >>> instance = namedtuple('FakeInstance', 'a c')(1, 3)
    >>> method_func(instance, 2, d='hello')
    'hello: 9'

    Which is:

    >>> assert method_func(instance, 2, d='hello') == func(1, 2, c=3, d='hello')

    Often, though, what you'll want is to include this method function directly in
    a class, as you're making that class "normally". That works too:

    >>> from dataclasses import dataclass
    >>> @dataclass
    ... class Klass:
    ...     a : int = 1
    ...     c : int = 3
    ...     method_func = func_to_method_func(func, 'a c')
    >>> instance = Klass(1, 3)
    >>> instance.method_func(2, d='hello')
    'hello: 9'

    What if your function has argument names that don't correspond to the names you
    have, or want, as attributes of the class? Or even, you have several functions that
    share an argument name that need to be bound to a different attribute?

    For that, just use ``map_names`` to wrap the function, giving it the names that
    you need to give it to have the effect you want (the binding of those arguments
    to attributes of the instance):

    >>> from i2.wrapper import ch_names
    >>> def func(x, y: int, z=2, *, d='bar'):
    ...     return f"{d}: {(x + y) * z}"
    >>> from dataclasses import dataclass
    >>> @dataclass
    ... class Klass:
    ...     a : int = 1
    ...     c : int = 3
    ...     method_func = func_to_method_func(ch_names(func, x='a', z='c'), 'a c')
    >>> instance = Klass(1, 3)
    >>> instance.method_func(2, d='hello')
    'hello: 9'

    """
    # get a signature object for func
    sig = Sig(func)
    # if method_name not give, use the function's name
    method_name = method_name or sig.name
    # if params expressed as string, split it into a list of parameter (names)
    if isinstance(instance_params, str):
        instance_params = instance_params.split()
    if isinstance(method_params, str):
        method_params = method_params.split()
    # if method_params is not given, take those parameters that aren't in instance_params
    method_params = method_params or tuple(
        name for name in sig.names if name not in instance_params
    )
    # the Sig object of the method: instance name followed with method_params
    method_sig = instance_arg_name + Sig(func)[method_params]
    # make the ingress function that will map method_sig's interface to sig's.
    ingress = Ingress(
        inner_sig=sig,
        # inside, foo will be doing the work, so need to map to its signature
        kwargs_trans=partial(
            # this is how to transform outer (args, kwargs) to inner ones
            kwargs_trans_to_extract_args_from_attrs,
            attr_names=instance_params,
            obj_param=instance_arg_name,
        ),
        outer_sig=method_sig,  # this is the signature we want at the interface
    )
    # wrap the function, name it and return it
    method_func = wrap(func, ingress=ingress)
    method_func.__name__ = method_name
    return method_func


# TODO: See bind_funcs_object_attrs_old and get rid of it if bind_funcs_object_attrs
#  is better and has all old has.
# TODO: Make instances picklable (class is already, but not instances!)!!
def bind_funcs_object_attrs(
    funcs, init_params=(), *, cls=None, module=None, **extra_attrs
):
    """Transform one or several functions into a class that contains them as methods
    sourcing specific arguments from the instance's attributes.

    >>> from inspect import signature
    >>> from dataclasses import dataclass
    >>>
    >>> def foo(a, b, c=2, *, d='bar'):
    ...     return f"{d}: {(a + b) * c}"
    >>> foo(1, 2)
    'bar: 6'
    >>> Klass = bind_funcs_object_attrs(foo, init_params='a c')
    >>> Klass.__name__
    'Foo'
    >>> instance = Klass(a=1, c=3)
    >>> assert instance.foo(2, d='hello') == 'hello: 9' == foo(
    ...     a=1, b=2, c=3, d='hello')
    >>> str(signature(Klass))
    '(a, c=2) -> None'
    >>>
    >>> instance = Klass(a=1, c=3)
    >>> str(instance)
    'Foo(a=1, c=3)'
    >>> str(signature(instance.foo))
    "(b, *, d='bar')"
    >>> instance.foo(2, d='hello')
    'hello: 9'
    >>> instance.foo(10, d='goodbye')
    'goodbye: 33'

    >>> def foo(a, b, c):
    ...     return a + b * c
    ...
    >>> def bar(d, e):
    ...     return f"{d=}, {e=}"
    ...
    >>> @dataclass
    ... class K:
    ...     a: int
    ...     e: int
    ...
    >>> C = bind_funcs_object_attrs([foo, bar], 'a e', cls=K)
    >>> str(signature(C))
    '(a: int, e: int) -> None'
    >>> c = C(1,2)
    >>> assert str(signature(c.foo)) == '(b, c)'
    >>> c.foo(3,4)
    13
    >>> assert str(signature(c.bar)) == '(d)'
    >>> c.bar(5)
    'd=5, e=2'
    """

    if isinstance(init_params, str):
        init_params = init_params.split()

    dflt_cls_name = "FuncsUnion"
    if callable(funcs) and not isinstance(funcs, Iterable):
        single_func = funcs
        dflt_cls_name = camelize(getattr(single_func, "__name__", dflt_cls_name))
        funcs = [single_func]

    # If cls not given, make one from the funcs and init_params
    if not isinstance(cls, type):
        # if the class is not given, we need to make one
        # ... for this we make a name
        if isinstance(cls, str):
            cls_name = cls
        else:
            cls_name = dflt_cls_name
        # ... and an actual class
        cls = _mk_base_class_for_funcs(cls_name, init_params, funcs)

    for func in funcs:
        method_func = func_to_method_func(func, init_params)
        setattr(cls, method_func.__name__, method_func)

    if module:
        cls.__module__ = module
    for attr_name, attr_val in extra_attrs.items():
        setattr(cls, attr_name, attr_val)

    return cls


class PickleHelperMixin:
    def __reduce__(self):
        return self.__name__


def _mk_base_class_for_funcs(cls_name, init_params, funcs):
    """
    Make a class with given init_params.

    :param cls_name: The name the class should have
    :param init_params: list of strings used to determine what arg names are in the init
    :param funcs: list of functions used to add defaults and annotations to the init args
    :return:
    """
    # Make the signature for the init
    class_init_sig = Sig(init_params)
    if funcs:
        for func in funcs:
            init_params_in_func_sig = list(
                filter(None, map(Sig(func).get, init_params))
            )
            class_init_sig = class_init_sig.merge_with_sig(
                init_params_in_func_sig,
                default_conflict_method="fill_defaults_and_annotations",
            )

    dataclass_fields = list(map(param_to_dataclass_field_tuple, class_init_sig.params))
    cls = make_dataclass(cls_name, dataclass_fields, bases=(PickleHelperMixin,))

    return cls


def bind_funcs_object_attrs_old(
    funcs,
    init_params=(),
    *,
    cls=None,
):
    """Transform one or several functions into a class that contains them as methods
    sourcing specific arguments from the instance's attributes.
    >>> from inspect import signature
    >>> from dataclasses import dataclass
    >>>
    >>> def foo(a, b, c=2, *, d='bar'):
    ...     return f"{d}: {(a + b) * c}"
    >>> foo(1, 2)
    'bar: 6'
    >>> Klass = bind_funcs_object_attrs_old(foo, init_params='a c')
    >>> Klass.__name__
    'Foo'
    >>> instance = Klass(a=1, c=3)
    >>> assert instance.foo(2, d='hello') == 'hello: 9' == foo(
    ...     a=1, b=2, c=3, d='hello')
    >>> str(signature(Klass))
    '(a, c=2) -> None'
    >>>
    >>> instance = Klass(a=1, c=3)
    >>> str(instance)
    'Foo(a=1, c=3)'
    >>> str(signature(instance.foo))
    "(b, *, d='bar')"
    >>> instance.foo(2, d='hello')
    'hello: 9'
    >>> instance.foo(10, d='goodbye')
    'goodbye: 33'
    >>> def foo(a, b, c):
    ...     return a + b * c
    ...
    >>> def bar(d, e):
    ...     return f"{d=}, {e=}"
    ...
    >>> @dataclass
    ... class K:
    ...     a: int
    ...     e: int
    ...
    >>> C = bind_funcs_object_attrs([foo, bar], 'a e', cls=K)
    >>> str(signature(C))
    '(a: int, e: int) -> None'
    >>> c = C(1,2)
    >>> assert str(signature(c.foo)) == '(b, c)'
    >>> c.foo(3,4)
    13
    >>> assert str(signature(c.bar)) == '(d)'
    >>> c.bar(5)
    'd=5, e=2'
    """

    if isinstance(init_params, str):
        init_params = init_params.split()

    dflt_cls_name = "FuncsUnion"
    if callable(funcs) and not isinstance(funcs, Iterable):
        single_func = funcs
        dflt_cls_name = camelize(getattr(single_func, "__name__", dflt_cls_name))
        funcs = [single_func]

    if not isinstance(cls, type):
        # if the class is not given, we need to make one
        if isinstance(cls, str):
            cls_name = cls
        else:
            cls_name = dflt_cls_name

        # init_parameter_objects = Sig(func)[init_params].params
        # Make the signature for the init
        class_init_sig = Sig()
        for func in funcs:
            class_init_sig = class_init_sig.merge_with_sig(func)[init_params]

        dataclass_fields = list(
            map(param_to_dataclass_field_tuple, class_init_sig.params)
        )
        cls = make_dataclass(cls_name, dataclass_fields)

    for func in funcs:
        method_func = func_to_method_func(func, init_params)
        setattr(cls, method_func.__name__, method_func)
    return cls


def _items_filt(d: dict, keys):
    for k, v in d.items():
        if k in keys:
            yield k, v


def _mk_sig_from_params_and_funcs(params, funcs):
    def gen():
        for param in params:
            pass


## An attempt to redo a func_to_method_func because I forgot func_to_method_func existed
## Has some different ideas, so keeping around until I decide it's time to let go.
# NoSuchKey = type('NoSuchKey', (), {})
# _instance_extractor: KwargsTrans
#
# # TODO: Add more (possibly optional) bind validation to fail early.
# def _instance_extractor(
#     outer_kwargs, bind: IdentifierMapping = (), instance_param: Identifier = 'self'
# ):
#     """
#
#     :param outer_kwargs: The input/outer keyword arguments
#     :param bind: The inner->outer param mapping that defines what we want to extract
#     :param instance_param: The name of the outer_kwargs key that contains the 'instance'
#         from which we'll extract the bound
#     :return:
#     """
#     """A KwargsTrans that extracts need arguments from one of the 'instance'
#     outer_kwargs values.
#
#
#     """
#     # Compute the inverse {outer:inner,...} of {inner:outer,...} bind
#     inv_bind = invert_map(bind)
#     outer_kwargs = outer_kwargs.copy()
#     instance = outer_kwargs.pop(instance_param)  # TODO: Better error handing
#
#     def gen():
#         for outer_param, outer_val in outer_kwargs.items():
#             if inner_param := inv_bind.get(outer_param, NoSuchKey) is not NoSuchKey:
#                 # if outer_param was bound, the bound inner_param should be tied to
#                 # the instance's attribute
#                 yield inner_param, getattr(instance, outer_val)
#             else:
#                 # take the arg name and val as is
#                 yield outer_param, outer_val
#
#     return dict(gen())

#
# def methodize(func, bind: Bind = ()):
#     bind = identifier_mapping(bind)
#     ingress = Ingress(
#         outer_sig=Sig(func),
#         kwargs_trans=partial(_instance_extractor, bind=bind),
#         inner_sig=Sig('self') + Sig(func) - Sig(list(bind)),  # TODO: solve type or lint
#     )
#     return wrap(func, ingress=ingress)


# ---------------------------------------------------------------------------------------
# Extended Wrapper class


class WrapperValidationError(ValueError):
    """Raised when wrapper some construction params are not valid"""


class EgressValidationError(WrapperValidationError):
    """Raised when a egress is not valid"""


class IngressValidationError(WrapperValidationError):
    """Raised when a ingress is not valid"""


class CallerValidationError(WrapperValidationError):
    """Raised when a caller is not valid"""


def _default_ingress(*args, **kwargs):
    return args, kwargs


def _default_egress(output, **egress_params):
    return output


def _default_caller(func, args, kwargs):
    return func(*args, **kwargs)


_keyword_kinds = {Sig.KEYWORD_ONLY, Sig.VAR_KEYWORD}


def _all_kinds_are_keyword_only_or_variadic_keyword(sig):
    return all(kind in _keyword_kinds for kind in list(sig.kinds.values())[3:])


# TODO: Factor out more common parts with Wrap and reuse (possibly through _Wrap)
class Wrapx(_Wrap):
    def __init__(self, func, ingress=None, egress=None, *, caller=None, name=None):
        """An extended wrapping object that allows more complex wrapping mechanisms.

        :param func: The wrapped function
        :param ingress: The incoming data transformer. It determines the argument properties
            (name, kind, default and annotation) as well as the actual input of the
            wrapped function.
        :param egress: The outgoing data transformer. It also takes precedence over the
            wrapped function to determine the return annotation of the ``Wrap`` instance
        :param caller: A caller defines what it means to call the ``func`` on the
            arguments it is given. It should be of the form
            ``caller(func, args, kwargs, *, ...extra_keyword_only_params)``.
            By default, the caller will simply return ``func(*args, **kwargs)``.
        :param name: Name to give the wrapper (will use wrapped func name by default)

        :return: A callable instance wrapping ``func``

        >>> from inspect import signature
        >>>
        >>> def func(x, y):
        ...     return x + y
        ...
        >>> def save_on_output_egress(v, *, k, s):
        ...     s[k] = v
        ...     return v
        ...
        >>> save_on_output = Wrapx(func, egress=save_on_output_egress)
        >>> # TODO: should be `(x, y, *, k, s)` --> Need to work on the merge for this.
        >>> str(signature(save_on_output))
        '(x, y, k, s)'
        >>>
        >>> store = dict()
        >>> save_on_output(1, 2, k='save_here', s=store)
        3
        >>> assert save_on_output(1, 2, k='save_here', s=store) == 3 == func(1, 2)
        >>> store  # see what's in the store now!
        {'save_here': 3}

        A caller is meant to control the way the function is called.
        It is given the ``func`` and the ``func_args`` and ``func_kwargs``
        (whatever the ingress function gives it, if present) and possibly additional
        params and will return... well, what ever you tell it to.

        This can be used, for example, to call the function in a subprocess,
        or on a remote system, differ computation (command pattern, for example, using
        ``functools.partial``, or do what ever needs to have a view both on the function
        and its inputs.

        Here, we will wrap the function so it will apply to an iterable of inputs
        (of the first argument), returning a list of results

        >>> def func(x, y=2):
        ...     return x + y
        ...
        >>> def iterize(func, args, kwargs):
        ...     first_arg_val = next(iter(kwargs.values()))
        ...     return list(map(func, first_arg_val))
        ...
        >>> iterized_func = Wrapx(func, caller=iterize)
        >>> iterized_func([1, 2, 3, 4])
        [3, 4, 5, 6]

        Let's do the same as above, but allow other variables (here ``y``) to be input as
        well. This takes a bit more work...

        >>> from functools import partial
        >>> def _iterize_first_arg(func, args, kwargs):
        ...     first_arg_name = next(iter(kwargs))
        ...     remaining_kwargs = {
        ...         k: v for k, v in kwargs.items() if k != first_arg_name
        ...     }
        ...     return list(
        ...         map(partial(func, **remaining_kwargs), kwargs[first_arg_name])
        ...     )

        Let's demo a different way of using Wrapx: Making a wrapper to apply at
        function definition time

        >>> iterize_first_arg = partial(Wrapx, caller=_iterize_first_arg)
        >>> @iterize_first_arg
        ... def func(x, y):
        ...     return x + y
        >>>
        >>> func([1, 2, 3, 4], 10)
        [11, 12, 13, 14]


        """
        super().__init__(func, ingress, egress, caller=caller, name=name)
        self.ingress, self.egress, self.caller, self.sig = _process_wrapx_params(
            func, ingress, egress, caller
        )

        self.__signature__ = self.sig
        self.__wrapped__ = func
        # TODO: Pros and cons analysis of pointing __wrapped__ to func. partial uses
        #  .func, but wraps looks for __wrapped__

    def __call__(self, *args, **kwargs):
        try:
            _kwargs = self.sig.map_arguments(args, kwargs)
            # TODO: Consider call_forgivingly(self.ingress, *args, **kwargs)
            #  because call_forgivingly(self.ingress, **_kwargs) doesn't allow
            #  same ingress functions to be used for Wrap and Wrapx
            func_args, func_kwargs = call_forgivingly(self.ingress, **_kwargs)
            inner_output = call_forgivingly(
                self.caller, self.func, func_args, func_kwargs
            )
            return call_forgivingly(self.egress, inner_output, **_kwargs)
        except Exception as e:
            # Try running again, but with more careful validation, to try to give
            # more specific error messages
            # We don't do this in the first run so that we don't incur the validation
            # overhead on every call.
            _kwargs = self.sig.map_arguments(args, kwargs)
            func_args, func_kwargs = _validate_ingress_output(
                call_forgivingly(self.ingress, **_kwargs)
            )
            inner_output = call_forgivingly(self.func, *func_args, **func_kwargs)
            return call_forgivingly(self.egress, inner_output, **_kwargs)


def _validate_ingress_output(ingress_output):
    if (
        not isinstance(ingress_output, tuple)
        or not len(ingress_output) == 2
        or not isinstance(ingress_output[0], tuple)
        or not isinstance(ingress_output[1], dict)
    ):
        raise IngressValidationError(
            f"An ingress function should return a (tuple, dict) pair. "
            f"This ingress function returned: {ingress_output}"
        )
    return ingress_output


def _process_wrapx_params(func, ingress, egress, caller):
    func_sig = Sig(func)

    ingress, ingress_sig = _init_ingress(func_sig, ingress)
    egress, egress_sig = _init_egress(func_sig, egress)
    caller, caller_sig = _init_caller(caller)

    sig = _mk_composite_sig(ingress_sig, egress_sig, caller_sig)
    return ingress, egress, caller, sig


def _mk_composite_sig(ingress_sig, egress_sig, caller_sig):
    egress_sig_minus_first_arg = egress_sig - egress_sig.names[0]
    caller_sig_minus_three_first_args = caller_sig - caller_sig.names[:3]
    sig = Sig(
        ingress_sig + egress_sig_minus_first_arg + caller_sig_minus_three_first_args,
        return_annotation=egress_sig.return_annotation,
    )
    return sig


def _init_caller(caller):
    if caller is None:
        caller = _default_caller
        caller_sig = Sig("func args kwargs")  # sig with three inputs
    else:
        caller_sig = Sig(caller)
        if len(caller_sig) < 3:
            raise CallerValidationError(
                f"A caller must have at least three arguments: "
                f"{caller} signature was {caller_sig}"
            )
        if not _all_kinds_are_keyword_only_or_variadic_keyword(caller_sig):
            raise CallerValidationError(
                f"A caller must have at least three arguments"
                f"{caller} signature was {caller_sig}"
            )
    return caller, caller_sig


def _init_egress(func_sig, egress):
    if egress is None:
        egress = _default_egress
        # signature with a single 'output' arg and func_sig's return_annotation
        egress_sig = Sig("output", return_annotation=func_sig.return_annotation)
        return_annotation = func_sig.return_annotation
    else:
        egress_sig = Sig(
            egress,
            # if egress has no return_annotation, use the func_sig's one.
            # TODO: Is this really correct/safe? What if egress changes the type?
            return_annotation=Sig(egress).return_annotation
            or func_sig.return_annotation,
        )

    return egress, egress_sig


def _init_ingress(func_sig, ingress):
    if ingress is None:
        ingress = _default_ingress
        ingress_sig = func_sig
    else:
        ingress_sig = Sig(ingress)
    return ingress, ingress_sig


# ---------------------------------------------------------------------------------------
# partialx


def partialx(
    func, *args, __name__=None, _rm_partialize=False, _allow_reordering=False, **kwargs
):
    """
    Extends the functionality of builtin ``functools.partial`` with the ability to

    - set ``__name__ ``

    - remove partialized arguments from signature

    - reorder params (so that defaults are at the end)

    >>> def f(a, b=2, c=3):
    ...     return a + b * c
    >>> curried_f = partialx(f, c=10, _rm_partialize=True)
    >>> curried_f.__name__
    'f'
    >>> from inspect import signature
    >>> str(signature(curried_f))
    '(a, b=2)'

    >>> def f(a, b, c=3):
    ...     return a + b * c

    Note that ``a`` gets a default, but ``b`` does not, yet is after ``a``.
    This is allowed because these parameters all became KEYWORD_ONLY.

    >>> g = partialx(f, a=1)
    >>> str(Sig(g))
    '(*, a=1, b, c=3)'

    If you wanted to reorder the parameters to have all defaulted kinds be at the end,
    as usual, you can do so using ``_allow_reordering=True``

    >>> g = partialx(f, a=1, _allow_reordering=True)
    >>> str(Sig(g))
    '(*, b, a=1, c=3)'

    """
    f = partial(func, *args, **kwargs)
    if _rm_partialize:
        sig = Sig(func)
        partialized = list(sig.map_arguments(args, kwargs, allow_partial=True))
        sig = sig - partialized
        f = sig(partial(f, *args, **kwargs))
    if _allow_reordering:
        # TODO: Instead of Sig(f).defaults, move only params that need to move
        # TODO: + Change signature so that the number of params that become keyword-only
        #   is minimize.
        f = move_params_to_the_end(f, Sig(f).defaults)
    f.__name__ = __name__ or name_of_obj(func)
    return f


def move_params_to_the_end(func: Callable, names_to_move: Callable | Iterable[str]):
    """
    Choose args from func, according to choice_args_func and move them
    to the right

    >>> from functools import partial
    >>> from i2 import Sig
    >>> def foo(a, b, c):
    ...     return a + b + c
    >>> g = partial(foo, b=4)  # fixing a, which is before b
    >>> h = move_params_to_the_end(g, Sig(g).defaults)
    >>> assert str(Sig(g)) == '(a, *, b=4, c)'
    >>> assert str(Sig(h)) == '(a, *, c, b=4)'

    """
    if callable(names_to_move):
        names_to_move = names_to_move(func)
    assert isinstance(names_to_move, Iterable), (
        f"names_to_move must be an iterable of names "
        f"or a callable producing one from a function. Was {names_to_move}"
    )

    names = Sig(func).names
    reordered = move_names_to_the_end(names, names_to_move)
    wrapped_func = include_exclude(func, include=reordered)
    return wrapped_func


def move_names_to_the_end(names, names_to_move_to_the_end):
    """
    Remove the items of ``names_to_move_to_the_end`` from ``names``
    and append to the right of names

    >>> names = ['a','c','d','e']
    >>> names_to_move_to_the_end = ['c','e']
    >>> move_names_to_the_end(names, names_to_move_to_the_end)
    ['a', 'd', 'c', 'e']
    >>> names_to_move_to_the_end = 'c e'
    >>> move_names_to_the_end(names, names_to_move_to_the_end)
    ['a', 'd', 'c', 'e']

    """
    if isinstance(names_to_move_to_the_end, str):
        names_to_move_to_the_end = names_to_move_to_the_end.split()
    else:
        names_to_move_to_the_end = list(names_to_move_to_the_end)
    removed = [x for x in names if x not in names_to_move_to_the_end]
    return list(removed) + names_to_move_to_the_end


# --------------------------------------------------------------------------------------
# smart defaults


# IDEA: Could make SmartDefault have an out and __call__, working like a meshed.FuncNode
@dataclass
class SmartDefault:
    func_computing_default: Callable
    original_default: Any = empty

    def __repr__(self):
        func = self.func_computing_default
        func_name = getattr(func, "__name__", str(func))
        if self.original_default is empty:
            return f"SmartDefault({func_name})"
        else:
            dflt = self.original_default
            if isinstance(dflt, str):
                return f"SmartDefault({func_name}, '{dflt}')"
            else:
                return f"SmartDefault({func_name}, {dflt})"


# PATTERN: Yet another "meshed dict completion"
# IDEA: Could make SmartDefault have an out and __call__, working like a meshed.FuncNode
def complete_dict_applying_functions(
    d: dict, /, _only_if_name_missing=True, _allow_overwrites=False, **func_for_name
):
    """Complete dict ``d`` by applying function to variables in ``d``, sequentially.

    That is, doing ``d[name] = func(**d)`` for all ``name, func in d.items()``.

    Set ``_allow_overwrites=True`` to allow overwrites.

    Set ``_only_if_name_missing=False`` to apply all functions of ``func_for_name``
    regardless if the ``name`` already exists in ``d`` or not.

    >>> func_for_name = dict(
    ...     b=lambda a: a * 10, c=lambda a, b: a + b, d=lambda c: c * 2
    ... )
    >>> complete_dict_applying_functions(dict(a=1), **func_for_name)
    {'a': 1, 'b': 10, 'c': 11, 'd': 22}

    Notice that when ``b`` is present in input ``dict``, it's value is conserved.
    That is, the ``b`` of ``func_for_name`` isn't applied to compute it.

    >>> complete_dict_applying_functions(dict(a=1, b=2), **func_for_name)
    {'a': 1, 'b': 2, 'c': 3, 'd': 6}

    If you specify ``_only_if_name_missing=False``,
    ``complete_dict_applying_functions`` will try to compute everything
    ``func_for_name`` tells it too, regardless if the input dictionary contains the
    key or not, resulting in an error:

    >>> complete_dict_applying_functions(
    ...     dict(a=1, b=2), **func_for_name, _only_if_name_missing=False
    ... )
    Traceback (most recent call last):
      ...
    i2.errors.OverwritesNotAllowed: You're not allowed to overwrite to the values of b

    If you want, on the other hand, to allow overwrites, you can do so specifying
    ``_allow_overwrites=True``:

    >>> complete_dict_applying_functions(
    ...     dict(a=1, b=2), **func_for_name,
    ...     _only_if_name_missing=False, _allow_overwrites=True
    ... )
    {'a': 1, 'b': 10, 'c': 11, 'd': 22}

    """

    if _only_if_name_missing:
        func_for_name = {
            name: func for name, func in func_for_name.items() if name not in d
        }
    elif not _allow_overwrites and not func_for_name.keys().isdisjoint(d):
        raise OverwritesNotAllowed.for_keys(func_for_name.keys() & d)

    for name, func in func_for_name.items():
        # TODO: Can optimize using under-the-hood of call_forgivingly
        #  (extracting from _kwargs directly)
        # complete the keyword arguments with defaults computed from existing arguments
        d[name] = call_forgivingly(func, **d)

    return d


def _compute_new_sigs(func, /, **smart_defaults):
    original_sig = Sig(func)
    return original_sig.ch_defaults(
        **{
            name: SmartDefault(
                func_computing_default=func,
                original_default=original_sig.defaults.get(name, empty),
            )
            for name, func in smart_defaults.items()
        }
    )


@double_up_as_factory
def add_smart_defaults(
    func=None,
    *,
    _only_if_name_missing=True,
    _allow_overwrites=False,
    **smart_defaults,
):
    """Add smart defaults to function.

    Smart defaults compute defaults of inputs based on the other given inputs.

    :param func: The function to wrap.
    :param smart_defaults: ``input_arg=function_to_compute_it_from_other_args`` where
        the function's argument names must match the argument names of ``func``, the
        wrapped function.
    :return:

    >>> def xyz_sum(x, y, z='c'):  return x + y + z
    >>> def times_two(x):  return x * 2
    >>> def just_z():  return 'Z'
    >>> f = add_smart_defaults(xyz_sum, y=times_two, z=just_z)
    >>>
    >>> f('a', 'b', 'c')
    'abc'
    >>> f('a', 'b')
    'abZ'
    >>> f('a')
    'aaaZ'
    >>> f
    <i2.Wrap xyz_sum(x, y=SmartDefault(times_two), z=SmartDefault(just_z, 'c'))>

    """
    names_not_in_func_arguments = smart_defaults.keys() - Sig(func).names
    assert (
        not names_not_in_func_arguments
    ), f"These weren't argument names of the {func} function: {names_not_in_func_arguments}"
    kwargs_trans = partial(
        complete_dict_applying_functions,
        _only_if_name_missing=_only_if_name_missing,
        _allow_overwrites=_allow_overwrites,
        **smart_defaults,
    )
    smart_defaults_ingress = Ingress(
        inner_sig=Sig(func),
        kwargs_trans=kwargs_trans,
        outer_sig=_compute_new_sigs(func, **smart_defaults),
        apply_defaults=False,
    )

    return smart_defaults_ingress.wrap(func)
```

## README.md

```python
# i2

Core tools for minting code.

[Documentation here.](https://i2mint.github.io/i2/)

## What's mint?

Mint stands for "Meta-INTerface".

Minting is core technique of i2i: It can be seen as the encapsulation of a construct’s interface into a (data) 
structure that contains everything one needs to know about the construct to perform a specific action 
with or on the construct.

A little note on the use of “encapsulation”. The term is widely used in computer science, 
and is typically tied to object oriented programming. Wikipedia provides two definitions:
* A language mechanism for restricting direct access to some of the object's components.
* A language construct that facilitates the bundling of data with the methods (or other functions) 
operating on that data.

Though both these definitions apply to minting, 
the original sense of the word “encapsulate” is even more relevant (from google definitions): 
* express the essential features of (something) succinctly
* enclose (something) in or as if in a capsule

Indeed, minting is the process of enclosing a construct into a “mint” (for “Meta INTerface”) 
that will express the features of the construct that are essential to the task at hand. 
The mint provides a declarative layer of the construct that allows one to write code that operates with this layer, 
which is designed to be (as) consistent (as possible) from one system/language to another.

For example, whether a (non-anonymous) function was written in C, Python, or JavaScript, 
it will at least have a name, and it’s arguments will (most often) have names, and may have types. 
Similarly with “data objects”: The data of both JavaScript and Python objects can be represented by a tree whose 
leaves are base types, which can in turn be represented by a C struct.
```